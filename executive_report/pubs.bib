@misc{2009RT09Rich2009,
  title = {The 2009 ({{RT-09}}) {{Rich Transcription Meeting Recognition Evaluation Plan}}},
  date = {2009-02-24},
  publisher = {{NIST}},
  file = {/Users/brono/Zotero/storage/HLSKBCPQ/rt09-meeting-eval-plan-v2.pdf}
}

@online{2009RT09Rich2017,
  title = {The 2009 ({{RT-09}}) {{Rich Transcription Meeting Recognition Evaluation Plan}}},
  date = {2017-01-19},
  url = {https://web.archive.org/web/20170119114252/http://www.itl.nist.gov/iad/mig/tests/rt/2009/docs/rt09-meeting-eval-plan-v2.pdf},
  urldate = {2023-03-27},
  file = {/Users/brono/Zotero/storage/IDRJJGU2/2017 - Wayback Machine.pdf}
}

@article{abdulMelFrequencyCepstral2022,
  title = {Mel {{Frequency Cepstral Coefficient}} and Its {{Applications}}: {{A Review}}},
  shorttitle = {Mel {{Frequency Cepstral Coefficient}} and Its {{Applications}}},
  author = {Abdul, Zrar Kh. and Al-Talabani, Abdulbasit K.},
  date = {2022},
  journaltitle = {IEEE Access},
  shortjournal = {IEEE Access},
  volume = {10},
  pages = {122136--122158},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3223444},
  url = {https://ieeexplore.ieee.org/document/9955539/},
  urldate = {2023-03-20},
  abstract = {Feature extraction and representation has significant impact on the performance of any machine learning method. Mel Frequency Cepstrum Coefficient (MFCC) is designed to model features of audio signal and is widely used in various fields. This paper aims to review the applications that the MFCC is used for in addition to some issues that facing the MFCC computation and its impact on the model performance. These issues include the use of MFCC for non-acoustic signals, adopting the MFCC alone or combining it with other features, the use of time series versus global representation of the MFCC, following the standard form of the MFCC computation versus modifying its parameters, and supplying the traditional machine learning methods versus the deep learning methods.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/74UVUBNL/Abdul and Al-Talabani - 2022 - Mel Frequency Cepstral Coefficient and its Applica.pdf}
}

@inproceedings{addleseeComprehensiveEvaluationIncremental2020,
  title = {A {{Comprehensive Evaluation}} of {{Incremental Speech Recognition}} and {{Diarization}} for {{Conversational AI}}},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Computational Linguistics}}},
  author = {Addlesee, Angus and Yu, Yanchao and Eshghi, Arash},
  date = {2020-12},
  pages = {3492--3503},
  publisher = {{International Committee on Computational Linguistics}},
  location = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.312},
  url = {https://aclanthology.org/2020.coling-main.312},
  urldate = {2023-03-23},
  abstract = {Automatic Speech Recognition (ASR) systems are increasingly powerful and more accurate, but also more numerous with several options existing currently as a service (e.g. Google, IBM, and Microsoft). Currently the most stringent standards for such systems are set within the context of their use in, and for, Conversational AI technology. These systems are expected to operate incrementally in real-time, be responsive, stable, and robust to the pervasive yet peculiar characteristics of conversational speech such as disfluencies and overlaps. In this paper we evaluate the most popular of such systems with metrics and experiments designed with these standards in mind. We also evaluate the speaker diarization (SD) capabilities of the same systems which will be particularly important for dialogue systems designed to handle multi-party interaction. We found that Microsoft has the leading incremental ASR system which preserves disfluent materials and IBM has the leading incremental SD system in addition to the ASR that is most robust to speech overlaps. Google strikes a balance between the two but none of these systems are yet suitable to reliably handle natural spontaneous conversations in real-time.},
  eventtitle = {{{COLING}} 2020},
  file = {/Users/brono/Zotero/storage/U5EMF8C6/Addlesee et al. - 2020 - A Comprehensive Evaluation of Incremental Speech R.pdf}
}

@inproceedings{ahmedAutomaticSleepSpindle2009,
  title = {An Automatic Sleep Spindle Detector Based on Wavelets and the Teager Energy Operator},
  booktitle = {2009 {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  author = {Ahmed, B. and Redissi, A. and Tafreshi, R.},
  date = {2009-09},
  pages = {2596--2599},
  publisher = {{IEEE}},
  location = {{Minneapolis, MN}},
  doi = {10.1109/IEMBS.2009.5335331},
  url = {http://ieeexplore.ieee.org/document/5335331/},
  urldate = {2023-02-01},
  abstract = {Sleep spindles are one of the most important short-lasting rhythmic events occurring in the EEG during Non-Rapid Eye Movement sleep. Their accurate identification in a polysomnographic signal is essential for sleep professionals to help them mark Stage 2 sleep. Visual spindle scoring however is a tedious workload, as there are often a thousand spindles in an all-night recording. In this paper a novel approach for the automatic detection of sleep spindles based upon the Teager Energy Operator and wavelet packets has been presented. The Teager operator was found to accurately enhance periodic activity in epochs of the EEG containing spindles. The wavelet packet transform proved effective in accurately locating spindles in the time-frequency domain. The autocorrelation function of the resultant Teager signal and the wavelet packet energy ratio were used to identify epochs with spindles. These two features were integrated into a spindle detection algorithm which achieved an accuracy of 93.7\%.},
  eventtitle = {2009 {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}},
  langid = {english},
  file = {/Users/brono/Zotero/storage/A4VD5VJ9/Ahmed et al. - 2009 - An automatic sleep spindle detector based on wavel.pdf}
}

@inproceedings{al-hadithySpeakerDiarizationBased2022,
  title = {Speaker {{Diarization}} Based on {{Deep Learning Techniques}}: {{A Review}}},
  shorttitle = {Speaker {{Diarization}} Based on {{Deep Learning Techniques}}},
  booktitle = {2022 {{International Symposium}} on {{Multidisciplinary Studies}} and {{Innovative Technologies}} ({{ISMSIT}})},
  author = {Al-Hadithy, Thaer M. and Frikha, Mondher and Maseer, Zaidoon Kamil},
  date = {2022-10-20},
  pages = {856--871},
  publisher = {{IEEE}},
  location = {{Ankara, Turkey}},
  doi = {10.1109/ISMSIT56059.2022.9932710},
  url = {https://ieeexplore-ieee-org.wwwproxy1.library.unsw.edu.au/stamp/stamp.jsp?tp=&arnumber=9932710},
  urldate = {2023-03-26},
  eventtitle = {2022 {{International Symposium}} on {{Multidisciplinary Studies}} and {{Innovative Technologies}} ({{ISMSIT}})},
  isbn = {978-1-66547-013-1},
  file = {/Users/brono/Zotero/storage/N5X4UVHI/Al-Hadithy et al. - 2022 - Speaker Diarization based on Deep Learning Techniq.pdf}
}

@inproceedings{alghamdiPartSpeechTagging2016,
  title = {Part of Speech Tagging for Code Switched Data},
  booktitle = {Proceedings of the {{Second Workshop}} on {{Computational Approaches}} to {{Code}}           {{Switching}}},
  author = {AlGhamdi, Fahad and Molina, Giovanni and Diab, Mona and Solorio, Thamar and Hawwari, Abdelati and Soto, Victor and Hirschberg, Julia},
  date = {2016},
  eprint = {1909.13006},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {98--107},
  doi = {10.18653/v1/W16-5812},
  url = {http://arxiv.org/abs/1909.13006},
  urldate = {2023-06-16},
  abstract = {We address the problem of Part of Speech tagging (POS) in the context of linguistic code switching (CS). CS is the phenomenon where a speaker switches between two languages or variants of the same language within or across utterances, known as intra-sentential or inter-sentential CS, respectively. Processing CS data is especially challenging in intra-sentential data given state of the art monolingual NLP technology since such technology is geared toward the processing of one language at a time. In this paper we explore multiple strategies of applying state of the art POS taggers to CS data. We investigate the landscape in two CS language pairs, Spanish-English and Modern Standard Arabic-Arabic dialects. We compare the use of two POS taggers vs. a unified tagger trained on CS data. Our results show that applying a machine learning framework using two state of the art POS taggers achieves better performance compared to all other approaches that we investigate.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/43I4FG8W/AlGhamdi et al. - 2016 - Part of speech tagging for code switched data.pdf;/Users/brono/Zotero/storage/9MEKYIUI/1909.html}
}

@article{angueraSpeakerDiarizationReview2012,
  title = {Speaker {{Diarization}}: {{A Review}} of {{Recent Research}}},
  shorttitle = {Speaker {{Diarization}}},
  author = {Anguera, Xavier and Bozonnet, S. and Evans, N. and Fredouille, C. and Friedland, G. and Vinyals, O.},
  date = {2012-02},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE Trans. Audio Speech Lang. Process.},
  volume = {20},
  number = {2},
  pages = {356--370},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2011.2125954},
  url = {http://ieeexplore.ieee.org/document/6135543/},
  urldate = {2023-01-21},
  abstract = {Speaker diarization is the task of determining “who spoke when?” in an audio or video recording that contains an unknown amount of speech and also an unknown number of speakers. Initially, it was proposed as a research topic related to automatic speech recognition, where speaker diarization serves as an upstream processing step. Over recent years, however, speaker diarization has become an important key technology for many tasks, such as navigation, retrieval, or higher-level inference on audio data. Accordingly, many important improvements in accuracy and robustness have been reported in journals and conferences in the area. The application domains, from broadcast news, to lectures and meetings, vary greatly and pose different problems, such as having access to multiple microphones and multimodal information or overlapping speech. The most recent review of existing technology dates back to 2006 and focuses on the broadcast news domain. In this paper we review the current state-of-the-art, focusing on research developed since 2006 that relates predominantly to speaker diarization for conference meetings. Finally, we present an analysis of speaker diarization performance as reported through the NIST Rich Transcription evaluations on meeting data and identify important areas for future research.},
  langid = {english},
  keywords = {Diarization},
  file = {/Users/brono/Zotero/storage/W3RIHGVD/Anguera et al. - 2012 - Speaker Diarization A Review of Recent Research.pdf}
}

@article{anidjarSpeechMultilingualNatural2023,
  title = {Speech and Multilingual Natural Language Framework for Speaker Change Detection and Diarization},
  author = {Anidjar, Or Haim and Estève, Yannick and Hajaj, Chen and Dvir, Amit and Lapidot, Itshak},
  date = {2023-03-01},
  journaltitle = {Expert Systems with Applications},
  shortjournal = {Expert Systems with Applications},
  volume = {213},
  pages = {119238},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.119238},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417422022564},
  urldate = {2023-04-16},
  abstract = {Speaker Change Detection (SCD) is the problem of splitting an audio-recording by its speaker-turns. Many real-world problems, such as the Speaker Diarization (SD) or automatic speech transcription, are influenced by the quality of the speaker-turns estimation. Previous works have already shown that auxiliary textual information (for mono-lingual systems) can be of great use for detection of speaker-turns and the diarization systems’ performance. In this paper, we suggest a framework for speaker-turn estimation, as well as the determination of clustered speaker identities to the SD system, and examine our approach over a multi-lingual dataset that consists of three mono-lingual datasets—in English, French, and Hebrew. As such, we propose a generic and language-independent framework for the SCD problem that is learned through textual information using state-of-the-art transformer-based techniques and speech-embedding modules. Comprehensive experimental evaluation shows that (i) our multi-lingual SCD framework is competitive enough when compared to a framework over mono-lingual datasets, and that (ii) textual information improves the solution’s quality compared to the speech signal-based approach. In addition, we show that our multi-lingual SCD approach does not harm the performance of SD systems.},
  langid = {english},
  keywords = {Speaker change detection,Speaker diarization,Speaker embedding,Speech recognition,Transformers},
  file = {/Users/brono/Zotero/storage/GXQTGBRU/Anidjar et al. - 2023 - Speech and multilingual natural language framework.pdf;/Users/brono/Zotero/storage/525DKR5W/S0957417422022564.html}
}

@online{anidjarThousandWordsAre2020,
  title = {A {{Thousand Words}} Are {{Worth More Than One Recording}}: {{NLP Based Speaker Change Point Detection}}},
  shorttitle = {A {{Thousand Words}} Are {{Worth More Than One Recording}}},
  author = {Anidjar, O. H. and Hajaj, C. and Dvir, A. and Gilad, I.},
  date = {2020-05-18},
  eprint = {2006.01206},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2006.01206},
  urldate = {2023-04-18},
  abstract = {Speaker Diarization (SD) consists of splitting or segmenting an input audio burst according to speaker identities. In this paper, we focus on the crucial task of the SD problem which is the audio segmenting process and suggest a solution for the Change Point Detection (CPD) problem. We empirically demonstrate the negative correlation between an increase in the number of speakers and the Recall and F1-Score measurements. This negative correlation is shown to be the outcome of a massive experimental evaluation process, which accounts its superiority to recently developed voice based solutions. In order to overcome the number of speakers issue, we suggest a robust solution based on a novel Natural Language Processing (NLP) technique, as well as a metadata features extraction process, rather than a vocal based alone. To the best of our knowledge, we are the first to propose an intelligent NLP based solution that (I) tackles the CPD problem with a dataset in Hebrew, and (II) solves the CPD variant of the SD problem. We empirically show, based on two distinct datasets, that our method is abled to accurately identify the CPDs in an audio burst with 82.12\% and 89.02\% of success in the Recall and F1-score measurements.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/HMJTUAQF/Anidjar et al. - 2020 - A Thousand Words are Worth More Than One Recording.pdf;/Users/brono/Zotero/storage/7FJRSE6G/2006.html}
}

@inproceedings{aronowitzNewAdvancesSpeaker2020a,
  title = {New {{Advances}} in {{Speaker Diarization}}},
  booktitle = {Interspeech 2020},
  author = {Aronowitz, Hagai and Zhu, Weizhong and Suzuki, Masayuki and Kurata, Gakuto and Hoory, Ron},
  date = {2020-10-25},
  pages = {279--283},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2020-1879},
  url = {https://www.isca-speech.org/archive/interspeech_2020/aronowitz20_interspeech.html},
  urldate = {2023-03-13},
  eventtitle = {Interspeech 2020},
  langid = {english},
  file = {/Users/brono/Zotero/storage/2HEMEQAE/Aronowitz et al. - 2020 - New Advances in Speaker Diarization.pdf}
}

@online{baghelDISPLACEChallengeDIarization2023,
  title = {{{DISPLACE Challenge}}: {{DIarization}} of {{SPeaker}} and {{LAnguage}} in {{Conversational Environments}}},
  shorttitle = {{{DISPLACE Challenge}}},
  author = {Baghel, Shikha and Ramoji, Shreyas and Sidharth and H, Ranjana and Singh, Prachi and Jain, Somil and Chowdhuri, Pratik Roy and Kulkarni, Kaustubh and Padhi, Swapnil and Vijayasenan, Deepu and Ganapathy, Sriram},
  date = {2023-03-01},
  eprint = {2303.00830},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2303.00830},
  urldate = {2023-04-04},
  abstract = {The DISPLACE challenge entails a first-of-kind task to perform speaker and language diarization on the same data, as the data contains multi-speaker social conversations in multilingual code-mixed speech. The challenge attempts to benchmark and improve Speaker Diarization (SD) in multilingual settings and Language Diarization (LD) in multi-speaker settings. For this challenge, a natural multilingual, multi-speaker conversational dataset is distributed for development and evaluation purposes. Automatic systems are evaluated on single-channel far-field recordings containing natural code-mix, code-switch, overlap, reverberation, short turns, short pauses, and multiple dialects of the same language. A total of 60 teams from industry and academia have registered for this challenge.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing},
  file = {/Users/brono/Zotero/storage/6R67DBP4/Baghel et al. - 2023 - DISPLACE Challenge DIarization of SPeaker and LAn.pdf;/Users/brono/Zotero/storage/RQQPZIHR/2303.html}
}

@online{baghelDISPLACEChallengeDIarization2023a,
  title = {{{DISPLACE Challenge}}: {{DIarization}} of {{SPeaker}} and {{LAnguage}} in {{Conversational Environments}}},
  shorttitle = {{{DISPLACE Challenge}}},
  author = {Baghel, Shikha and Ramoji, Shreyas and Sidharth and H, Ranjana and Singh, Prachi and Jain, Somil and Chowdhuri, Pratik Roy and Kulkarni, Kaustubh and Padhi, Swapnil and Vijayasenan, Deepu and Ganapathy, Sriram},
  date = {2023-05-23},
  eprint = {2303.00830},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2303.00830},
  urldate = {2023-05-29},
  abstract = {In multilingual societies, social conversations often involve code-mixed speech. The current speech technology may not be well equipped to extract information from multi-lingual multi-speaker conversations. The DISPLACE challenge entails a first-of-kind task to benchmark speaker and language diarization on the same data, as the data contains multi-speaker conversations in multilingual code-mixed speech. The challenge attempts to highlight outstanding issues in speaker diarization (SD) in multilingual settings with code-mixing. Further, language diarization (LD) in multi-speaker settings also introduces new challenges, where the system has to disambiguate speaker switches with code switches. For this challenge, a natural multilingual, multi-speaker conversational dataset is distributed for development and evaluation purposes. The systems are evaluated on single-channel far-field recordings. We also release a baseline system and report the highlights of the system submissions.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing},
  file = {/Users/brono/Zotero/storage/3KCGZAD4/2303.00830.pdf;/Users/brono/Zotero/storage/VDSEV94M/Baghel et al. - 2023 - DISPLACE Challenge DIarization of SPeaker and LAn.pdf;/Users/brono/Zotero/storage/IMP8PJGX/2303.html}
}

@article{baghelDISPLACEChallengeEvaluation,
  title = {{{DISPLACE Challenge Evaluation Plan}}},
  author = {Baghel, Shikha and Ramoji, Shreyas and Jain, Somil and Chowdhuri, Pratik Roy and Ganapathy, Sriram},
  langid = {english},
  file = {/Users/brono/Zotero/storage/X99CNGLN/Baghel et al. - DISPLACE Challenge Evaluation Plan.pdf}
}

@online{barkerFifthCHiMESpeech2018,
  title = {The Fifth '{{CHiME}}' {{Speech Separation}} and {{Recognition Challenge}}: {{Dataset}}, Task and Baselines},
  shorttitle = {The Fifth '{{CHiME}}' {{Speech Separation}} and {{Recognition Challenge}}},
  author = {Barker, Jon and Watanabe, Shinji and Vincent, Emmanuel and Trmal, Jan},
  date = {2018-03-28},
  eprint = {1803.10609},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1803.10609},
  urldate = {2023-03-11},
  abstract = {The CHiME challenge series aims to advance robust automatic speech recognition (ASR) technology by promoting research at the interface of speech and language processing, signal processing , and machine learning. This paper introduces the 5th CHiME Challenge, which considers the task of distant multi-microphone conversational ASR in real home environments. Speech material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech and recorded by 6 Kinect microphone arrays and 4 binaural microphone pairs. The challenge features a single-array track and a multiple-array track and, for each track, distinct rankings will be produced for systems focusing on robustness with respect to distant-microphone capture vs. systems attempting to address all aspects of the task including conversational language modeling. We discuss the rationale for the challenge and provide a detailed description of the data collection procedure, the task, and the baseline systems for array synchronization, speech enhancement, and conventional and end-to-end ASR.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/HFDPJEQL/Barker et al. - 2018 - The fifth 'CHiME' Speech Separation and Recognitio.pdf;/Users/brono/Zotero/storage/ANN6XNI7/1803.html}
}

@inproceedings{basuOverviewSpeakerDiarization2016,
  title = {An Overview of Speaker Diarization: {{Approaches}}, Resources and Challenges},
  shorttitle = {An Overview of Speaker Diarization},
  booktitle = {2016 {{Conference}} of {{The Oriental Chapter}} of {{International Committee}} for {{Coordination}} and {{Standardization}} of {{Speech Databases}} and {{Assessment Techniques}} ({{O-COCOSDA}})},
  author = {Basu, Joyanta and Khan, Soma and Roy, Rajib and Pal, Madhab and Basu, Tulika and Bepari, Milton Samirakshma and Basu, Tapan Kumar},
  date = {2016-10},
  pages = {166--171},
  publisher = {{IEEE}},
  location = {{Bali, Indonesia}},
  doi = {10.1109/ICSDA.2016.7919005},
  url = {http://ieeexplore.ieee.org/document/7919005/},
  urldate = {2023-03-21},
  eventtitle = {2016 {{Conference}} of {{The Oriental Chapter}} of {{International Committee}} for {{Coordination}} and {{Standardization}} of {{Speech Databases}} and {{Assessment Techniques}} ({{O-COCOSDA}})},
  isbn = {978-1-5090-3516-8},
  file = {/Users/brono/Zotero/storage/27WBBJ4N/Basu et al. - 2016 - An overview of speaker diarization Approaches, re.pdf}
}

@inproceedings{bawaAccommodationConversationalCodeChoice2018,
  title = {Accommodation of {{Conversational Code-Choice}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Computational Approaches}} to {{Linguistic Code-Switching}}},
  author = {Bawa, Anshul and Choudhury, Monojit and Bali, Kalika},
  date = {2018-07},
  pages = {82--91},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/W18-3210},
  url = {https://aclanthology.org/W18-3210},
  urldate = {2023-04-23},
  abstract = {Bilingual speakers often freely mix languages. However, in such bilingual conversations, are the language choices of the speakers coordinated? How much does one speaker's choice of language affect other speakers? In this paper, we formulate code-choice as a linguistic style, and show that speakers are indeed sensitive to and accommodating of each other's code-choice. We find that the saliency or markedness of a language in context directly affects the degree of accommodation observed. More importantly, we discover that accommodation of code-choices persists over several conversational turns. We also propose an alternative interpretation of conversational accommodation as a retrieval problem, and show that the differences in accommodation characteristics of code-choices are based on their markedness in context.},
  eventtitle = {{{ACL}} 2018},
  file = {/Users/brono/Zotero/storage/C67BNRCX/Bawa et al. - 2018 - Accommodation of Conversational Code-Choice.pdf}
}

@online{BetterBrainsBabies,
  title = {Better {{Brains}} for {{Babies}} - {{Infant-Directed Speech}} - {{YouTube}}},
  url = {https://www.youtube.com/watch?v=F9qS_AZCdgM},
  urldate = {2023-04-21},
  abstract = {Enjoy the videos and music you love, upload original content, and share it all with friends, family, and the world on YouTube.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/KUXYX74E/watch.html}
}

@article{bhangaleReviewSpeechProcessing2021,
  title = {A Review on Speech Processing Using Machine Learning Paradigm},
  author = {Bhangale, Kishor Barasu and Mohanaprasad, K.},
  date = {2021-06},
  journaltitle = {International Journal of Speech Technology},
  shortjournal = {Int J Speech Technol},
  volume = {24},
  number = {2},
  pages = {367--388},
  issn = {1381-2416, 1572-8110},
  doi = {10.1007/s10772-021-09808-0},
  url = {https://link.springer.com/10.1007/s10772-021-09808-0},
  urldate = {2023-04-09},
  abstract = {Speech processing plays a crucial role in many signal processing applications, while the last decade has bought gigantic evolution based on machine learning prototype. Speech processing has a close relationship with computer linguistics, human–machine interaction, natural language processing, and psycholinguistics. This review article majorly discusses the feature extraction techniques and machine learning classifiers employed in speech processing and recognition activities. The performance of several machine learning techniques is validated for speech emotion recognition application on Berlin EmoDB database. Further, it gives the broad application areas and challenges in machine learning for speech processing.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/RJ6ZIKWL/Bhangale and Mohanaprasad - 2021 - A review on speech processing using machine learni.pdf}
}

@inproceedings{boothEvaluatingImprovingChildDirected2020,
  title = {Evaluating and {{Improving Child-Directed Automatic Speech Recognition}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Booth, Eric and Carns, Jake and Kennington, Casey and Rafla, Nader},
  date = {2020-05},
  pages = {6340--6345},
  publisher = {{European Language Resources Association}},
  location = {{Marseille, France}},
  url = {https://aclanthology.org/2020.lrec-1.778},
  urldate = {2023-04-17},
  abstract = {Speech recognition has seen dramatic improvements in the last decade, though those improvements have focused primarily on adult speech. In this paper, we assess child-directed speech recognition and leverage a transfer learning approach to improve child-directed speech recognition by training the recent DeepSpeech2 model on adult data, then apply additional tuning to varied amounts of child speech data. We evaluate our model using the CMU Kids dataset as well as our own recordings of child-directed prompts. The results from our experiment show that even a small amount of child audio data improves significantly over a baseline of adult-only or child-only trained models. We report a final general Word-Error-Rate of 29\% over a baseline of 62\% that uses the adult-trained model. Our analyses show that our model adapts quickly using a small amount of data and that the general child model works better than school grade-specific models. We make available our trained model and our data collection tool.},
  eventtitle = {{{LREC}} 2020},
  isbn = {979-10-95546-34-4},
  langid = {english},
  file = {/Users/brono/Zotero/storage/HNS8MEUS/Booth et al. - 2020 - Evaluating and Improving Child-Directed Automatic .pdf}
}

@article{borsdorfGlobalPhoneMixtoSeparateOut2020,
  title = {{{GlobalPhone Mix-to-Separate}} out of 2: {{A Multilingual}} 2000 {{Speakers Mixtures Database}} for {{Speech Separation}}},
  author = {Borsdorf, Marvin and Xu, Chenglin and Li, Haizhou and Schultz, Tanja},
  date = {2020},
  abstract = {Monaural speech separation has been well studied on various databases. However, these databases mostly concern English speech. Research in multi-speaker scenarios, such as speech recognition, speaker recognition, speaker diarization, and speech separation calls for speaker mixtures databases comprising multiple languages. In this paper, we propose a new extensive multilingual database for speech separation tasks derived from the GlobalPhone 2000 Speaker Package, called “GlobalPhone Mix-to-Separate out of 2” (GlobalPhoneMS2). We describe the construction of the database and conduct speech separation experiments in monolingual and multilingual as well as seen and unseen languages settings. When trained on a multilingual dataset, the networks improve their performances for unseen languages, and across almost all seen languages. We show that replacing a monolingual dataset with a trilingual one, while keeping the data size roughly the same, helps to improve the performance in most cases. We attribute this to a larger diversity in speech, language, speaker, and recording characteristics. Based on the GlobalPhoneMS2 database, speech separation results for two-speaker mixing scenarios are reported in 22 spoken languages for the first time.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/XFFNQF86/Borsdorf et al. - GlobalPhone Mix-to-Separate out of 2 A Multilingu.pdf}
}

@inproceedings{bredinPyannoteAudioNeural2020,
  title = {Pyannote.{{Audio}}: {{Neural Building Blocks}} for {{Speaker Diarization}}},
  shorttitle = {Pyannote.{{Audio}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bredin, Herve and Yin, Ruiqing and Coria, Juan Manuel and Gelly, Gregory and Korshunov, Pavel and Lavechin, Marvin and Fustes, Diego and Titeux, Hadrien and Bouaziz, Wassim and Gill, Marie-Philippe},
  date = {2020-05},
  pages = {7124--7128},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9052974},
  url = {https://ieeexplore.ieee.org/document/9052974/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5},
  file = {/Users/brono/Zotero/storage/TXPR5E43/Bredin et al. - 2020 - Pyannote.Audio Neural Building Blocks for Speaker.pdf}
}

@inproceedings{brouxS4DSpeakerDiarization2018,
  title = {{{S4D}}: {{Speaker Diarization Toolkit}} in {{Python}}},
  shorttitle = {{{S4D}}},
  booktitle = {Interspeech 2018},
  author = {Broux, Pierre-Alexandre and Desnous, Florent and Larcher, Anthony and Petitrenaud, Simon and Carrive, Jean and Meignier, Sylvain},
  date = {2018-09-02},
  pages = {1368--1372},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1232},
  url = {https://www.isca-speech.org/archive/interspeech_2018/broux18_interspeech.html},
  urldate = {2023-04-07},
  abstract = {In this paper, we present S4D, a new open-source Python toolkit dedicated to speaker diarization. S4D provides various state-ofthe-art components and the possibility to easily develop end-toend diarization prototype systems. S4D offers a large panel of clustering, segmentation, scoring and visualization algorithms. S4D has been thought to be easily understood, installed, modified and used in order to allow fast transfers of diarization technologies to industry and facilitate development of new approaches. Examples, benchmarks on standard tasks and tutorials are provided in this paper. S4D is an extension of the opensource toolkit for speaker recognition: SIDEKIT.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/Z86LJ7UQ/Broux et al. - 2018 - S4D Speaker Diarization Toolkit in Python.pdf}
}

@article{bulgarelliLookWhoTalking2020,
  title = {Look Who’s Talking: {{A}} Comparison of Automated and Human-Generated Speaker Tags in Naturalistic Day-Long Recordings},
  shorttitle = {Look Who’s Talking},
  author = {Bulgarelli, Federica and Bergelson, Elika},
  date = {2020-04},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {52},
  number = {2},
  pages = {641--653},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01265-7},
  url = {http://link.springer.com/10.3758/s13428-019-01265-7},
  urldate = {2023-04-19},
  abstract = {The LENA system has revolutionized research on language acquisition, providing both a wearable device to collect daylong recordings of children’s environments, and a set of automated outputs that process, identify, and classify speech using proprietary algorithms. This output includes information about input sources (e.g., adult male, electronics). While this system has been tested across a variety of settings, here we delve deeper into validating the accuracy and reliability of LENA’s automated diarization, i.e., tags of who is talking. Specifically, we compare LENA’s output with a gold standard set of manually generated talker tags from a dataset of 88 day-long recordings, taken from 44 infants at 6 and 7 months, which includes 57,983 utterances. We compare accuracy across a range of classifications from the original Lena Technical Report, alongside a set of analyses examining classification accuracy by utterance type (e.g., declarative, singing). Consistent with previous validations, we find overall high agreement between the human and LENA-generated speaker tags for adult speech in particular, with poorer performance identifying child, overlap, noise, and electronic speech (accuracy range across all measures: 0–92\%). We discuss several clear benefits of using this automated system alongside potential caveats based on the error patterns we observe, concluding with implications for research using LENA-generated speaker tags.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/7QDIJ48X/Bulgarelli and Bergelson - 2020 - Look who’s talking A comparison of automated and .pdf}
}

@inproceedings{bullockOverlapAwareDiarizationResegmentation2020,
  title = {Overlap-{{Aware Diarization}}: {{Resegmentation Using Neural End-to-End Overlapped Speech Detection}}},
  shorttitle = {Overlap-{{Aware Diarization}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Bullock, Latane and Bredin, Herve and Garcia-Perera, Leibny Paola},
  date = {2020-05},
  pages = {7114--7118},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9053096},
  url = {https://ieeexplore.ieee.org/document/9053096/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5},
  file = {/Users/brono/Zotero/storage/8FBLS3H9/Bullock et al. - 2020 - Overlap-Aware Diarization Resegmentation Using Ne.pdf}
}

@online{CallPapersInterspeech,
  title = {Call for {{Papers}} – {{Interspeech}} 2023},
  url = {https://www.interspeech2023.org/call-for-papers/},
  urldate = {2023-04-10},
  langid = {american},
  file = {/Users/brono/Zotero/storage/TVX9XY52/call-for-papers.html}
}

@article{cameron-faulknerConstructionBasedAnalysis2003,
  title = {A Construction Based Analysis of Child Directed Speech},
  author = {Cameron-Faulkner, Thea and Lieven, Elena and Tomasello, Michael},
  date = {2003},
  journaltitle = {Cognitive Science},
  volume = {27},
  number = {6},
  pages = {843--873},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2706_2},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2706_2},
  urldate = {2023-04-02},
  abstract = {The child directed speech of twelve English-speaking motherswas analyzed in terms of utterance-level constructions. First, the mothers' utterances were categorized in terms of general constructional categories such as Wh-questions, copulas and transitives. Second, mothers' utterances within these categories were further specified in terms of the initial words that framed the utterance, item-based phrases such as Are you …, I'll …, It's …, Let's …, What did … The findings were: (i) overall, only about 15\% of all maternal utterances had SVO form (most were questions, imperatives, copulas, and fragments); (ii) 51\% of all maternal utterances began with one of 52 item-based phrases, mostly consisting of two words or morphemes (45\% began with one of just 17 words); and (iii) children used many of these same item-based phrases, in some cases at a rate that correlated highly with their own mother's frequency of use. We suggest that analyses of adult–child linguistic interaction should take into account not just general constructional categories, but also the item-based constructions that adults and children use and the frequency with which they use them.},
  langid = {english},
  keywords = {Constructions,Input,Language development,Syntax},
  file = {/Users/brono/Zotero/storage/F5X4SIL7/Cameron-Faulkner et al. - 2003 - A construction based analysis of child directed sp.pdf;/Users/brono/Zotero/storage/BFSXRZFH/s15516709cog2706_2.html}
}

@inproceedings{caughlinEndtoEndNeuralNetwork2021,
  title = {End-to-{{End Neural Network}} for {{Feature Extraction}} and {{Cancer Diagnosis}} of {{In Vivo Fluorescence Lifetime Images}} of {{Oral Lesions}}},
  booktitle = {2021 43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  author = {Caughlin, Kayla and Duran-Sierra, Elvis and Cheng, Shuna and Cuenca, Rodrigo and Ahmed, Beena and Ji, Jim and Yakovlev, Vladislav V. and Martinez, Mathias and Al-Khalil, Moustafa and Al-Enazi, Hussain and Jo, Javier A. and Busso, Carlos},
  date = {2021-11-01},
  pages = {3894--3897},
  publisher = {{IEEE}},
  location = {{Mexico}},
  doi = {10.1109/EMBC46164.2021.9629739},
  url = {https://ieeexplore.ieee.org/document/9629739/},
  urldate = {2023-02-01},
  abstract = {In contrast to previous studies that focused on classical machine learning algorithms and hand-crafted features, we present an end-to-end neural network classification method able to accommodate lesion heterogeneity for improved oral cancer diagnosis using multispectral autofluorescence lifetime imaging (maFLIM) endoscopy. Our method uses an autoencoder framework jointly trained with a classifier designed to handle overfitting problems with reduced databases, which is often the case in healthcare applications. The autoencoder guides the feature extraction process through the reconstruction loss and enables the potential use of unsupervised data for domain adaptation and improved generalization. The classifier ensures the features extracted are task-specific, providing discriminative information for the classification task. The data-driven feature extraction method automatically generates task-specific features directly from fluorescence decays, eliminating the need for iterative signal reconstruction. We validate our proposed neural network method against support vector machine (SVM) baselines, with our method showing a 6.5\%-8.3\% increase in sensitivity. Our results show that neural networks that implement data-driven feature extraction provide superior results and enable the capacity needed to target specific issues, such as inter-patient variability and the heterogeneity of oral lesions.},
  eventtitle = {2021 43rd {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} \& {{Biology Society}} ({{EMBC}})},
  isbn = {978-1-72811-179-7},
  langid = {english},
  file = {/Users/brono/Zotero/storage/XFQ63357/Caughlin et al. - 2021 - End-to-End Neural Network for Feature Extraction a.pdf}
}

@video{centerforlanguageandspeechclsp@jhuMultilingualCodeSwitchingSpeech2022a,
  entrysubtype = {video},
  title = {Multilingual and {{Code-Switching Speech Recognition}}},
  editor = {{Center for Language and Speech (CLSP) @ JHU}},
  editortype = {director},
  date = {2022-11-05},
  url = {https://www.youtube.com/watch?v=ErYZWAAs7Q0},
  urldate = {2023-04-18},
  abstract = {Multilingual and Code-Switching Speech Recognition Multilingual and code-switching speech recognition are important challenges due to the growing adoption of personal assistant devices and smartphones. With the rise of globalisation, there is an increasing demand for multilingual ASR, handling language and dialectal variation of spoken content. Recent studies show its efficacy over monolingual systems. The prevalence of code-switching in spoken content has enforced automatic speech recognition (ASR) systems to handle mixed input. Yet, designing a CS-ASR has many challenges, mainly due to the data scarcity, grammatical structure complexity and mismatch along with unbalanced language usage distribution. We propose to study the multi-lingual and code-switching phenomena in two frequently spoken language sets (English and Arabic), along with low resourced and indigenous languages.: – Arabic and English: 1,000 hours of multi-dialectal Arabic Data (MGB-2), English (Tedlium, librispeech). – Minority languages of English speaking countries: Gaelic (British Isles), Maori (New Zealand) and many indigenous languages of the US and Canada – Languages of sub-Saharan Africa, eg. Zulu, Xhosa, Yoruba The code-switching study will feature both intersentential (switching between-utterances) and intrasentential (within utterances). The evaluation of the designed system and the analysis of the phenomena will be driven based on real test cases, collected from real meetings and interviews. Our proposal, for the summer workshop, focuses on investigating novel techniques to build practical large vocabulary continuous speech recognition (LVCSR) systems capable of dealing with both the monolingual and code-switching spoken utterances. We aim to explore data augmentation and state of the art modelling techniques – using transfer learning and self supervised learning – to deal with the lack of balanced transcribed data, for multilingual and code-switching. Moreover, we also aim to address the challenge of evaluating code-switching ASR output. The summer school will include four work packages running simultaneously while sharing outcomes to achieve desired goals. The work packages are: WP1: Design a multilingual ASR with code-switching capabilities, in this WP, we will focus on pre-trained models and self-supervised models. WP2: Handle low-resourced languages/dialects by generating synthetic code-switching data covering synthetic textual and speech data, upholding language dependent construction and triggers. WP3: Build a robust evaluation measure considering the mixed script output system. The current systems are evaluated based on transliterated word error rate and character error rate. This method lacks generalization, especially when there is code-mix within the same word. WP4: Understand where/why code-switching happens in speech analysis for system/human code-switching points. This will address issues such as: complex social factors, dominant language may be used for education literacy, is the code-switching a topic/domain dependent. This WP will offer more insights of the challenge rather than building a better system to improve accuracy.}
}

@article{chanAutomaticRecognitionCantoneseEnglish,
  title = {Automatic {{Recognition}} of {{Cantonese-English Code-Mixing Speech}}},
  author = {Chan, Joyce Y C and Cao, Houwei and Ching, P C and Lee, Tan},
  abstract = {Code-mixing is a common phenomenon in bilingual societies. It refers to the intra-sentential switching of two different languages in a spoken utterance. This paper presents the first study on automatic recognition of Cantonese-English code-mixing speech, which is common in Hong Kong. This study starts with the design and compilation of code-mixing speech and text corpora. The problems of acoustic modeling, language modeling, and language boundary detection are investigated. Subsequently, a large-vocabulary code-mixing speech recognition system is developed based on a two-pass decoding algorithm. For acoustic modeling, it is shown that cross-lingual acoustic models are more appropriate than language-dependent models. The language models being used are character tri-grams, in which the embedded English words are grouped into a small number of classes. Language boundary detection is done either by exploiting the phonological and lexical differences between the two languages or is done based on the result of cross-lingual speech recognition. The language boundary information is used to re-score the hypothesized syllables or words in the decoding process. The proposed code-mixing speech recognition system attains the accuracies of 56.4\% and 53.0\% for the Cantonese syllables and English words in code-mixing utterances.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/GELWM8TT/Chan et al. - Automatic Recognition of Cantonese-English Code-Mi.pdf}
}

@inproceedings{chanDevelopmentCantoneseEnglishCodemixing2005,
  title = {Development of a {{Cantonese-English}} Code-Mixing Speech Corpus},
  booktitle = {Interspeech 2005},
  author = {Chan, Joyce Y. C. and Ching, P. C. and Lee, Tan},
  date = {2005-09-04},
  pages = {1533--1536},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2005-450},
  url = {https://www.isca-speech.org/archive/interspeech_2005/chan05b_interspeech.html},
  urldate = {2023-06-12},
  abstract = {This paper describes the design and compilation of the CUMIX Cantonese-English code-mixing speech corpus. Code-mixing is a common phenomenon in many bilingual societies and it usually involves at least two different languages within one utterance. In Hong Kong, people usually mix English words and phrases with Cantonese in their daily conversation. Although there are many monolingual corpora of Cantonese and English, code-mixing speech database of these two languages is not available. The aim of developing this corpus is to study of the effect of Cantonese accents in English, the design of effective language boundary detection algorithm in code-mixing utterances [1], and evaluation of the performance of code-mixing speech recognizers.},
  eventtitle = {Interspeech 2005},
  langid = {english},
  file = {/Users/brono/Zotero/storage/J6WUD88P/Chan et al. - 2005 - Development of a Cantonese-English code-mixing spe.pdf}
}

@online{chenContinuousSpeechSeparation2020,
  title = {Continuous Speech Separation: Dataset and Analysis},
  shorttitle = {Continuous Speech Separation},
  author = {Chen, Zhuo and Yoshioka, Takuya and Lu, Liang and Zhou, Tianyan and Meng, Zhong and Luo, Yi and Wu, Jian and Xiao, Xiong and Li, Jinyu},
  date = {2020-05-07},
  eprint = {2001.11482},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2001.11482},
  urldate = {2023-03-11},
  abstract = {This paper describes a dataset and protocols for evaluating continuous speech separation algorithms. Most prior studies on speech separation use pre-segmented signals of artificially mixed speech utterances which are mostly \textbackslash emph\{fully\} overlapped, and the algorithms are evaluated based on signal-to-distortion ratio or similar performance metrics. However, in natural conversations, a speech signal is continuous, containing both overlapped and overlap-free components. In addition, the signal-based metrics have very weak correlations with automatic speech recognition (ASR) accuracy. We think that not only does this make it hard to assess the practical relevance of the tested algorithms, it also hinders researchers from developing systems that can be readily applied to real scenarios. In this paper, we define continuous speech separation (CSS) as a task of generating a set of non-overlapped speech signals from a \textbackslash textit\{continuous\} audio stream that contains multiple utterances that are \textbackslash emph\{partially\} overlapped by a varying degree. A new real recorded dataset, called LibriCSS, is derived from LibriSpeech by concatenating the corpus utterances to simulate a conversation and capturing the audio replays with far-field microphones. A Kaldi-based ASR evaluation protocol is also established by using a well-trained multi-conditional acoustic model. By using this dataset, several aspects of a recently proposed speaker-independent CSS algorithm are investigated. The dataset and evaluation scripts are available to facilitate the research in this direction.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/MHIELG3R/Chen et al. - 2020 - Continuous speech separation dataset and analysis.pdf;/Users/brono/Zotero/storage/VLVXJ6SC/2001.html}
}

@online{chengConversationalShortphraseSpeaker2022,
  title = {The {{Conversational Short-phrase Speaker Diarization}} ({{CSSD}}) {{Task}}: {{Dataset}}, {{Evaluation Metric}} and {{Baselines}}},
  shorttitle = {The {{Conversational Short-phrase Speaker Diarization}} ({{CSSD}}) {{Task}}},
  author = {Cheng, Gaofeng and Chen, Yifan and Yang, Runyan and Li, Qingxuan and Yang, Zehui and Ye, Lingxuan and Zhang, Pengyuan and Zhang, Qingqing and Xie, Lei and Qian, Yanmin and Lee, Kong Aik and Yan, Yonghong},
  date = {2022-08-16},
  eprint = {2208.08042},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2208.08042},
  urldate = {2023-03-13},
  abstract = {The conversation scenario is one of the most important and most challenging scenarios for speech processing technologies because people in conversation respond to each other in a casual style. Detecting the speech activities of each person in a conversation is vital to downstream tasks, like natural language processing, machine translation, etc. People refer to the detection technology of "who speak when" as speaker diarization (SD). Traditionally, diarization error rate (DER) has been used as the standard evaluation metric of SD systems for a long time. However, DER fails to give enough importance to short conversational phrases, which are short but important on the semantic level. Also, a carefully and accurately manually-annotated testing dataset suitable for evaluating the conversational SD technologies is still unavailable in the speech community. In this paper, we design and describe the Conversational Short-phrases Speaker Diarization (CSSD) task, which consists of training and testing datasets, evaluation metric and baselines. In the dataset aspect, despite the previously open-sourced 180-hour conversational MagicData-RAMC dataset, we prepare an individual 20-hour conversational speech test dataset with carefully and artificially verified speakers timestamps annotations for the CSSD task. In the metric aspect, we design the new conversational DER (CDER) evaluation metric, which calculates the SD accuracy at the utterance level. In the baseline aspect, we adopt a commonly used method: Variational Bayes HMM x-vector system, as the baseline of the CSSD task. Our evaluation metric is publicly available at https://github.com/SpeechClub/CDER\_Metric.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/3ERYVLWY/Cheng et al. - 2022 - The Conversational Short-phrase Speaker Diarizatio.pdf;/Users/brono/Zotero/storage/D6GFAN2V/2208.html}
}

@article{chenGREATSHUSpeakerDiarization,
  title = {{{GREAT-SHU Speaker Diarization System}} for {{DIHARD III Challenge}}},
  author = {Chen, Zhiyong and Ren, Zongze and Ma, Runze and Wu, Bo and Xu, Shugong},
  abstract = {In this paper, we present the submitted system for the third DIHARD Speech Diarization Challenge from the GREAT-SHU team. Our diarization system includes multiple modules, namely voice activity detection (VAD), segmentation, speaker embedding extraction, similarity scoring, matrix refinement, and clustering. For each module, we explore different techniques to enhance performance. Our final submission employs the oracle VAD, uniform-refined segmentation, the Deep ECAPATDNN based speaker embedding, the cosine similarity based similarity scoring and spectral clustering. We did not implement resegmentation stage and overlap detection. Our proposed system achieves 22\% DER in Track1 and 39\% DER in Track2. We believe that the diarization task is still challenging.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/3CY584BI/Chen et al. - GREAT-SHU Speaker Diarization System for DIHARD II.pdf}
}

@book{ChilddirectedSpeechIts2018,
  title = {Child-Directed Speech and Its Role in Language Acquisition},
  date = {2018-04-07},
  url = {https://www.grin.com/document/419408},
  urldate = {2023-04-02},
  abstract = {Child-directed speech and its role in language acquisition - English Language and Literature Studies / Other - Term Paper 2016 - ebook 12.99 € - GRIN},
  isbn = {978-3-668-68067-8 978-3-668-68068-5},
  langid = {english}
}

@inproceedings{chungSpotConversationSpeaker2020,
  title = {Spot the Conversation: Speaker Diarisation in the Wild},
  shorttitle = {Spot the Conversation},
  booktitle = {Interspeech 2020},
  author = {Chung, Joon Son and Huh, Jaesung and Nagrani, Arsha and Afouras, Triantafyllos and Zisserman, Andrew},
  date = {2020-10-25},
  eprint = {2007.01216},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {299--303},
  doi = {10.21437/Interspeech.2020-2337},
  url = {http://arxiv.org/abs/2007.01216},
  urldate = {2023-03-11},
  abstract = {The goal of this paper is speaker diarisation of videos collected 'in the wild'. We make three key contributions. First, we propose an automatic audio-visual diarisation method for YouTube videos. Our method consists of active speaker detection using audio-visual methods and speaker verification using self-enrolled speaker models. Second, we integrate our method into a semi-automatic dataset creation pipeline which significantly reduces the number of hours required to annotate videos with diarisation labels. Finally, we use this pipeline to create a large-scale diarisation dataset called VoxConverse, collected from 'in the wild' videos, which we will release publicly to the research community. Our dataset consists of overlapping speech, a large and diverse speaker pool, and challenging background conditions.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Image and Video Processing,voxconverse},
  file = {/Users/brono/Zotero/storage/P3LV7Y6X/Chung et al. - 2020 - Spot the conversation speaker diarisation in the .pdf;/Users/brono/Zotero/storage/XD5LM7BB/2007.html}
}

@online{CodaLabCompetition,
  title = {{{CodaLab}} - {{Competition}}},
  url = {https://codalab.lisn.upsaclay.fr/competitions/10588#participate},
  urldate = {2023-04-10},
  file = {/Users/brono/Zotero/storage/JCWPVPIV/10588.html}
}

@inproceedings{coriaOverlapAwareLowLatencyOnline2021,
  title = {Overlap-{{Aware Low-Latency Online Speaker Diarization Based}} on {{End-to-End Local Segmentation}}},
  booktitle = {2021 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Coria, Juan M. and Bredin, Herve and Ghannay, Sahar and Rosset, Sophie},
  date = {2021-12-13},
  pages = {1139--1146},
  publisher = {{IEEE}},
  location = {{Cartagena, Colombia}},
  doi = {10.1109/ASRU51503.2021.9688044},
  url = {https://ieeexplore.ieee.org/document/9688044/},
  urldate = {2023-03-23},
  eventtitle = {2021 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-66543-739-4},
  file = {/Users/brono/Zotero/storage/3G54SBYP/Coria et al. - 2021 - Overlap-Aware Low-Latency Online Speaker Diarizati.pdf}
}

@inproceedings{cristiaTalkerDiarizationWild2018,
  title = {Talker {{Diarization}} in the {{Wild}}: The {{Case}} of {{Child-centered Daylong Audio-recordings}}},
  shorttitle = {Talker {{Diarization}} in the {{Wild}}},
  booktitle = {Interspeech 2018},
  author = {Cristia, Alejandrina and Ganesh, Shobhana and Casillas, Marisa and Ganapathy, Sriram},
  date = {2018-09-02},
  pages = {2583--2587},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-2078},
  url = {https://www.isca-speech.org/archive/interspeech_2018/cristia18_interspeech.html},
  urldate = {2023-04-17},
  abstract = {Speaker diarization (answering ’who spoke when’) is a widely researched subject within speech technology. Numerous experiments have been run on datasets built from broadcast news, meeting data, and call centers—the task sometimes appears close to being solved. Much less work has begun to tackle the hardest diarization task of all: spontaneous conversations in real-world settings. Such diarization would be particularly useful for studies of language acquisition, where researchers investigate the speech children produce and hear in their daily lives. In this paper, we study audio gathered with a recorder worn by small children as they went about their normal days. As a result, each child was exposed to different acoustic environments with a multitude of background noises and a varying number of adults and peers. The inconsistency of speech and noise within and across samples poses a challenging task for speaker diarization systems, which we tackled via retraining and data augmentation techniques. We further studied sources of structured variation across raw audio files, including the impact of speaker type distribution, proportion of speech from children, and child age on diarization performance. We discuss the extent to which these findings might generalize to other samples of speech in the wild.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/MPFK9GK9/Cristia et al. - 2018 - Talker Diarization in the Wild the Case of Child-.pdf}
}

@inproceedings{dau-chenglyuSpeechRecognitionCodeSwitching2006,
  title = {Speech {{Recognition}} on {{Code-Switching Among}} the {{Chinese Dialects}}},
  booktitle = {2006 {{IEEE International Conference}} on {{Acoustics Speed}} and {{Signal Processing Proceedings}}},
  author = {{Dau-cheng Lyu} and {Ren-yuan Lyu} and {Yuang-chin Chiang} and {Chun-nan Hsu}},
  date = {2006},
  volume = {1},
  pages = {I-1105-I-1108},
  publisher = {{IEEE}},
  location = {{Toulouse, France}},
  doi = {10.1109/ICASSP.2006.1660218},
  url = {http://ieeexplore.ieee.org/document/1660218/},
  urldate = {2023-04-16},
  abstract = {We propose an integrated approach to do automatic speech recognition on code-switching utterances, where speakers switch back and forth between at least 2 languages. This one-pass framework avoids the degradation of accuracy due to the imperfectly intermediate decisions of language detection and language identification. It is based on a three-layer recognition scheme, which consists of a mixed-language HMM-based acoustic model, a knowledge-based plus data-driven probabilistic pronunciation model, and a tree-structured searching net. The traditional multi-pass recognizer including language boundary detection, language identification and language-dependent speech recognition is also implemented for comparison. Experimental results show that the proposed approach, with a much simpler recognition scheme, could achieve as high accuracy as that could be achieved by using the traditional approach.},
  eventtitle = {2006 {{IEEE International Conference}} on {{Acoustics Speed}} and {{Signal Processing}}},
  isbn = {978-1-4244-0469-8},
  langid = {english},
  file = {/Users/brono/Zotero/storage/QZAF22PP/Dau-cheng Lyu et al. - 2006 - Speech Recognition on Code-Switching Among the Chi.pdf}
}

@inproceedings{dawalatabadECAPATDNNEmbeddingsSpeaker2021,
  title = {{{ECAPA-TDNN Embeddings}} for {{Speaker Diarization}}},
  booktitle = {Interspeech 2021},
  author = {Dawalatabad, Nauman and Ravanelli, Mirco and Grondin, François and Thienpondt, Jenthe and Desplanques, Brecht and Na, Hwidong},
  date = {2021-08-30},
  eprint = {2104.01466},
  eprinttype = {arxiv},
  eprintclass = {eess},
  pages = {3560--3564},
  doi = {10.21437/Interspeech.2021-941},
  url = {http://arxiv.org/abs/2104.01466},
  urldate = {2023-03-13},
  abstract = {Learning robust speaker embeddings is a crucial step in speaker diarization. Deep neural networks can accurately capture speaker discriminative characteristics and popular deep embeddings such as x-vectors are nowadays a fundamental component of modern diarization systems. Recently, some improvements over the standard TDNN architecture used for x-vectors have been proposed. The ECAPA-TDNN model, for instance, has shown impressive performance in the speaker verification domain, thanks to a carefully designed neural model. In this work, we extend, for the first time, the use of the ECAPA-TDNN model to speaker diarization. Moreover, we improved its robustness with a powerful augmentation scheme that concatenates several contaminated versions of the same signal within the same training batch. The ECAPA-TDNN model turned out to provide robust speaker embeddings under both close-talking and distant-talking conditions. Our results on the popular AMI meeting corpus show that our system significantly outperforms recently proposed approaches.},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/87DH7IMC/Dawalatabad et al. - 2021 - ECAPA-TDNN Embeddings for Speaker Diarization.pdf;/Users/brono/Zotero/storage/IDCTKYQX/2104.html}
}

@article{dehakLowdimensionalSpeechRepresentation,
  title = {Low-Dimensional Speech Representation Based on {{Factor Analysis}} and Its Applications!},
  author = {Dehak, Najim and Shum, Stephen},
  journaltitle = {Emotion recognition},
  langid = {english},
  keywords = {I-vector},
  file = {/Users/brono/Zotero/storage/WUCF64HM/Dehak and Shum - Low-dimensional speech representation based on Fac.pdf}
}

@inproceedings{desplanquesECAPATDNNEmphasizedChannel2020,
  title = {{{ECAPA-TDNN}}: {{Emphasized Channel Attention}}, {{Propagation}} and {{Aggregation}} in {{TDNN Based Speaker Verification}}},
  shorttitle = {{{ECAPA-TDNN}}},
  booktitle = {Interspeech 2020},
  author = {Desplanques, Brecht and Thienpondt, Jenthe and Demuynck, Kris},
  date = {2020-10-25},
  eprint = {2005.07143},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {3830--3834},
  doi = {10.21437/Interspeech.2020-2650},
  url = {http://arxiv.org/abs/2005.07143},
  urldate = {2023-04-14},
  abstract = {Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/A4LH2FIU/Desplanques et al. - 2020 - ECAPA-TDNN Emphasized Channel Attention, Propagat.pdf;/Users/brono/Zotero/storage/IDCNNVDG/2005.html}
}

@article{deucharMiamiCorpusDocumentation,
  title = {The {{Miami Corpus}}: {{Documentation File}}},
  author = {Deuchar, Margaret},
  url = {http://bangortalk.org.uk/speakers.php?c=miami},
  langid = {english},
  file = {/Users/brono/Zotero/storage/WV4NUCTS/Deuchar - The Miami Corpus Documentation File.pdf}
}

@article{deyHindiEnglishCodeSwitching,
  title = {A {{Hindi-English}} Code Switching Corpus},
  author = {Dey, Anik and Fung, Pascale},
  abstract = {The aim of this paper is to investigate the rules and constraints of code-switching (CS) in Hindi-English mixed language data. In this paper, we’ll discuss how we collected the mixed language corpus. This corpus is primarily made up of student interview speech. The speech was manually transcribed and verified by bilingual speakers of Hindi and English. The code-switching cases in the corpus are discussed and the reasons for code-switching are explained.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/DAIG8T8Y/Dey and Fung - A Hindi-English code switching corpus.pdf}
}

@inproceedings{diezBayesianHMMBased2019,
  title = {Bayesian {{HMM Based}} X-{{Vector Clustering}} for {{Speaker Diarization}}},
  booktitle = {Interspeech 2019},
  author = {Diez, Mireia and Burget, Lukáš and Wang, Shuai and Rohdin, Johan and Černocký, Jan},
  date = {2019-09-15},
  pages = {346--350},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2813},
  url = {https://www.isca-speech.org/archive/interspeech_2019/diez19_interspeech.html},
  urldate = {2023-03-13},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/Users/brono/Zotero/storage/BQGTZ4T3/Diez et al. - 2019 - Bayesian HMM Based x-Vector Clustering for Speaker.pdf}
}

@inproceedings{diezBayesianHMMBased2019a,
  title = {Bayesian {{HMM Based}} X-{{Vector Clustering}} for {{Speaker Diarization}}},
  booktitle = {Interspeech 2019},
  author = {Diez, Mireia and Burget, Lukáš and Wang, Shuai and Rohdin, Johan and Černocký, Jan},
  date = {2019-09-15},
  pages = {346--350},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2813},
  url = {https://www.isca-speech.org/archive/interspeech_2019/diez19_interspeech.html},
  urldate = {2023-04-15},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/Users/brono/Zotero/storage/AADE3PCQ/Diez et al. - 2019 - Bayesian HMM Based x-Vector Clustering for Speaker.pdf}
}

@inproceedings{diezOptimizingBayesianHmm2020,
  title = {Optimizing {{Bayesian Hmm Based X-Vector Clustering}} for the {{Second Dihard Speech Diarization Challenge}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Diez, Mireia and Burget, Lukáš and Landini, Federico and Wang, Shuai and Černocký, Honza},
  date = {2020-05},
  pages = {6519--6523},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9053982},
  abstract = {This paper presents an analysis of our diarization system winning the second DIHARD speech diarization challenge, track 1. This system is based on clustering x-vector speaker embeddings extracted every 0.25s from short segments of the input recording. In this paper, we focus on the two x-vector clustering methods employed, namely Agglomerative Hierarchical Clustering followed by a clustering based on Bayesian Hidden Markov Model (BHMM). Even though the system submitted to the challenge had further post-processing steps, we will show that using this BHMM solely is enough to achieve the best performance in the challenge. The analysis will show improvements achieved by optimizing individual processing steps, including a simple procedure to effectively perform "domain adaptation" by Probabilistic Linear Discriminant Analysis model interpolation. All experiments are performed in the DIHARD II evaluation framework.},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {Adaptation models,Bayes methods,DIHARD,Hidden Markov models,HMM,Interpolation,Probabilistic logic,Signal processing,Speaker Diarization,Variational Bayes,Voice activity detection,x-vector},
  file = {/Users/brono/Zotero/storage/SMMSG8SX/Diez et al. - 2020 - Optimizing Bayesian Hmm Based X-Vector Clustering .pdf;/Users/brono/Zotero/storage/V5KMEGCH/stamp.html}
}

@inproceedings{diezSpeakerDiarizationBased2018,
  title = {Speaker {{Diarization}} Based on {{Bayesian HMM}} with {{Eigenvoice Priors}}},
  booktitle = {The {{Speaker}} and {{Language Recognition Workshop}} ({{Odyssey}} 2018)},
  author = {Diez, Mireia and Burget, Lukas and Matejka, Pavel},
  date = {2018-06-26},
  pages = {147--154},
  publisher = {{ISCA}},
  doi = {10.21437/Odyssey.2018-21},
  url = {https://www.isca-speech.org/archive/odyssey_2018/diez18_odyssey.html},
  urldate = {2023-03-16},
  eventtitle = {The {{Speaker}} and {{Language Recognition Workshop}} ({{Odyssey}} 2018)},
  langid = {english},
  file = {/Users/brono/Zotero/storage/U7EUGJX7/Diez et al. - 2018 - Speaker Diarization based on Bayesian HMM with Eig.pdf}
}

@inproceedings{diezSpeakerDiarizationBased2018a,
  title = {Speaker {{Diarization}} Based on {{Bayesian HMM}} with {{Eigenvoice Priors}}},
  booktitle = {The {{Speaker}} and {{Language Recognition Workshop}} ({{Odyssey}} 2018)},
  author = {Diez, Mireia and Burget, Lukas and Matejka, Pavel},
  date = {2018-06-26},
  pages = {147--154},
  publisher = {{ISCA}},
  doi = {10.21437/Odyssey.2018-21},
  url = {https://www.isca-speech.org/archive/odyssey_2018/diez18_odyssey.html},
  urldate = {2023-04-03},
  abstract = {In this work, we describe the systems developed for tackling speaker diarization problem for the 3rd edition of DIHARD 2020 challenge. We submit systems only for track 1 task of this challenge. For this task, our developed systems employ the well-known x-vector/PLDA/AHC framework followed by the Bayesian Hidden Markov Model (HMM) with eigenvoice priors applied at the x-vector embeddings domain. For the extraction of x-vector embeddings we adopt three deep learning architectures, namely, TDNN with statistics pooling, TDNNLSTM and TDNN with multi-level (i.e., from more than one layer) statistics pooling. PLDA model is trained on the out-ofdomain data and then adapted to the DIHARD 2020 development data. 30-dimensional Mel-filterbank features are used as frontend. As a pre-processing step, we dereverberate the development and evaluation data of DIHARD 2020 using weighted prediction error (WPE) dereverberation algorithm.},
  eventtitle = {The {{Speaker}} and {{Language Recognition Workshop}} ({{Odyssey}} 2018)},
  langid = {english},
  file = {/Users/brono/Zotero/storage/3A6W6B5I/Diez et al. - 2018 - Speaker Diarization based on Bayesian HMM with Eig.pdf}
}

@inproceedings{diezSystemDIHARDSpeech2018,
  title = {{{BUT System}} for {{DIHARD Speech Diarization Challenge}} 2018},
  booktitle = {Interspeech 2018},
  author = {Diez, Mireia and Landini, Federico and Burget, Lukáš and Rohdin, Johan and Silnova, Anna and Žmolíková, Kateřina and Novotný, Ondřej and Veselý, Karel and Glembek, Ondřej and Plchot, Oldřich and Mošner, Ladislav and Matějka, Pavel},
  date = {2018-09-02},
  pages = {2798--2802},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1749},
  url = {https://www.isca-speech.org/archive/interspeech_2018/diez18_interspeech.html},
  urldate = {2023-03-16},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/IUTRQ5JT/Diez et al. - 2018 - BUT System for DIHARD Speech Diarization Challenge.pdf}
}

@online{DISPLACEChallenge,
  title = {{{DISPLACE Challenge}}},
  url = {https://displace2023.github.io/#about},
  urldate = {2023-04-04},
  file = {/Users/brono/Zotero/storage/PUHQ2SHC/displace2023.github.io.html}
}

@inproceedings{dissenSelfsupervisedSpeakerDiarization2022,
  title = {Self-Supervised {{Speaker Diarization}}},
  booktitle = {Interspeech 2022},
  author = {Dissen, Yehoshua and Kreuk, Felix and Keshet, Joseph},
  date = {2022-09-18},
  pages = {4013--4017},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-777},
  url = {https://www.isca-speech.org/archive/interspeech_2022/dissen22_interspeech.html},
  urldate = {2023-02-27},
  abstract = {Over the last few years, deep learning has grown in popularity for speaker verification, identification, and diarization. Inarguably, a significant part of this success is due to the demonstrated effectiveness of their speaker representations. These, however, are heavily dependent on large amounts of annotated data and can be sensitive to new domains. This study proposes an entirely unsupervised deep-learning model for speaker diarization. Specifically, the study focuses on generating highquality neural speaker representations without any annotated data, as well as on estimating secondary hyperparameters of the model without annotations.},
  eventtitle = {Interspeech 2022},
  langid = {english},
  keywords = {TODO},
  file = {/Users/brono/Zotero/storage/394HLWR8/Dissen et al. - 2022 - Self-supervised Speaker Diarization.pdf}
}

@article{dowlagarCodemixedTaskorientedDialog2023,
  title = {A Code-Mixed Task-Oriented Dialog Dataset for Medical Domain},
  author = {Dowlagar, Suman and Mamidi, Radhika},
  date = {2023-03},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {78},
  pages = {101449},
  issn = {08852308},
  doi = {10.1016/j.csl.2022.101449},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230822000729},
  urldate = {2023-06-13},
  langid = {english},
  file = {/Users/brono/Zotero/storage/3G6M3Y88/Dowlagar and Mamidi - 2023 - A code-mixed task-oriented dialog dataset for medi.pdf}
}

@online{duSpeakerOverlapawareNeural2022,
  title = {Speaker {{Overlap-aware Neural Diarization}} for {{Multi-party Meeting Analysis}}},
  author = {Du, Zhihao and Zhang, Shiliang and Zheng, Siqi and Yan, Zhijie},
  date = {2022-11-18},
  eprint = {2211.10243},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2211.10243},
  urldate = {2023-03-02},
  abstract = {Recently, hybrid systems of clustering and neural diarization models have been successfully applied in multi-party meeting analysis. However, current models always treat overlapped speaker diarization as a multi-label classification problem, where speaker dependency and overlaps are not well considered. To overcome the disadvantages, we reformulate overlapped speaker diarization task as a single-label prediction problem via the proposed power set encoding (PSE). Through this formulation, speaker dependency and overlaps can be explicitly modeled. To fully leverage this formulation, we further propose the speaker overlap-aware neural diarization (SOND) model, which consists of a context-independent (CI) scorer to model global speaker discriminability, a context-dependent scorer (CD) to model local discriminability, and a speaker combining network (SCN) to combine and reassign speaker activities. Experimental results show that using the proposed formulation can outperform the state-of-the-art methods based on target speaker voice activity detection, and the performance can be further improved with SOND, resulting in a 6.30\% relative diarization error reduction.},
  pubstate = {preprint},
  keywords = {Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,overlap,TODO},
  file = {/Users/brono/Zotero/storage/5M5RPC6G/Du et al. - 2022 - Speaker Overlap-aware Neural Diarization for Multi.pdf;/Users/brono/Zotero/storage/PYFIQ9K4/2211.html}
}

@online{DynamicScaleWeighting2022,
  title = {Dynamic {{Scale Weighting Through Multiscale Speaker Diarization}}},
  date = {2022-09-16T21:38+00:00},
  url = {https://developer.nvidia.com/blog/dynamic-scale-weighting-through-multiscale-speaker-diarization/},
  urldate = {2023-04-14},
  abstract = {MSDD is a neural model that can be trained on 2-speaker dataset and the proposed model enables overlap-aware speaker diarization on flexible number of speakers.},
  langid = {american},
  organization = {{NVIDIA Technical Blog}},
  file = {/Users/brono/Zotero/storage/9PYW5PEJ/dynamic-scale-weighting-through-multiscale-speaker-diarization.html}
}

@online{ekstedtAutomaticEvaluationTurntaking2023,
  title = {Automatic {{Evaluation}} of {{Turn-taking Cues}} in {{Conversational Speech Synthesis}}},
  author = {Ekstedt, Erik and Wang, Siyang and Székely, Éva and Gustafson, Joakim and Skantze, Gabriel},
  date = {2023-05-29},
  eprint = {2305.17971},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2305.17971},
  urldate = {2023-06-06},
  abstract = {Turn-taking is a fundamental aspect of human communication where speakers convey their intention to either hold, or yield, their turn through prosodic cues. Using the recently proposed Voice Activity Projection model, we propose an automatic evaluation approach to measure these aspects for conversational speech synthesis. We investigate the ability of three commercial, and two open-source, Text-To-Speech (TTS) systems ability to generate turn-taking cues over simulated turns. By varying the stimuli, or controlling the prosody, we analyze the models performances. We show that while commercial TTS largely provide appropriate cues, they often produce ambiguous signals, and that further improvements are possible. TTS, trained on read or spontaneous speech, produce strong turn-hold but weak turn-yield cues. We argue that this approach, that focus on functional aspects of interaction, provides a useful addition to other important speech metrics, such as intelligibility and naturalness.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/JMXBSSL8/Ekstedt et al. - 2023 - Automatic Evaluation of Turn-taking Cues in Conver.pdf;/Users/brono/Zotero/storage/FS6CPFGG/2305.html}
}

@video{englishmadeeasywithswarnshikhaCodeMixingDefinition2022,
  entrysubtype = {video},
  title = {Code Mixing | Definition | Examples | Basic and Supra Languages | Bilingual | Multilingual | \#shorts},
  editor = {{English made easy with Swarnshikha}},
  editortype = {director},
  date = {2022-10-02},
  url = {https://www.youtube.com/watch?v=ap-rw7zhmp8},
  urldate = {2023-04-22}
}

@article{evansComparativeStudyBottomUp2012,
  title = {A {{Comparative Study}} of {{Bottom-Up}} and {{Top-Down Approaches}} to {{Speaker Diarization}}},
  author = {Evans, Nicholas and Bozonnet, Simon and Wang, Dong and Fredouille, Corinne and Troncy, Raphaël},
  date = {2012-02},
  journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {20},
  number = {2},
  pages = {382--392},
  issn = {1558-7924},
  doi = {10.1109/TASL.2011.2159710},
  abstract = {This paper presents a theoretical framework to analyze the relative merits of the two most general, dominant approaches to speaker diarization involving bottom-up and top-down hierarchical clustering. We present an original qualitative comparison which argues how the two approaches are likely to exhibit different behavior in speaker inventory optimization and model training: bottom-up approaches will capture comparatively purer models and will thus be more sensitive to nuisance variation such as that related to the speech content; top-down approaches, in contrast, will produce less discriminative speaker models but, importantly, models which are potentially better normalized against nuisance variation. We report experiments conducted on two standard, single-channel NIST RT evaluation datasets which validate our hypotheses. Results show that competitive performance can be achieved with both bottom-up and top-down approaches (average DERs of 21\% and 22\%), and that neither approach is superior. Speaker purification, which aims to improve speaker discrimination, gives more consistent improvements with the top-down system than with the bottom-up system (average DERs of 19\% and 25\%), thereby confirming that the top-down system is less discriminative and that the bottom-up system is less stable. Finally, we report a new combination strategy that exploits the merits of the two approaches. Combination delivers an average DER of 17\% and confirms the intrinsic complementary of the two approaches.},
  eventtitle = {{{IEEE Transactions}} on {{Audio}}, {{Speech}}, and {{Language Processing}}},
  keywords = {Acoustics,Clustering,Data models,Hidden Markov models,Merging,NIST,rich transcription,segmentation,speaker diarization,Speech,Training},
  file = {/Users/brono/Zotero/storage/76UR7V57/Evans et al. - 2012 - A Comparative Study of Bottom-Up and Top-Down Appr.pdf;/Users/brono/Zotero/storage/TIY55KU6/stamp.html}
}

@unpublished{finnianIvectorsXvectorsGenerational2019,
  title = {From I-Vectors to x-Vectors a Generational Change in Speaker Recognition Illustrated on the {{NFI-FRIDA}} Database},
  author = {Finnian, Kelly and Anil, Alexander and Oscar, Forth and David, van der Vloed},
  date = {2019-07-15},
  eventtitle = {{{IAFPA Conference}}},
  file = {/Users/brono/Zotero/storage/3ART6KZ2/From i-vectors to x-vectors a generational change .pdf}
}

@book{fiscusRichTranscription20062006,
  title = {The {{Rich Transcription}} 2006 {{Spring Meeting Recognition Evaluation}}},
  author = {Fiscus, Jonathan and Ajot, Jerome and Michel, Martial and Garofolo, John},
  date = {2006-01-01},
  pages = {322},
  doi = {10.1007/11677482_32},
  abstract = {This paper presents the design and results of the Rich Transcription Spring 2005 (RT-05S) Meeting Recognition Evaluation. This evaluation is the third in a series of community-wide evaluations of language technologies in the meeting domain. For 2005, four evaluation tasks were supported. These included a speech-to-text (STT) transcription task and three diarization tasks: “Who Spoke When”, “Speech Activity Detection”, and “Source Localization.” The latter two were first-time experimental proof-of-concept tasks and were treated as “dry runs”. For the STT task, the lowest word error rate for the multiple distant microphone condition was 30.0\% which represented an impressive 33\% relative reduction from the best result obtained in the last such evaluation – the Rich Transcription Spring 2004 Meeting Recognition Evaluation. For the diarization “Who Spoke When” task, the lowest diarization error rate was 18.56\% which represented a 19\% relative reduction from that of RT-04S.},
  isbn = {978-3-540-32549-9},
  pagetotal = {309},
  file = {/Users/brono/Zotero/storage/3PJLC6IU/The_Rich_Transcription_2006_Spring_Meeting_Recogni.pdf;/Users/brono/Zotero/storage/99YXGBB5/Fiscus et al. - 2006 - The Rich Transcription 2006 Spring Meeting Recogni.pdf}
}

@article{foulkesPhonologicalVariationChildDirected2005,
  title = {Phonological {{Variation}} in {{Child-Directed Speech}}},
  author = {Foulkes, Paul and Docherty, Gerard J. and Watt, Dominic},
  date = {2005},
  journaltitle = {Language},
  shortjournal = {Language},
  volume = {81},
  number = {1},
  pages = {177--206},
  issn = {1535-0665},
  doi = {10.1353/lan.2005.0018},
  url = {http://muse.jhu.edu/content/crossref/journals/language/v081/81.1foulkes.pdf},
  urldate = {2023-04-02},
  langid = {english}
}

@online{fujitaEndtoEndNeuralDiarization2020,
  title = {End-to-{{End Neural Diarization}}: {{Reformulating Speaker Diarization}} as {{Simple Multi-label Classification}}},
  shorttitle = {End-to-{{End Neural Diarization}}},
  author = {Fujita, Yusuke and Watanabe, Shinji and Horiguchi, Shota and Xue, Yawen and Nagamatsu, Kenji},
  date = {2020-02-24},
  eprint = {2003.02966},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2003.02966},
  url = {http://arxiv.org/abs/2003.02966},
  urldate = {2023-03-13},
  abstract = {The most common approach to speaker diarization is clustering of speaker embeddings. However, the clustering-based approach has a number of problems; i.e., (i) it is not optimized to minimize diarization errors directly, (ii) it cannot handle speaker overlaps correctly, and (iii) it has trouble adapting their speaker embedding models to real audio recordings with speaker overlaps. To solve these problems, we propose the End-to-End Neural Diarization (EEND), in which a neural network directly outputs speaker diarization results given a multi-speaker recording. To realize such an end-to-end model, we formulate the speaker diarization problem as a multi-label classification problem and introduce a permutation-free objective function to directly minimize diarization errors. Besides its end-to-end simplicity, the EEND method can explicitly handle speaker overlaps during training and inference. Just by feeding multi-speaker recordings with corresponding speaker segment labels, our model can be easily adapted to real conversations. We evaluated our method on simulated speech mixtures and real conversation datasets. The results showed that the EEND method outperformed the state-of-the-art x-vector clustering-based method, while it correctly handled speaker overlaps. We explored the neural network architecture for the EEND method, and found that the self-attention-based neural network was the key to achieving excellent performance. In contrast to conditioning the network only on its previous and next hidden states, as is done using bidirectional long short-term memory (BLSTM), self-attention is directly conditioned on all the frames. By visualizing the attention weights, we show that self-attention captures global speaker characteristics in addition to local speech activity dynamics, making it especially suitable for dealing with the speaker diarization problem.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/XDYWCHE6/Fujita et al. - 2020 - End-to-End Neural Diarization Reformulating Speak.pdf;/Users/brono/Zotero/storage/6UHXT4FS/2003.html}
}

@online{fujitaEndtoEndNeuralSpeaker2019,
  title = {End-to-{{End Neural Speaker Diarization}} with {{Permutation-Free Objectives}}},
  author = {Fujita, Yusuke and Kanda, Naoyuki and Horiguchi, Shota and Nagamatsu, Kenji and Watanabe, Shinji},
  date = {2019-09-12},
  eprint = {1909.05952},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1909.05952},
  urldate = {2023-03-03},
  abstract = {In this paper, we propose a novel end-to-end neural-network-based speaker diarization method. Unlike most existing methods, our proposed method does not have separate modules for extraction and clustering of speaker representations. Instead, our model has a single neural network that directly outputs speaker diarization results. To realize such a model, we formulate the speaker diarization problem as a multi-label classification problem, and introduces a permutation-free objective function to directly minimize diarization errors without being suffered from the speaker-label permutation problem. Besides its end-to-end simplicity, the proposed method also benefits from being able to explicitly handle overlapping speech during training and inference. Because of the benefit, our model can be easily trained/adapted with real-recorded multi-speaker conversations just by feeding the corresponding multi-speaker segment labels. We evaluated the proposed method on simulated speech mixtures. The proposed method achieved diarization error rate of 12.28\%, while a conventional clustering-based system produced diarization error rate of 28.77\%. Furthermore, the domain adaptation with real-recorded speech provided 25.6\% relative improvement on the CALLHOME dataset. Our source code is available online at https://github.com/hitachi-speech/EEND.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,EEND,Electrical Engineering and Systems Science - Audio and Speech Processing,TODO},
  file = {/Users/brono/Zotero/storage/3DUW595E/Fujita et al. - 2019 - End-to-End Neural Speaker Diarization with Permuta.pdf;/Users/brono/Zotero/storage/SUPXVG6Z/1909.html}
}

@online{fujitaEndtoEndNeuralSpeaker2019a,
  title = {End-to-{{End Neural Speaker Diarization}} with {{Self-attention}}},
  author = {Fujita, Yusuke and Kanda, Naoyuki and Horiguchi, Shota and Xue, Yawen and Nagamatsu, Kenji and Watanabe, Shinji},
  date = {2019-09-13},
  eprint = {1909.06247},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1909.06247},
  urldate = {2023-03-05},
  abstract = {Speaker diarization has been mainly developed based on the clustering of speaker embeddings. However, the clustering-based approach has two major problems; i.e., (i) it is not optimized to minimize diarization errors directly, and (ii) it cannot handle speaker overlaps correctly. To solve these problems, the End-to-End Neural Diarization (EEND), in which a bidirectional long short-term memory (BLSTM) network directly outputs speaker diarization results given a multi-talker recording, was recently proposed. In this study, we enhance EEND by introducing self-attention blocks instead of BLSTM blocks. In contrast to BLSTM, which is conditioned only on its previous and next hidden states, self-attention is directly conditioned on all the other frames, making it much suitable for dealing with the speaker diarization problem. We evaluated our proposed method on simulated mixtures, real telephone calls, and real dialogue recordings. The experimental results revealed that the self-attention was the key to achieving good performance and that our proposed method performed significantly better than the conventional BLSTM-based method. Our method was even better than that of the state-of-the-art x-vector clustering-based method. Finally, by visualizing the latent representation, we show that the self-attention can capture global speaker characteristics in addition to local speech activity dynamics. Our source code is available online at https://github.com/hitachi-speech/EEND.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/FAFWU9QY/Fujita et al. - 2019 - End-to-End Neural Speaker Diarization with Self-at.pdf;/Users/brono/Zotero/storage/K5SYT6KN/1909.html}
}

@online{fujitaNeuralSpeakerDiarization2020,
  title = {Neural {{Speaker Diarization}} with {{Speaker-Wise Chain Rule}}},
  author = {Fujita, Yusuke and Watanabe, Shinji and Horiguchi, Shota and Xue, Yawen and Shi, Jing and Nagamatsu, Kenji},
  date = {2020-06-02},
  eprint = {2006.01796},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2006.01796},
  urldate = {2023-03-13},
  abstract = {Speaker diarization is an essential step for processing multi-speaker audio. Although an end-to-end neural diarization (EEND) method achieved state-of-the-art performance, it is limited to a fixed number of speakers. In this paper, we solve this fixed number of speaker issue by a novel speaker-wise conditional inference method based on the probabilistic chain rule. In the proposed method, each speaker's speech activity is regarded as a single random variable, and is estimated sequentially conditioned on previously estimated other speakers' speech activities. Similar to other sequence-to-sequence models, the proposed method produces a variable number of speakers with a stop sequence condition. We evaluated the proposed method on multi-speaker audio recordings of a variable number of speakers. Experimental results show that the proposed method can correctly produce diarization results with a variable number of speakers and outperforms the state-of-the-art end-to-end speaker diarization methods in terms of diarization error rate.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/7JJPIXKQ/Fujita et al. - 2020 - Neural Speaker Diarization with Speaker-Wise Chain.pdf;/Users/brono/Zotero/storage/XCR6S2R9/2006.html}
}

@article{ganjiIITGHingCoSCorpusHinglish2019,
  title = {{{IITG-HingCoS}} Corpus: {{A Hinglish}} Code-Switching Database for Automatic Speech Recognition},
  shorttitle = {{{IITG-HingCoS}} Corpus},
  author = {Ganji, Sreeram and Dhawan, Kunal and Sinha, Rohit},
  date = {2019-07},
  journaltitle = {Speech Communication},
  shortjournal = {Speech Communication},
  volume = {110},
  pages = {76--89},
  issn = {01676393},
  doi = {10.1016/j.specom.2019.04.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167639318304217},
  urldate = {2023-06-10},
  langid = {english},
  file = {/Users/brono/Zotero/storage/ZPQY252R/Ganji et al. - 2019 - IITG-HingCoS corpus A Hinglish code-switching dat.pdf}
}

@inproceedings{garcia-romeroSpeakerDiarizationUsing2017,
  title = {Speaker Diarization Using Deep Neural Network Embeddings},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Garcia-Romero, Daniel and Snyder, David and Sell, Gregory and Povey, Daniel and McCree, Alan},
  date = {2017-03},
  pages = {4930--4934},
  publisher = {{IEEE}},
  location = {{New Orleans, LA}},
  doi = {10.1109/ICASSP.2017.7953094},
  url = {http://ieeexplore.ieee.org/document/7953094/},
  urldate = {2023-03-12},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-4117-6},
  file = {/Users/brono/Zotero/storage/BWH2UGQD/Garcia-Romero et al. - 2017 - Speaker diarization using deep neural network embe.pdf}
}

@inproceedings{gargCodeswitchedLanguageModels2018,
  title = {Code-Switched {{Language Models Using Dual RNNs}} and {{Same-Source Pretraining}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Garg, Saurabh and Parekh, Tanmay and Jyothi, Preethi},
  date = {2018},
  pages = {3078--3083},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1346},
  url = {http://aclweb.org/anthology/D18-1346},
  urldate = {2023-04-16},
  eventtitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  file = {/Users/brono/Zotero/storage/TVERQE5T/Garg et al. - 2018 - Code-switched Language Models Using Dual RNNs and .pdf}
}

@online{gargDualLanguageModels2018,
  title = {Dual {{Language Models}} for {{Code Switched Speech Recognition}}},
  author = {Garg, Saurabh and Parekh, Tanmay and Jyothi, Preethi},
  date = {2018-08-03},
  eprint = {1711.01048},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1711.01048},
  urldate = {2023-06-12},
  abstract = {In this work, we present a simple and elegant approach to language modeling for bilingual code-switched text. Since code-switching is a blend of two or more different languages, a standard bilingual language model can be improved upon by using structures of the monolingual language models. We propose a novel technique called dual language models, which involves building two complementary monolingual language models and combining them using a probabilistic model for switching between the two. We evaluate the efficacy of our approach using a conversational Mandarin-English speech corpus. We prove the robustness of our model by showing significant improvements in perplexity measures over the standard bilingual language model without the use of any external information. Similar consistent improvements are also reflected in automatic speech recognition error rates.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/T55Z27BD/Garg et al. - 2018 - Dual Language Models for Code Switched Speech Reco.pdf;/Users/brono/Zotero/storage/P7UKDTE5/1711.html}
}

@inproceedings{gebreGesturerSpeaker2013,
  title = {The Gesturer Is the Speaker},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Gebre, Binyam Gebrekidan and Wittenburg, Peter and Heskes, Tom},
  date = {2013-05},
  pages = {3751--3755},
  publisher = {{IEEE}},
  location = {{Vancouver, BC, Canada}},
  doi = {10.1109/ICASSP.2013.6638359},
  url = {http://ieeexplore.ieee.org/document/6638359/},
  urldate = {2023-02-01},
  abstract = {We present and solve the speaker diarization problem in a novel way. We hypothesize that the gesturer is the speaker and that identifying the gesturer can be taken as identifying the active speaker. We provide evidence in support of the hy­ pothesis from gesture literature and audio-visual synchrony studies. We also present a vision-only diarization algorithm that relies on gestures (i.e. upper body movements). Experi­ ments carried out on 8.9 hours of a publicly available dataset (the AMI meeting data) show that diarization error rates as low as 15\% can be achieved.},
  eventtitle = {{{ICASSP}} 2013 - 2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-0356-6},
  langid = {english},
  keywords = {TODO},
  file = {/Users/brono/Zotero/storage/W824FZBB/Gebre et al. - 2013 - The gesturer is the speaker.pdf}
}

@inproceedings{gelderloosLearningUnderstandChilddirected2020,
  title = {Learning to {{Understand Child-directed}} and {{Adult-directed Speech}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Gelderloos, Lieke and Chrupała, Grzegorz and Alishahi, Afra},
  date = {2020},
  pages = {1--6},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.1},
  url = {https://www.aclweb.org/anthology/2020.acl-main.1},
  urldate = {2023-04-19},
  abstract = {Speech directed to children differs from adultdirected speech in linguistic aspects such as repetition, word choice, and sentence length, as well as in aspects of the speech signal itself, such as prosodic and phonemic variation. Human language acquisition research indicates that child-directed speech helps language learners. This study explores the effect of child-directed speech when learning to extract semantic information from speech directly. We compare the task performance of models trained on adult-directed speech (ADS) and child-directed speech (CDS). We find indications that CDS helps in the initial stages of learning, but eventually, models trained on ADS reach comparable task performance, and generalize better. The results suggest that this is at least partially due to linguistic rather than acoustic properties of the two registers, as we see the same pattern when looking at models trained on acoustically comparable synthetic speech.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  file = {/Users/brono/Zotero/storage/86N2948B/Gelderloos et al. - 2020 - Learning to Understand Child-directed and Adult-di.pdf}
}

@misc{ghoshalSpeakerDiarizationBRIEF,
  title = {Speaker {{Diarization}}: {{A BRIEF OVERVIEW}}},
  author = {Ghoshal, Parijat},
  url = {https://www.zhaw.ch/storage/engineering/institute-zentren/cai/MSE_EVA19_Speaker_Diarization_Survey_Ghoshal.pdf},
  file = {/Users/brono/Zotero/storage/83D28AFH/MSE_EVA19_Speaker_Diarization_Survey_Ghoshal.pdf}
}

@online{gilkersonLENANaturalLanguage2008,
  title = {The {{LENA Natural Language Study}}},
  author = {Gilkerson, Jill and {Jeffrey A. Richards}},
  date = {2008},
  url = {https://www.lena.org/wp-content/uploads/2016/07/LTR-02-2_Natural_Language_Study.pdf},
  urldate = {2023-04-19},
  file = {/Users/brono/Zotero/storage/FJFTTFHX/LTR-02-2_Natural_Language_Study.pdf}
}

@article{gomezSpeechEnhancementBinaural,
  title = {Speech Enhancement through Binaural Negative Filtering},
  author = {Gomez, Pedro and Alvarez, Agustin and Martinez, Rafael and Nieto, Victor and Rodellar, Victoria},
  abstract = {Through the present paper the use of Negative Beamforming for Speech Enhancement is explored. This method may be used to eliminate or enhance a specific signal using a binaural array, and its extension to three-microphone arrays is also shown. The fundamentals of the technique are reviewed, and a structure to control and improve its angular selectivity is presented. Results obtained in a real situation are also commented. Applications of this technique may be found in improving Noise Robust Speech Recognition Methods for Security Systems or Domotic Control.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/MGEA527Z/Gomez et al. - Speech enhancement through binaural negative filte.pdf}
}

@article{gonzalezvillasantiAutomatizedAnalysisChildren2020,
  title = {Automatized Analysis of Children’s Exposure to Child-Directed Speech in Reschool Settings: {{Validation}} and Application},
  shorttitle = {Automatized Analysis of Children’s Exposure to Child-Directed Speech in Reschool Settings},
  author = {Gonzalez Villasanti, Hugo and Justice, Laura M. and Chaparro-Moreno, Leidy Johana and Lin, Tzu-Jung and Purtell, Kelly},
  editor = {Nittrouer, Susan},
  date = {2020-11-25},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {15},
  number = {11},
  pages = {e0242511},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0242511},
  url = {https://dx.plos.org/10.1371/journal.pone.0242511},
  urldate = {2023-04-04},
  abstract = {The present study explored whether a tool for automatic detection and recognition of interactions and child-directed speech (CDS) in preschool classrooms could be developed, validated, and applied to non-coded video recordings representing children’s classroom experiences. Using first-person video recordings collected by 13 preschool children during a morning in their classrooms, we extracted high-level audiovisual features from recordings using automatic speech recognition and computer vision services from a cloud computing provider. Using manual coding for interactions and transcriptions of CDS as reference, we trained and tested supervised classifiers and linear mappings to measure five variables of interest. We show that the supervised classifiers trained with speech activity, proximity, and high-level facial features achieve adequate accuracy in detecting interactions. Furthermore, in combination with an automatic speech recognition service, the supervised classifier achieved error rates for CDS measures that are in line with other open-source automatic decoding tools in early childhood settings. Finally, we demonstrate our tool’s applicability by using it to automatically code and transcribe children’s interactions and CDS exposure vertically within a classroom day (morning to afternoon) and horizontally over time (fall to winter). Developing and scaling tools for automatized capture of children’s interactions with others in the preschool classroom, as well as exposure to CDS, may revolutionize scientific efforts to identify precise mechanisms that foster young children’s language development.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/V3HRF5SR/Gonzalez Villasanti et al. - 2020 - Automatized analysis of children’s exposure to chi.pdf}
}

@article{goosLectureNotesComputer,
  title = {Lecture {{Notes}} in {{Computer Science}}},
  author = {Goos, Gerhard and Hartmanis, Juris and family=Leeuwen, given=Jan, prefix=van, useprefix=true and Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M and Mattern, Friedemann and Mitchell, John C and Naor, Moni and Nierstrasz, Oscar and Rangan, C Pandu and Steffen, Bernhard},
  langid = {english},
  file = {/Users/brono/Zotero/storage/A99WRBBS/Goos et al. - Lecture Notes in Computer Science.pdf}
}

@online{gorinThisHoustonSay2020,
  title = {"{{This}} Is {{Houston}}. {{Say}} Again, Please". {{The Behavox}} System for the {{Apollo-11 Fearless Steps Challenge}} (Phase {{II}})},
  author = {Gorin, Arseniy and Kulko, Daniil and Grima, Steven and Glasman, Alex},
  date = {2020-08-04},
  eprint = {2008.01504},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2008.01504},
  urldate = {2023-04-03},
  abstract = {We describe the speech activity detection (SAD), speaker diarization (SD), and automatic speech recognition (ASR) experiments conducted by the Behavox team for the Interspeech 2020 Fearless Steps Challenge (FSC-2). A relatively small amount of labeled data, a large variety of speakers and channel distortions, specific lexicon and speaking style resulted in high error rates on the systems which involved this data. In addition to approximately 36 hours of annotated NASA mission recordings, the organizers provided a much larger but unlabeled 19k hour Apollo-11 corpus that we also explore for semi-supervised training of ASR acoustic and language models, observing more than 17\% relative word error rate improvement compared to training on the FSC-2 data only. We also compare several SAD and SD systems to approach the most difficult tracks of the challenge (track 1 for diarization and ASR), where long 30-minute audio recordings are provided for evaluation without segmentation or speaker information. For all systems, we report substantial performance improvements compared to the FSC-2 baseline systems, and achieved a first-place ranking for SD and ASR and fourth-place for SAD in the challenge.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/QTAKAAIR/Gorin et al. - 2020 - This is Houston. Say again, please. The Behavox .pdf;/Users/brono/Zotero/storage/9NXZNWCZ/2008.html}
}

@inproceedings{gosztolyaDNNBasedFeatureExtraction2017,
  title = {{{DNN-Based Feature Extraction}} and {{Classifier Combination}} for {{Child-Directed Speech}}, {{Cold}} and {{Snoring Identification}}},
  booktitle = {Interspeech 2017},
  author = {Gosztolya, Gábor and Busa-Fekete, Róbert and Grósz, Tamás and Tóth, László},
  date = {2017-08-20},
  pages = {3522--3526},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-905},
  url = {https://www.isca-speech.org/archive/interspeech_2017/gosztolya17b_interspeech.html},
  urldate = {2023-04-04},
  abstract = {In this study we deal with the three sub-challenges of the Interspeech ComParE Challenge 2017, where the goal is to identify child-directed speech, speakers having a cold, and different types of snoring sounds. For the first two sub-challenges we propose a simple, two-step feature extraction and classification scheme: first we perform frame-level classification via Deep Neural Networks (DNNs), and then we extract utterancelevel features from the DNN outputs. By utilizing these features for classification, we were able to match the performance of the standard paralinguistic approach (which involves extracting thousands of features, many of them being completely irrelevant to the actual task). As for the Snoring Sub-Challenge, we divided the recordings into segments, and averaged out some frame-level features segment-wise, which were then used for utterance-level classification. When combining the predictions of the proposed approaches with those got by the standard paralinguistic approach, we managed to outperform the baseline values of the Cold and Snoring sub-challenges on the hidden test sets.},
  eventtitle = {Interspeech 2017},
  langid = {english},
  file = {/Users/brono/Zotero/storage/WQPVKRGS/Gosztolya et al. - 2017 - DNN-Based Feature Extraction and Classifier Combin.pdf}
}

@online{guoStudySemisupervisedApproaches2018,
  title = {Study of {{Semi-supervised Approaches}} to {{Improving English-Mandarin Code-Switching Speech Recognition}}},
  author = {Guo, Pengcheng and Xu, Haihua and Xie, Lei and Chng, Eng Siong},
  date = {2018-06-16},
  eprint = {1806.06200},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.06200},
  urldate = {2023-04-16},
  abstract = {In this paper, we present our overall efforts to improve the performance of a code-switching speech recognition system using semi-supervised training methods from lexicon learning to acoustic modeling, on the South East Asian Mandarin-English (SEAME) data. We first investigate semi-supervised lexicon learning approach to adapt the canonical lexicon, which is meant to alleviate the heavily accented pronunciation issue within the code-switching conversation of the local area. As a result, the learned lexicon yields improved performance. Furthermore, we attempt to use semi-supervised training to deal with those transcriptions that are highly mismatched between human transcribers and ASR system. Specifically, we conduct semi-supervised training assuming those poorly transcribed data as unsupervised data. We found the semi-supervised acoustic modeling can lead to improved results. Finally, to make up for the limitation of the conventional n-gram language models due to data sparsity issue, we perform lattice rescoring using neural network language models, and significant WER reduction is obtained.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/8CTFDQ3G/Guo et al. - 2018 - Study of Semi-supervised Approaches to Improving E.pdf;/Users/brono/Zotero/storage/LK42AM57/1806.html}
}

@online{guptaSpokenLanguageIdentification2023,
  title = {Spoken {{Language Identification System}} for {{English-Mandarin Code-Switching Child-Directed Speech}}},
  author = {Gupta, Shashi Kant and Hiray, Sushant and Kukde, Prashant},
  date = {2023-06-01},
  eprint = {2306.00736},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2306.00736},
  urldate = {2023-07-30},
  abstract = {This work focuses on improving the Spoken Language Identification (LangId) system for a challenge that focuses on developing robust language identification systems that are reliable for non-standard, accented (Singaporean accent), spontaneous code-switched, and child-directed speech collected via Zoom. We propose a two-stage Encoder-Decoder-based E2E model. The encoder module consists of 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with a global context. The decoder module uses an attentive temporal pooling mechanism to get fixed length time-independent feature representation. The total number of parameters in the model is around 22.1 M, which is relatively light compared to using some large-scale pre-trained speech models. We achieved an EER of 15.6\% in the closed track and 11.1\% in the open track (baseline system 22.1\%). We also curated additional LangId data from YouTube videos (having Singaporean speakers), which will be released for public use.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/KBM2WNAE/Gupta et al. - 2023 - Spoken Language Identification System for English-.pdf;/Users/brono/Zotero/storage/7QVPDIXJ/2306.html}
}

@inproceedings{guzmanMetricsModelingCodeSwitching2017,
  title = {Metrics for {{Modeling Code-Switching Across Corpora}}},
  booktitle = {Interspeech 2017},
  author = {Guzmán, Gualberto and Ricard, Joseph and Serigos, Jacqueline and Bullock, Barbara E. and Toribio, Almeida Jacqueline},
  date = {2017-08-20},
  pages = {67--71},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-1429},
  url = {https://www.isca-speech.org/archive/interspeech_2017/guzman17_interspeech.html},
  urldate = {2023-06-27},
  abstract = {In developing technologies for code-switched speech, it would be desirable to be able to predict how much language mixing might be expected in the signal and the regularity with which it might occur. In this work, we offer various metrics that allow for the classification and visualization of multilingual corpora according to the ratio of languages represented, the probability of switching between them, and the time-course of switching. Applying these metrics to corpora of different languages and genres, we find that they display distinct probabilities and periodicities of switching, information useful for speech processing of mixed-language data.},
  eventtitle = {Interspeech 2017},
  langid = {english},
  file = {/Users/brono/Zotero/storage/5ZQUIE4U/Guzmán et al. - 2017 - Metrics for Modeling Code-Switching Across Corpora.pdf}
}

@article{halmariCodeswitchingRegisterShift1994,
  title = {Code-Switching and Register Shift: {{Evidence}} from {{Finnish-English}} Child Bilingual Conversation},
  shorttitle = {Code-Switching and Register Shift},
  author = {Halmari, Helena and Smith, Wendy},
  date = {1994-04},
  journaltitle = {Journal of Pragmatics},
  shortjournal = {Journal of Pragmatics},
  volume = {21},
  number = {4},
  pages = {427--445},
  issn = {03782166},
  doi = {10.1016/0378-2166(94)90013-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0378216694900132},
  urldate = {2023-04-23},
  langid = {english},
  file = {/Users/brono/Zotero/storage/DIPQ3UXG/Halmari and Smith - 1994 - Code-switching and register shift Evidence from F.pdf}
}

@inproceedings{hansen2019InauguralFearless2019,
  title = {The 2019 {{Inaugural Fearless Steps Challenge}}: {{A Giant Leap}} for {{Naturalistic Audio}}},
  shorttitle = {The 2019 {{Inaugural Fearless Steps Challenge}}},
  booktitle = {Interspeech 2019},
  author = {Hansen, John H.L. and Joglekar, Aditya and Shekhar, Meena Chandra and Kothapally, Vinay and Yu, Chengzhu and Kaushik, Lakshmish and Sangwan, Abhijeet},
  date = {2019-09-15},
  pages = {1851--1855},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2301},
  url = {https://www.isca-speech.org/archive/interspeech_2019/hansen19_interspeech.html},
  urldate = {2023-04-03},
  abstract = {The 2019 FEARLESS STEPS (FS-1) Challenge is an initial step to motivate a streamlined and collaborative effort from the speech and language community towards addressing massive naturalistic audio, the first of its kind. The Fearless Steps Corpus is a collection of 19,000 hours of multi-channel recordings of spontaneous speech from over 450 speakers under multiple noise conditions. A majority of the Apollo Missions original analog data is unlabeled and has thus far motivated the development of both unsupervised and semi-supervised strategies. This edition of the challenge encourages the development of core speech and language technology systems for data with limited groundtruth/low resource availability and is intended to serve as the “First Step” towards extracting high-level information from such massive unlabeled corpora. In conjunction with the Challenge, 11,000 hours of synchronized 30-channel Apollo-11 audio data has also been released to the public by CRSS-UTDallas. We describe in this paper the Fearless Steps Corpus, Challenge Tasks, their associated baseline systems, and results. In conclusion, we also provide insights gained by the CRSS-UTDallas team during the inaugural Fearless Steps Challenge.},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/Users/brono/Zotero/storage/F5SR6ZLJ/Hansen et al. - 2019 - The 2019 Inaugural Fearless Steps Challenge A Gia.pdf}
}

@article{hansenSpeechLanguageProcessing2019,
  title = {Speech and Language Processing for Assessing Child–Adult Interaction Based on Diarization and Location},
  author = {Hansen, John H. L. and Najafian, Maryam and Lileikyte, Rasa and Irvin, Dwight and Rous, Beth},
  date = {2019-09},
  journaltitle = {International Journal of Speech Technology},
  shortjournal = {Int J Speech Technol},
  volume = {22},
  number = {3},
  pages = {697--709},
  issn = {1381-2416, 1572-8110},
  doi = {10.1007/s10772-019-09590-0},
  url = {http://link.springer.com/10.1007/s10772-019-09590-0},
  urldate = {2023-04-17},
  abstract = {Understanding and assessing child verbal communication patterns is critical in facilitating effective language development. Typically speaker diarization is performed to explore children’s verbal engagement. Understanding which activity areas stimulate verbal communication can help promote more efficient language development. In this study, we present a twostage children vocal engagement prediction system that consists of (1) a near to real-time, noise robust system that measures the duration of child-to-adult and child-to-child conversations, and tracks the number of conversational turn-takings, (2) a novel child location tracking strategy, that determines in which activity areas a child spends most/least of their time. A proposed child–adult turn-taking solution relies exclusively on vocal cues observed during the interaction between a child and other children, and/or classroom teachers. By employing a threshold optimized speech activity detection using a linear combination of voicing measures, it is possible to achieve effective speech/non-speech segment detection prior to conversion assessment. This TO-COMBO-SAD reduces classification error rates for adult-child audio by 21.34\% and 27.3\% compared to a baseline i-Vector and standard Bayesian Information Criterion diarization systems, respectively. In addition, this study presents a unique location tracking system adult-child that helps determine the quantity of child–adult communication in specific activity areas, and which activities stimulate voice communication engagement in a child–adult education space. We observe that our proposed location tracking solution offers unique opportunities to assess speech and language interaction for children, and quantify the location context which would contribute to improve verbal communication.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/YQD8T55N/Hansen et al. - 2019 - Speech and language processing for assessing child.pdf}
}

@article{hardyNovoKaposiSarcoma1976,
  title = {De Novo {{Kaposi}}'s Sarcoma in Renal Transplantation. {{Case}} Report and Brief Review},
  author = {Hardy, M. A. and Goldfarb, P. and Levine, S. and Dattner, A. and Muggia, F. M. and Levitt, S. and Weinstein, E.},
  date = {1976-07},
  journaltitle = {Cancer},
  shortjournal = {Cancer},
  volume = {38},
  number = {1},
  eprint = {59624},
  eprinttype = {pmid},
  pages = {144--148},
  issn = {0008-543X},
  doi = {10.1002/1097-0142(197607)38:1<144::aid-cncr2820380123>3.0.co;2-7},
  abstract = {This report describes a de novo development of Kaposi's sarcoma in a Puerto-Rician man 9 months after a cadaveric renal transplant. Progression of the disease was observed despite local irradiation, while the patient remained immunosuppressed with prednisone and azathioprine. This was accompanied by depressed immunologic tests. Discontinuation of azathioprine and addition of chemotherapy (bleomycin and vincristine), while continuing prednisone to maintain functional survival of renal allograft, has led in this patient to regression of extensive cutaneous and suspected pulmonary Kaposi's sarcoma lesions. The possible importance of a depressed immunosurveillance mechanism and activation of latent oncogenic virus by the presence of an allograft in the de novo appearance of Kaposi's sarcoma in transplant recipients is briefly discussed.},
  langid = {english},
  keywords = {Antibody Formation,Azathioprine,Bleomycin,Humans,{Immunity, Cellular},Immunosuppression Therapy,Kidney Transplantation,Male,Middle Aged,{Remission, Spontaneous},{Sarcoma, Kaposi},Skin Neoplasms,{Transplantation, Homologous},Vincristine}
}

@article{heoNAVERCLOVASUBMISSION,
  title = {{{NAVER CLOVA SUBMISSION TO THE THIRD DIHARD CHALLENGE}}},
  author = {Heo, Hee-Soo and Jung, Jee-weon and Kwon, Youngki and Kim, You Jin and Huh, Jaesung and Chung, Joon Son and Lee, Bong-Jin},
  abstract = {This report describes the NAVER CLOVA speaker diarization system for the third DIHARD challenge. Our system comprises the following five subsystems: end-point detection, overlapped speech detection, speaker embedding extraction, feature enhancement, and clustering. Its process pipeline has two improvements over the conventional diarization systems: feature enhancement and overlapped speech detection. For feature enhancement, our proposed approach first adopts an utterance-wise autoencoder that reduces the dimensionality of extracted speaker embeddings. Then, we apply a self-attention mechanism in which we refer to as the attention-based aggregation. We aim to adapt and enhance the speaker representation for clustering using these two techniques. Also, variants of CRNN based overlapped speech detection systems, trained as a three-class classifier, and their ensemble are explored to further reduce the missed detection of overlapped speech regions. The submitted system achieves a diarization error rate of 14.96\% and 15.40\% for the development and the evaluation datasets of DIHARDIII\_Task1\_CORE track, which ranks the 3rd place.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/LHAU9KG7/Heo et al. - NAVER CLOVA SUBMISSION TO THE THIRD DIHARD CHALLEN.pdf}
}

@inproceedings{higuchiSpeakerEmbeddingsIncorporating2020,
  title = {Speaker {{Embeddings Incorporating Acoustic Conditions}} for {{Diarization}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Higuchi, Yosuke and Suzuki, Masayuki and Kurata, Gakuto},
  date = {2020-05},
  pages = {7129--7133},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9054273},
  url = {https://ieeexplore.ieee.org/document/9054273/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5}
}

@article{hintonDeepNeuralNetworks2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  date = {2012},
  abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/7C8IT96F/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf}
}

@online{horiguchiEndtoEndSpeakerDiarization2020,
  title = {End-to-{{End Speaker Diarization}} for an {{Unknown Number}} of {{Speakers}} with {{Encoder-Decoder Based Attractors}}},
  author = {Horiguchi, Shota and Fujita, Yusuke and Watanabe, Shinji and Xue, Yawen and Nagamatsu, Kenji},
  date = {2020-10-05},
  eprint = {2005.09921},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2005.09921},
  urldate = {2023-03-06},
  abstract = {End-to-end speaker diarization for an unknown number of speakers is addressed in this paper. Recently proposed end-to-end speaker diarization outperformed conventional clustering-based speaker diarization, but it has one drawback: it is less flexible in terms of the number of speakers. This paper proposes a method for encoder-decoder based attractor calculation (EDA), which first generates a flexible number of attractors from a speech embedding sequence. Then, the generated multiple attractors are multiplied by the speech embedding sequence to produce the same number of speaker activities. The speech embedding sequence is extracted using the conventional self-attentive end-to-end neural speaker diarization (SA-EEND) network. In a two-speaker condition, our method achieved a 2.69 \% diarization error rate (DER) on simulated mixtures and a 8.07 \% DER on the two-speaker subset of CALLHOME, while vanilla SA-EEND attained 4.56 \% and 9.54 \%, respectively. In unknown numbers of speakers conditions, our method attained a 15.29 \% DER on CALLHOME, while the x-vector-based clustering method achieved a 19.43 \% DER.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/BM4AJAZE/Horiguchi et al. - 2020 - End-to-End Speaker Diarization for an Unknown Numb.pdf;/Users/brono/Zotero/storage/BXSKC938/2005.html}
}

@inproceedings{horiguchiEndToEndSpeakerDiarization2021,
  title = {End-{{To-End Speaker Diarization}} as {{Post-Processing}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Horiguchi, Shota and Garcia, Paola and Fujita, Yusuke and Watanabe, Shinji and Nagamatsu, Kenji},
  date = {2021-06-06},
  pages = {7188--7192},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9413436},
  url = {https://ieeexplore.ieee.org/document/9413436/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  file = {/Users/brono/Zotero/storage/EGQT6LE6/Horiguchi et al. - 2021 - End-To-End Speaker Diarization as Post-Processing.pdf}
}

@article{horiguchiHitachiJHUDIHARDIII,
  title = {The {{Hitachi-JHU DIHARD III System}}: {{Competitive End-to-End Neural Diarization}} and {{X-Vector Clustering Systems Combined}} by {{DOVER-Lap}}},
  author = {Horiguchi, Shota and Yalta, Nelson and Garcıa, Paola and Takashima, Yuki and Xue, Yawen and Raj, Desh and Huang, Zili and Fujita, Yusuke and Watanabe, Shinji and Khudanpur, Sanjeev},
  abstract = {This paper provides a detailed description of the Hitachi-JHU system that was submitted to the Third DIHARD Speech Diarization Challenge. The system outputs the ensemble results of the five subsystems: two x-vector-based subsystems, two end-to-end neural diarization-based subsystems, and one hybrid subsystem. We refine each system and all five subsystems become competitive and complementary. After the DOVER-Lap based system combination, it achieved diarization error rates of 11.58\% and 14.09\% in Track 1 full and core, and 16.94\% and 20.01\% in Track 2 full and core, respectively. With their results, we won second place in all the tasks of the challenge.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/4WXYTH4H/Horiguchi et al. - The Hitachi-JHU DIHARD III System Competitive End.pdf}
}

@online{horiguchiHitachiJHUDIHARDIII2021,
  title = {The {{Hitachi-JHU DIHARD III System}}: {{Competitive End-to-End Neural Diarization}} and {{X-Vector Clustering Systems Combined}} by {{DOVER-Lap}}},
  shorttitle = {The {{Hitachi-JHU DIHARD III System}}},
  author = {Horiguchi, Shota and Yalta, Nelson and Garcia, Paola and Takashima, Yuki and Xue, Yawen and Raj, Desh and Huang, Zili and Fujita, Yusuke and Watanabe, Shinji and Khudanpur, Sanjeev},
  date = {2021-02-02},
  eprint = {2102.01363},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2102.01363},
  urldate = {2023-03-13},
  abstract = {This paper provides a detailed description of the Hitachi-JHU system that was submitted to the Third DIHARD Speech Diarization Challenge. The system outputs the ensemble results of the five subsystems: two x-vector-based subsystems, two end-to-end neural diarization-based subsystems, and one hybrid subsystem. We refine each system and all five subsystems become competitive and complementary. After the DOVER-Lap based system combination, it achieved diarization error rates of 11.58 \% and 14.09 \% in Track 1 full and core, and 16.94 \% and 20.01 \% in Track 2 full and core, respectively. With their results, we won second place in all the tasks of the challenge.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/VVT67KIY/Horiguchi et al. - 2021 - The Hitachi-JHU DIHARD III System Competitive End.pdf;/Users/brono/Zotero/storage/H4DEWM6J/2102.html}
}

@inproceedings{horiguchiNeuralDiarizationUnlimited2021,
  title = {Towards {{Neural Diarization}} for {{Unlimited Numbers}} of {{Speakers Using Global}} and {{Local Attractors}}},
  booktitle = {2021 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Horiguchi, Shota and Watanabe, Shinji and Garcia, Paola and Xue, Yawen and Takashima, Yuki and Kawaguchi, Yohei},
  date = {2021-12-13},
  pages = {98--105},
  publisher = {{IEEE}},
  location = {{Cartagena, Colombia}},
  doi = {10.1109/ASRU51503.2021.9687875},
  url = {https://ieeexplore.ieee.org/document/9687875/},
  urldate = {2023-03-13},
  eventtitle = {2021 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-66543-739-4},
  file = {/Users/brono/Zotero/storage/WMY8NZI2/Horiguchi et al. - 2021 - Towards Neural Diarization for Unlimited Numbers o.pdf}
}

@online{huangSpeakerDiarizationRegion2020,
  title = {Speaker {{Diarization}} with {{Region Proposal Network}}},
  author = {Huang, Zili and Watanabe, Shinji and Fujita, Yusuke and Garcia, Paola and Shao, Yiwen and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2020-02-14},
  eprint = {2002.06220},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2002.06220},
  urldate = {2023-03-01},
  abstract = {Speaker diarization is an important pre-processing step for many speech applications, and it aims to solve the "who spoke when" problem. Although the standard diarization systems can achieve satisfactory results in various scenarios, they are composed of several independently-optimized modules and cannot deal with the overlapped speech. In this paper, we propose a novel speaker diarization method: Region Proposal Network based Speaker Diarization (RPNSD). In this method, a neural network generates overlapped speech segment proposals, and compute their speaker embeddings at the same time. Compared with standard diarization systems, RPNSD has a shorter pipeline and can handle the overlapped speech. Experimental results on three diarization datasets reveal that RPNSD achieves remarkable improvements over the state-of-the-art x-vector baseline.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,E2E,Electrical Engineering and Systems Science - Audio and Speech Processing,Overlap,RPN,TODO},
  file = {/Users/brono/Zotero/storage/HFQXKNNA/Huang et al. - 2020 - Speaker Diarization with Region Proposal Network.pdf;/Users/brono/Zotero/storage/XQN54ER3/2002.html}
}

@inproceedings{huangSpeakerDiarizationRegion2020a,
  title = {Speaker {{Diarization}} with {{Region Proposal Network}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Huang, Zili and Watanabe, Shinji and Fujita, Yusuke and Garcia, Paola and Shao, Yiwen and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2020-05},
  pages = {6514--6518},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICASSP40776.2020.9053760},
  url = {https://ieeexplore.ieee.org/document/9053760/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-6631-5},
  file = {/Users/brono/Zotero/storage/86GX7Q22/Huang et al. - 2020 - Speaker Diarization with Region Proposal Network.pdf}
}

@inproceedings{huckvaleItSoundsYou2017,
  title = {It {{Sounds Like You Have}} a {{Cold}}! {{Testing Voice Features}} for the {{Interspeech}} 2017 {{Computational Paralinguistics Cold Challenge}}},
  booktitle = {Interspeech 2017},
  author = {Huckvale, Mark and Beke, András},
  date = {2017-08-20},
  pages = {3447--3451},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-1261},
  url = {https://www.isca-speech.org/archive/interspeech_2017/huckvale17_interspeech.html},
  urldate = {2023-04-04},
  eventtitle = {Interspeech 2017},
  langid = {english},
  file = {/Users/brono/Zotero/storage/JGEFU554/Huckvale and Beke - 2017 - It Sounds Like You Have a Cold! Testing Voice Feat.pdf}
}

@misc{IndicSpeechTexttoSpeechCorpus,
  title = {{{IndicSpeech}}: {{Text-to-Speech Corpus}} for {{Indian Languages}}},
  file = {/Users/brono/Zotero/storage/F9UVV32I/2020-lrec.pdf}
}

@online{jeoungImprovingTransformerbasedEndtoEnd2023,
  title = {Improving {{Transformer-based End-to-End Speaker Diarization}} by {{Assigning Auxiliary Losses}} to {{Attention Heads}}},
  author = {Jeoung, Ye-Rin and Yang, Joon-Young and Choi, Jeong-Hwan and Chang, Joon-Hyuk},
  date = {2023-03-02},
  eprint = {2303.01192},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2303.01192},
  urldate = {2023-04-09},
  abstract = {Transformer-based end-to-end neural speaker diarization (EEND) models utilize the multi-head self-attention (SA) mechanism to enable accurate speaker label prediction in overlapped speech regions. In this study, to enhance the training effectiveness of SA-EEND models, we propose the use of auxiliary losses for the SA heads of the transformer layers. Specifically, we assume that the attention weight matrices of an SA layer are redundant if their patterns are similar to those of the identity matrix. We then explicitly constrain such matrices to exhibit specific speaker activity patterns relevant to voice activity detection or overlapped speech detection tasks. Consequently, we expect the proposed auxiliary losses to guide the transformer layers to exhibit more diverse patterns in the attention weights, thereby reducing the assumed redundancies in the SA heads. The effectiveness of the proposed method is demonstrated using the simulated and CALLHOME datasets for two-speaker diarization tasks, reducing the diarization error rate of the conventional SA-EEND model by 32.58\% and 17.11\%, respectively.},
  langid = {english},
  pubstate = {preprint},
  keywords = {68T10(Primary) 68T07(Secondary),Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/NXP8KAA9/Jeoung et al. - 2023 - Improving Transformer-based End-to-End Speaker Dia.pdf}
}

@online{joglekarFearlessStepsChallenge2022,
  title = {Fearless {{Steps Challenge Phase-1 Evaluation Plan}}},
  author = {Joglekar, Aditya and Hansen, John H. L.},
  date = {2022-11-03},
  eprint = {2211.02051},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2211.02051},
  urldate = {2023-04-03},
  abstract = {The Fearless Steps Challenge 2019 Phase-1 (FSC-P1) is the inaugural Challenge of the Fearless Steps Initiative hosted by the Center for Robust Speech Systems (CRSS) at the University of Texas at Dallas. The goal of this Challenge is to evaluate the performance of state-of-the-art speech and language systems for large task-oriented teams with naturalistic audio in challenging environments. Researchers may select to participate in any single or multiple of these challenge tasks. Researchers may also choose to employ the FEARLESS STEPS corpus for other related speech applications. All participants are encouraged to submit their solutions and results for consideration in the ISCA INTERSPEECH-2019 special session.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/YLVNM7XB/Joglekar and Hansen - 2022 - Fearless Steps Challenge Phase-1 Evaluation Plan.pdf;/Users/brono/Zotero/storage/GDUTIZ2J/2211.html}
}

@online{jungPushingLimitsRaw2022,
  title = {Pushing the Limits of Raw Waveform Speaker Recognition},
  author = {Jung, Jee-weon and Kim, You Jin and Heo, Hee-Soo and Lee, Bong-Jin and Kwon, Youngki and Chung, Joon Son},
  date = {2022-03-28},
  eprint = {2203.08488},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2203.08488},
  urldate = {2023-04-14},
  abstract = {In recent years, speaker recognition systems based on raw waveform inputs have received increasing attention. However, the performance of such systems are typically inferior to the state-of-the-art handcrafted feature-based counterparts, which demonstrate equal error rates under 1\% on the popular VoxCeleb1 test set. This paper proposes a novel speaker recognition model based on raw waveform inputs. The model incorporates recent advances in machine learning and speaker verification, including the Res2Net backbone module and multi-layer feature aggregation. Our best model achieves an equal error rate of 0.89\%, which is competitive with the state-of-the-art models based on handcrafted features, and outperforms the best model based on raw waveform inputs by a large margin. We also explore the application of the proposed model in the context of self-supervised learning framework. Our self-supervised model outperforms single phase-based existing works in this line of research. Finally, we show that self-supervised pre-training is effective for the semi-supervised scenario where we only have a small set of labelled training data, along with a larger set of unlabelled examples.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/S57KV9KF/Jung et al. - 2022 - Pushing the limits of raw waveform speaker recogni.pdf;/Users/brono/Zotero/storage/WTZIP8E5/2203.html}
}

@online{jungSearchStrongEmbedding2022,
  title = {In Search of Strong Embedding Extractors for Speaker Diarisation},
  author = {Jung, Jee-weon and Heo, Hee-Soo and Lee, Bong-Jin and Huh, Jaesung and Brown, Andrew and Kwon, Youngki and Watanabe, Shinji and Chung, Joon Son},
  date = {2022-10-26},
  eprint = {2210.14682},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2210.14682},
  urldate = {2023-03-07},
  abstract = {Speaker embedding extractors (EEs), which map input audio to a speaker discriminant latent space, are of paramount importance in speaker diarisation. However, there are several challenges when adopting EEs for diarisation, from which we tackle two key problems. First, the evaluation is not straightforward because the features required for better performance differ between speaker verification and diarisation. We show that better performance on widely adopted speaker verification evaluation protocols does not lead to better diarisation performance. Second, embedding extractors have not seen utterances in which multiple speakers exist. These inputs are inevitably present in speaker diarisation because of overlapped speech and speaker changes; they degrade the performance. To mitigate the first problem, we generate speaker verification evaluation protocols that mimic the diarisation scenario better. We propose two data augmentation techniques to alleviate the second problem, making embedding extractors aware of overlapped speech or speaker change input. One technique generates overlapped speech segments, and the other generates segments where two speakers utter sequentially. Extensive experimental results using three state-of-the-art speaker embedding extractors demonstrate that both proposed approaches are effective.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/QIYKY47W/Jung et al. - 2022 - In search of strong embedding extractors for speak.pdf;/Users/brono/Zotero/storage/F3VRILH6/2210.html}
}

@article{kadiriExtractionUtilizationExcitation2021,
  title = {Extraction and {{Utilization}} of {{Excitation Information}} of {{Speech}}: {{A Review}}},
  shorttitle = {Extraction and {{Utilization}} of {{Excitation Information}} of {{Speech}}},
  author = {Kadiri, Sudarsana Reddy and Alku, Paavo and Yegnanarayana, B.},
  date = {2021-12},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {109},
  number = {12},
  pages = {1920--1941},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2021.3126493},
  url = {https://ieeexplore.ieee.org/document/9628174/},
  urldate = {2023-03-20},
  abstract = {Speech production can be regarded as a process where a time-varying vocal tract system (filter) is excited by a time-varying excitation. In addition to its linguistic message, the speech signal also carries information about, for example, the gender and age of the speaker. Moreover, the speech signal includes acoustical cues about several speaker traits, such as the emotional state and the state of health of the speaker. In order to understand the production of these acoustical cues by the human speech production mechanism and utilize this information in speech technology, it is necessary to extract features describing both the excitation and the filter of the human speech production mechanism. While the methods to estimate and parameterize the vocal tract system are well established, the excitation appears less studied. This article provides a review of signal processing approaches used for the extraction of excitation information from speech. This article highlights the importance of excitation information in the analysis and classification of phonation type and vocal emotions, in the analysis of nonverbal laughter sounds, and in studying pathological voices. Furthermore, recent developments of deep learning techniques in the context of extraction and utilization of the excitation information are discussed.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/GBHUGNBA/Kadiri et al. - 2021 - Extraction and Utilization of Excitation Informati.pdf}
}

@inproceedings{karthikRobustSpeakerDiarization2018,
  title = {Robust {{Speaker Diarization}} for {{News Broadcast}}},
  booktitle = {2018 {{International Conference}} on {{Wireless Communications}}, {{Signal Processing}} and {{Networking}} ({{WiSPNET}})},
  author = {Karthik, M. L. N. S. and Ganesh, Mirishkar Sai and Patnaik, Bijayananda},
  date = {2018-03},
  pages = {1--4},
  doi = {10.1109/WiSPNET.2018.8538527},
  abstract = {This contribution presents an efficient method of speaker diarization that employs bayesian information criterion for speaker embeddings. In contrast to the traditional approaches the speaker segmentation is done using manually spectral features. The proposed method is capable enough to segment audio recording of a broadcast news by i-vectors as well as GMM speaker model and the conventional GMM based agglomerative for clustering the data. An unsupervised Voice Active Detector (VAD) has been developed, so that it could distinguish between speech frame and non-speech frame such that the non-speech frames can be discarded. The results of our proposed method showed significantly outperformed with the benchmark methods and reduced the diarization error margin by 14\%.},
  eventtitle = {2018 {{International Conference}} on {{Wireless Communications}}, {{Signal Processing}} and {{Networking}} ({{WiSPNET}})},
  keywords = {Bayes methods,Benchmark testing,Clustering,Data models,Detectors,Discrete cosine transforms,Feature extraction,Hidden Markov models,Speaker Diarization,Speaker segmentation,Voice Active Detector},
  file = {/Users/brono/Zotero/storage/W4XAYN2V/Karthik et al. - 2018 - Robust Speaker Diarization for News Broadcast.pdf;/Users/brono/Zotero/storage/I2LP6KP3/stamp.html}
}

@article{kellyIvectorsXvectorsGenerational,
  title = {From I-Vectors to x-Vectors – a Generational Change in Speaker Recognition Illustrated on the {{NFI-FRIDA}} Database},
  author = {Kelly, Finnian and Alexander, Anil and Forth, Oscar},
  langid = {english},
  file = {/Users/brono/Zotero/storage/P6WVIXV5/Kelly et al. - From i-vectors to x-vectors – a generational chang.pdf}
}

@inproceedings{khanMultilingualConversationalTelephony2016,
  title = {Multilingual Conversational Telephony Speech Corpus Creation for Real World Speaker Diarization and Recognition},
  booktitle = {2016 {{Conference}} of {{The Oriental Chapter}} of {{International Committee}} for {{Coordination}} and {{Standardization}} of {{Speech Databases}} and {{Assessment Techniques}} ({{O-COCOSDA}})},
  author = {Khan, Soma and Basu, Joyanta and Pal, Madhab and Roy, Rajib and Bepari, Milton Samirakshma},
  date = {2016-10},
  pages = {177--182},
  publisher = {{IEEE}},
  location = {{Bali, Indonesia}},
  doi = {10.1109/ICSDA.2016.7919007},
  url = {http://ieeexplore.ieee.org/document/7919007/},
  urldate = {2023-04-18},
  eventtitle = {2016 {{Conference}} of {{The Oriental Chapter}} of {{International Committee}} for {{Coordination}} and {{Standardization}} of {{Speech Databases}} and {{Assessment Techniques}} ({{O-COCOSDA}})},
  isbn = {978-1-5090-3516-8},
  file = {/Users/brono/Zotero/storage/2N7B7WC3/Khan et al. - 2016 - Multilingual conversational telephony speech corpu.pdf}
}

@article{kiktovaComparisonDiarizationTools2015,
  title = {Comparison of {{Diarization Tools}} for {{Building Speaker Database}}},
  author = {Kiktova, Eva and Juhar, Jozef},
  date = {2015-11-29},
  journaltitle = {Advances in Electrical and Electronic Engineering},
  shortjournal = {AEEE},
  volume = {13},
  number = {4},
  pages = {314--319},
  issn = {1804-3119, 1336-1376},
  doi = {10.15598/aeee.v13i4.1468},
  url = {http://advances.utc.sk/index.php/AEEE/article/view/1468},
  urldate = {2023-03-12},
  file = {/Users/brono/Zotero/storage/TUHS5ZLH/Kiktova and Juhar - 2015 - Comparison of Diarization Tools for Building Speak.pdf}
}

@article{kiktovaComparisonDiarizationTools2015a,
  title = {Comparison of {{Diarization Tools}} for {{Building Speaker Database}}},
  author = {Kiktova, Eva and Juhar, Jozef},
  date = {2015-11-29},
  journaltitle = {Advances in Electrical and Electronic Engineering},
  shortjournal = {AEEE},
  volume = {13},
  number = {4},
  pages = {314--319},
  issn = {1804-3119, 1336-1376},
  doi = {10.15598/aeee.v13i4.1468},
  url = {http://advances.utc.sk/index.php/AEEE/article/view/1468},
  urldate = {2023-03-13},
  file = {/Users/brono/Zotero/storage/C2ZMTSCH/Kiktova and Juhar - 2015 - Comparison of Diarization Tools for Building Speak.pdf}
}

@article{kiktovaComparisonDiarizationTools2015b,
  title = {Comparison of {{Diarization Tools}} for {{Building Speaker Database}}},
  author = {Kiktova, Eva and Juhar, Jozef},
  date = {2015-11-29},
  journaltitle = {Advances in Electrical and Electronic Engineering},
  shortjournal = {AEEE},
  volume = {13},
  number = {4},
  pages = {314--319},
  issn = {1804-3119, 1336-1376},
  doi = {10.15598/aeee.v13i4.1468},
  url = {http://advances.utc.sk/index.php/AEEE/article/view/1468},
  urldate = {2023-03-20},
  file = {/Users/brono/Zotero/storage/BDMWJKVG/Kiktova and Juhar - 2015 - Comparison of Diarization Tools for Building Speak.pdf;/Users/brono/Zotero/storage/ZW4YGXR7/Sandouk-Ubai-ProgressReport.pdf}
}

@online{kinoshitaAdvancesIntegrationEndtoend2021,
  title = {Advances in Integration of End-to-End Neural and Clustering-Based Diarization for Real Conversational Speech},
  author = {Kinoshita, Keisuke and Delcroix, Marc and Tawara, Naohiro},
  date = {2021-08-31},
  eprint = {2105.09040},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2105.09040},
  urldate = {2023-03-13},
  abstract = {Recently, we proposed a novel speaker diarization method called End-to-End-Neural-Diarization-vector clustering (EEND-vector clustering) that integrates clustering-based and end-to-end neural network-based diarization approaches into one framework. The proposed method combines advantages of both frameworks, i.e. high diarization performance and handling of overlapped speech based on EEND, and robust handling of long recordings with an arbitrary number of speakers based on clustering-based approaches. However, the method was only evaluated so far on simulated 2-speaker meeting-like data. This paper is to (1) report recent advances we made to this framework, including newly introduced robust constrained clustering algorithms, and (2) experimentally show that the method can now significantly outperform competitive diarization methods such as Encoder-Decoder Attractor (EDA)-EEND, on CALLHOME data which comprises real conversational speech data including overlapped speech and an arbitrary number of speakers. By further analyzing the experimental results, this paper also discusses pros and cons of the proposed method and reveals potential for further improvement. A set of the code to reproduce the results is available at https://github.com/nttcslab-sp/EEND-vector-clustering.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/HEWJ9Q79/Kinoshita et al. - 2021 - Advances in integration of end-to-end neural and c.pdf;/Users/brono/Zotero/storage/MSAPAFGE/2105.html}
}

@inproceedings{kinoshitaIntegratingEndtoEndNeural2021,
  title = {Integrating {{End-to-End Neural}} and {{Clustering-Based Diarization}}: {{Getting}} the {{Best}} of {{Both Worlds}}},
  shorttitle = {Integrating {{End-to-End Neural}} and {{Clustering-Based Diarization}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Kinoshita, Keisuke and Delcroix, Marc and Tawara, Naohiro},
  date = {2021-06-06},
  pages = {7198--7202},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9414333},
  url = {https://ieeexplore.ieee.org/document/9414333/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  file = {/Users/brono/Zotero/storage/SYCK4HQG/Kinoshita et al. - 2021 - Integrating End-to-End Neural and Clustering-Based.pdf}
}

@article{knoxWhereDidGo2012,
  title = {Where Did {{I}} Go Wrong?: {{Identifying}} Troublesome Segments for Speaker Diarization Systems},
  author = {Knox, Mary Tai and Mirghafori, Nikki and Friedland, Gerald},
  date = {2012},
  abstract = {The focus of this work is to identify types of segments that are difficult for speaker diarization systems. The diarization outputs of five state-of-the-art systems are analyzed on short/long segments as well as segments surrounding speaker changepoints. We found that for all five systems as the duration of the segment decreased the diarization error rate (DER) increased. Also, segments immediately preceding and following speaker changepoints performed much worse than their respective counterparts. In fact, at least 40\% of the DER for all five systems is attributed to time within 0.5 seconds of a speaker changepoint. We hope the results of this work motivate future improvements of speaker diarization systems.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/HPJKGSFZ/Knox et al. - 2012 - Where did I go wrong Identifying troublesome seg.pdf}
}

@online{koluguriMetalearningRobustChildadult2019,
  title = {Meta-Learning for Robust Child-Adult Classification from Speech},
  author = {Koluguri, Nithin Rao and Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Narayanan, Shrikanth},
  date = {2019-10-28},
  eprint = {1910.11400},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1910.11400},
  urldate = {2023-04-17},
  abstract = {Computational modeling of naturalistic conversations in clinical applications has seen growing interest in the past decade. An important use-case involves child-adult interactions within the autism diagnosis and intervention domain. In this paper, we address a specific sub-problem of speaker diarization, namely child-adult speaker classification in such dyadic conversations with specified roles. Training a speaker classification system robust to speaker and channel conditions is challenging due to inherent variability in the speech within children and the adult interlocutors. In this work, we propose the use of meta-learning, in particular, prototypical networks which optimize a metric space across multiple tasks. By modeling every child-adult pair in the training set as a separate task during meta-training, we learn a representation with improved generalizability compared to conventional supervised learning. We demonstrate improvements over state-of-the-art speaker embeddings (x-vectors) under two evaluation settings: weakly supervised classification (up to 14.53\% relative improvement in F1-scores) and clustering (up to relative 9.66\% improvement in cluster purity). Our results show that protonets can potentially extract robust speaker embeddings for child-adult classification from speech.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/ZNVXTTGB/Koluguri et al. - 2019 - Meta-learning for robust child-adult classificatio.pdf;/Users/brono/Zotero/storage/XH8PYLUE/1910.html}
}

@article{kothalkarChildVsAdult2021,
  title = {Child vs {{Adult Speaker Diarization}} of Naturalistic Audio Recordings in Preschool Environment Using {{Deep Neural Networks}}},
  author = {Kothalkar, Prasanna V and Hansen, John H L and Irvin, Dwight and Buzhardt, Jay},
  date = {2021},
  abstract = {Speech and language development in children is crucial for ensuring optimal outcomes in their long term development and life-long educational journey. A child’s vocabulary size at the time of kindergarten entry is an early indicator of learning to read and potential long-term success in school. The preschool classroom is thus a promising venue for monitoring growth in young children by measuring their interactions with teachers and classmates. Automatic Speech Recognition (ASR) technologies provide the ability for ‘Early Childhood’ researchers for automatically analyzing naturalistic recordings in these settings. For this purpose, data are collected in a high-quality childcare center in the United States using Language Environment Analysis (LENA) devices worn by the preschool children. A preliminary task for ASR of daylong audio recordings would involve diarization, i.e., segmenting speech into smaller parts for identifying ‘who spoke when.’ This study investigates a Deep Learning-based diarization system for classroom interactions of 3-5-year-old children. However, the focus is on ’speaker group’ diarization, which includes classifying speech segments as being from adults or children from across multiple classrooms. SincNet based diarization systems achieve utterance level Diarization Error Rate of 19.1\%. Utterance level speaker group confusion matrices also show promising, balanced results. These diarization systems have potential applications in developing metrics for adult-to-child or child-to-child rapid conversational turns in a naturalistic noisy early childhood setting. Such technical advancements will also help teachers better and more efficiently quantify and understand their interactions with children, make changes as needed, and monitor the impact of those changes.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/PMNRJFRQ/Kothalkar et al. - 2021 - Child vs Adult Speaker Diarization of naturalistic.pdf}
}

@inproceedings{kothalkarMeasuringFrequencyChilddirected2021,
  title = {Measuring {{Frequency}} of {{Child-directed WH-Question Words}} for {{Alternate Preschool Locations}} Using {{Speech Recognition}} and {{Location Tracking Technologies}}},
  booktitle = {Companion {{Publication}} of the 2021 {{International Conference}} on {{Multimodal Interaction}}},
  author = {Kothalkar, Prasanna V. and Datla, Sathvik and Dutta, Satwik and Hansen, John H. L. and Seven, Yagmur and Irvin, Dwight and Buzhardt, Jay},
  date = {2021-10-18},
  pages = {414--418},
  publisher = {{ACM}},
  location = {{Montreal QC Canada}},
  doi = {10.1145/3461615.3485440},
  url = {https://dl.acm.org/doi/10.1145/3461615.3485440},
  urldate = {2023-04-04},
  eventtitle = {{{ICMI}} '21: {{INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION}}},
  isbn = {978-1-4503-8471-1},
  langid = {english},
  file = {/Users/brono/Zotero/storage/EH8ATN6W/Kothalkar et al. - 2021 - Measuring Frequency of Child-directed WH-Question .pdf}
}

@article{kottiSpeakerSegmentationClustering2008,
  title = {Speaker Segmentation and Clustering},
  author = {Kotti, Margarita and Moschou, Vassiliki and Kotropoulos, Constantine},
  date = {2008-05},
  journaltitle = {Signal Processing},
  shortjournal = {Signal Processing},
  volume = {88},
  number = {5},
  pages = {1091--1124},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2007.11.017},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S016516840700391X},
  urldate = {2023-03-20},
  abstract = {This survey focuses on two challenging speech processing topics, namely: speaker segmentation and speaker clustering. Speaker segmentation aims at finding speaker change points in an audio stream, whereas speaker clustering aims at grouping speech segments based on speaker characteristics. Model-based, metric-based, and hybrid speaker segmentation algorithms are reviewed. Concerning speaker clustering, deterministic and probabilistic algorithms are examined. A comparative assessment of the reviewed algorithms is undertaken, the algorithm advantages and disadvantages are indicated, insight to the algorithms is offered, and deductions as well as recommendations are given. Rich transcription and movie analysis are candidate applications that benefit from combined speaker segmentation and clustering.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/9DLDLZUH/Kotti et al. - 2008 - Speaker segmentation and clustering.pdf}
}

@inproceedings{krishnamachariDevelopingNeuralRepresentations2021,
  title = {Developing {{Neural Representations}} for {{Robust Child-Adult Diarization}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Krishnamachari, Suchitra and Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Narayanan, Shrikanth},
  date = {2021-01-19},
  pages = {590--597},
  publisher = {{IEEE}},
  location = {{Shenzhen, China}},
  doi = {10.1109/SLT48900.2021.9383488},
  url = {https://ieeexplore.ieee.org/document/9383488/},
  urldate = {2023-04-17},
  abstract = {Automated processing and analysis of child speech has been long acknowledged as a harder problem compared to understanding speech by adults. Specifically, conversations between a child and adult involve spontaneous speech which often compounds idiosyncrasies associated with child speech. In this work, we improve upon the task of speaker diarization (determining who spoke when) from audio of child-adult conversations in naturalistic settings. We select conversations from the autism diagnosis and intervention domains, wherein speaker diarization forms an important step towards computational behavioral analysis in support of clinical research and decision making. We train deep speaker embeddings using publicly available child speech and adult speech corpora, unlike predominant state-of-art models which typically utilize only adult speech for speaker embedding training. We demonstrate significant reductions in relative diarization error rate (DER) on DIHARD II (dev) sessions containing child speech (22.88\%) and two internal corpora representing interactions involving children with Autism: excerpts from ADOS Mod3 sessions (33.7\%) and combination of fulllength ADOS and BOSCC sessions (44.99\%). Further, we validate our improvements in identifying the child speaker (typically with short speaking time) using the recall measure. Finally, we analyze the effect of fundamental frequency augmentation and the effect of child age, gender on speaker diarization performance.},
  eventtitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  isbn = {978-1-72817-066-4},
  langid = {english},
  file = {/Users/brono/Zotero/storage/MFC9SYB4/Krishnamachari et al. - 2021 - Developing Neural Representations for Robust Child.pdf}
}

@article{kumarABSPSystemThird,
  title = {{{ABSP System}} for {{The Third DIHARD Challenge}}},
  author = {Kumar, A Kishore and Waldekar, Shefali and Saha, Goutam},
  abstract = {This report describes the speaker diarization system developed by the ABSP Laboratory team for the third DIHARD speech diarization challenge. Our primary contribution is to develop acoustic domain identification (ADI) system for speaker diarization. We investigate speaker embeddings based ADI system. We apply a domain-dependent threshold for agglomerative hierarchical clustering. Besides, we optimize the parameters for PCA-based dimensionality reduction in a domain-dependent way. Our method of integrating domain-based processing schemes in the baseline system of the challenge achieved a relative improvement of 9.63\% and 10.64\% in DER for core and full conditions, respectively, for Track 1 of the DIHARD III evaluation set.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/SML54765/Kumar et al. - ABSP System for The Third DIHARD Challenge.pdf}
}

@article{kumarImprovingSpeakerDiarization2020a,
  title = {Improving Speaker Diarization for Naturalistic Child-Adult Conversational Interactions Using Contextual Information},
  author = {Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Narayanan, Shrikanth},
  date = {2020-02},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {147},
  number = {2},
  pages = {EL196-EL200},
  issn = {0001-4966},
  doi = {10.1121/10.0000736},
  url = {http://asa.scitation.org/doi/10.1121/10.0000736},
  urldate = {2023-04-17},
  langid = {english},
  file = {/Users/brono/Zotero/storage/UR6MH9BD/Kumar et al. - 2020 - Improving speaker diarization for naturalistic chi.pdf}
}

@article{kumarLeveragingLinguisticContext2020,
  title = {Leveraging {{Linguistic Context}} in {{Dyadic Interactions}} to {{Improve Automatic Speech Recognition}} for {{Children}}},
  author = {Kumar, Manoj and Kim, So Hyun and Lord, Catherine and Lyon, Thomas D. and Narayanan, Shrikanth},
  date = {2020-09},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {63},
  pages = {101101},
  issn = {08852308},
  doi = {10.1016/j.csl.2020.101101},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0885230820300346},
  urldate = {2023-03-30},
  abstract = {Automatic speech recognition for child speech has been long considered a more challenging problem than for adult speech. Various contributing factors have been identified such as larger acoustic speech variability including mispronunciations due to continuing biological changes in growth, developing vocabulary and linguistic skills, and scarcity of training corpora. A further challenge arises when dealing with spontaneous speech of children involved in a conversational interaction, and especially when the child may have limited or impaired communication ability. This includes health applications, one of the motivating domains of this paper, that involve goal-oriented dyadic interactions between a child and clinician/adult social partner as a part of behavioral assessment. In this work, we use linguistic context information from the interaction to adapt speech recognition models for children speech. Specifically, spoken language from the interacting adult speech provides the context for the child’s speech. We propose two methods to exploit this context: lexical repetitions and semantic response generation. For the latter, we make use of sequence-to-sequence models that learn to predict the target child utterance given context adult utterances. Long-term context is incorporated in the model by propagating the cell-state across the duration of conversation. We use interpolation techniques to adapt language models at the utterance level, and analyze the effect of length and direction of context (forward and backward). Two different domains are used in our experiments to demonstrate the generalized nature of our methods - interactions between a child with ASD and an adult social partner in a play-based, naturalistic setting, and in forensic interviews between a child and a trained interviewer. In both cases, context-adapted models yield significant improvement (upto 10.71\% in absolute word error rate) over the baseline and perform consistently across context windows and directions. Using statistical analysis, we investigate the effect of source-based (adult) and target-based (child) factors on adaptation methods. Our results demonstrate the applicability of our modeling approach in improving child speech recognition by employing information transfer from the adult interlocutor.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/AQPGUD8X/Kumar et al. - 2020 - Leveraging Linguistic Context in Dyadic Interactio.pdf}
}

@article{kumarRobustAcousticDomain2022,
  title = {Robust Acoustic Domain Identification with Its Application to Speaker Diarization},
  author = {Kumar, A Kishore and Waldekar, Shefali and Sahidullah, Md and Saha, Goutam},
  date = {2022-12},
  journaltitle = {International Journal of Speech Technology},
  shortjournal = {Int J Speech Technol},
  volume = {25},
  number = {4},
  pages = {933--945},
  issn = {1381-2416, 1572-8110},
  doi = {10.1007/s10772-022-09990-9},
  url = {https://link.springer.com/10.1007/s10772-022-09990-9},
  urldate = {2023-03-12},
  langid = {english},
  file = {/Users/brono/Zotero/storage/XF9ILVRH/Kumar et al. - 2022 - Robust acoustic domain identification with its app.pdf}
}

@online{kwonMultiscaleSpeakerEmbeddingbased2021,
  title = {Multi-Scale Speaker Embedding-Based Graph Attention Networks for Speaker Diarisation},
  author = {Kwon, Youngki and Heo, Hee-Soo and Jung, Jee-weon and Kim, You Jin and Lee, Bong-Jin and Chung, Joon Son},
  date = {2021-10-07},
  eprint = {2110.03361},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2110.03361},
  urldate = {2023-03-07},
  abstract = {The objective of this work is effective speaker diarisation using multi-scale speaker embeddings. Typically, there is a trade-off between the ability to recognise short speaker segments and the discriminative power of the embedding, according to the segment length used for embedding extraction. To this end, recent works have proposed the use of multi-scale embeddings where segments with varying lengths are used. However, the scores are combined using a weighted summation scheme where the weights are fixed after the training phase, whereas the importance of segment lengths can differ with in a single session. To address this issue, we present three key contributions in this paper: (1) we propose graph attention networks for multi-scale speaker diarisation; (2) we design scale indicators to utilise scale information of each embedding; (3) we adapt the attention-based aggregation to utilise a pre-computed affinity matrix from multi-scale embeddings. We demonstrate the effectiveness of our method in various datasets where the speaker confusion which constitutes the primary metric drops over 10\% in average relative compared to the baseline.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/N2CQUYW2/Kwon et al. - 2021 - Multi-scale speaker embedding-based graph attentio.pdf;/Users/brono/Zotero/storage/GD3H666L/2110.html}
}

@article{lamichhaneSpeakerDiarizationEmbeddings,
  title = {Speaker {{Diarization}} with {{Embeddings}} from a {{VGGish Model}}},
  author = {Lamichhane, Bishal},
  abstract = {VGGish model based embeddings are commonly used for representing audio segments in several audio processing applications. In this work, the relevance of VGGish model embeddings for speaker diarization in the context of the DIHARD-III challenge was investigated. The pre-trained VGGish network (VGGish-Vanilla) is primarily developed as an audio classification model. Therefore, the VGGish-Vanilla model was adapted towards speaker recognition using the Voxceleb1 dataset to obtain VGGish-Adapted model. Then embeddings from the VGGish-Vanilla and the VGGish-Adapted were compared with the commonly used Xvectors for the diarization task. A simple diarization pipeline based on gaussian divergence for speaker segmentation and AHC clustering with cosine distance based scoring was used in this comparison. The adaptation of VGGish network towards speaker recognition improved diarization performance. However, the embeddings from VGGish models were still not competitive compared to the Xvectors. Given that the VGGish network is primarily trained for audio classification, the VGGish embeddings could rather be useful for the identification of audio context/domain and adaptation of the diarization pipeline based on the identified context/domain. The preliminary evaluations done in this work, with AHC threshold adapted to identified audio context, showed that diarization pipeline adaptation gives marginal gains in diarization performance. The adaptive threshold provided gains in the DIHARD-III baseline system also where DER improved from 20.31 (with global threshold) to 19.58 (with adaptive threshold) in the development set. Further investigation is required to better understand how and where pretrained networks like VGGish model could be useful in speaker diarization tasks. Identification of audio context/domain based on VGGish embeddings for a context-dependent diarization model seems to be a promising direction for further explorations.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/A9DJW2QQ/Lamichhane - Speaker Diarization with Embeddings from a VGGish .pdf}
}

@inproceedings{landiniAnalysisDiarizationSystem2021,
  title = {Analysis of the but {{Diarization System}} for {{Voxconverse Challenge}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Landini, Federico and Glembek, Ondrej and Matejka, Pavel and Rohdin, Johan and Burget, Lukas and Diez, Mireia and Silnova, Anna},
  date = {2021-06-06},
  pages = {5819--5823},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9414315},
  url = {https://ieeexplore.ieee.org/document/9414315/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  file = {/Users/brono/Zotero/storage/6ATHH58P/Landini et al. - 2021 - Analysis of the but Diarization System for Voxconv.pdf}
}

@article{landiniBayesianHMMClustering2022,
  title = {Bayesian {{HMM}} Clustering of X-Vector Sequences ({{VBx}}) in Speaker Diarization: {{Theory}}, Implementation and Analysis on Standard Tasks},
  shorttitle = {Bayesian {{HMM}} Clustering of X-Vector Sequences ({{VBx}}) in Speaker Diarization},
  author = {Landini, Federico and Profant, Ján and Diez, Mireia and Burget, Lukáš},
  date = {2022-01-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {71},
  pages = {101254},
  issn = {0885-2308},
  doi = {10.1016/j.csl.2021.101254},
  url = {https://www.sciencedirect.com/science/article/pii/S0885230821000619},
  urldate = {2023-03-13},
  abstract = {The recently proposed VBx diarization method uses a Bayesian hidden Markov model to find speaker clusters in a sequence of x-vectors. In this work we perform an extensive comparison of performance of the VBx diarization with other approaches in the literature and we show that VBx achieves superior performance on three of the most popular datasets for evaluating diarization: CALLHOME, AMI and DIHARD II datasets. Further, we present for the first time the derivation and update formulae for the VBx model, focusing on the efficiency and simplicity of this model as compared to the previous and more complex BHMM model working on frame-by-frame standard Cepstral features. Together with this publication, we release the recipe for training the x-vector extractors used in our experiments on both wide and narrowband data, and the VBx recipes that attain state-of-the-art performance on all three datasets. Besides, we point out the lack of a standardized evaluation protocol for AMI dataset and we propose a new protocol for both Beamformed and Mix-Headset audios based on the official AMI partitions and transcriptions.},
  langid = {english},
  keywords = {AMI,HMM,Speaker diarization,Variational Bayes,x-vector},
  file = {/Users/brono/Zotero/storage/64NKQEI7/Landini et al. - 2022 - Bayesian HMM clustering of x-vector sequences (VBx.pdf;/Users/brono/Zotero/storage/ST34KEMV/Landini et al. - 2022 - Bayesian HMM clustering of x-vector sequences (VBx.pdf;/Users/brono/Zotero/storage/TGEPVWNT/S0885230821000619.html}
}

@online{landiniSimulatedMixturesSimulated2022,
  title = {From {{Simulated Mixtures}} to {{Simulated Conversations}} as {{Training Data}} for {{End-to-End Neural Diarization}}},
  author = {Landini, Federico and Lozano-Diez, Alicia and Diez, Mireia and Burget, Lukáš},
  date = {2022-06-25},
  eprint = {2204.00890},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2204.00890},
  urldate = {2023-04-15},
  abstract = {End-to-end neural diarization (EEND) is nowadays one of the most prominent research topics in speaker diarization. EEND presents an attractive alternative to standard cascaded diarization systems since a single system is trained at once to deal with the whole diarization problem. Several EEND variants and approaches are being proposed, however, all these models require large amounts of annotated data for training but available annotated data are scarce. Thus, EEND works have used mostly simulated mixtures for training. However, simulated mixtures do not resemble real conversations in many aspects. In this work we present an alternative method for creating synthetic conversations that resemble real ones by using statistics about distributions of pauses and overlaps estimated on genuine conversations. Furthermore, we analyze the effect of the source of the statistics, different augmentations and amounts of data. We demonstrate that our approach performs substantially better than the original one, while reducing the dependence on the fine-tuning stage. Experiments are carried out on 2-speaker telephone conversations of Callhome and DIHARD 3. Together with this publication, we release our implementations of EEND and the method for creating simulated conversations.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/9TWGRJYC/Landini et al. - 2022 - From Simulated Mixtures to Simulated Conversations.pdf}
}

@online{landiniSimulatedMixturesSimulated2022a,
  title = {From {{Simulated Mixtures}} to {{Simulated Conversations}} as {{Training Data}} for {{End-to-End Neural Diarization}}},
  author = {Landini, Federico and Lozano-Diez, Alicia and Diez, Mireia and Burget, Lukáš},
  date = {2022-06-25},
  eprint = {2204.00890},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2204.00890},
  urldate = {2023-04-16},
  abstract = {End-to-end neural diarization (EEND) is nowadays one of the most prominent research topics in speaker diarization. EEND presents an attractive alternative to standard cascaded diarization systems since a single system is trained at once to deal with the whole diarization problem. Several EEND variants and approaches are being proposed, however, all these models require large amounts of annotated data for training but available annotated data are scarce. Thus, EEND works have used mostly simulated mixtures for training. However, simulated mixtures do not resemble real conversations in many aspects. In this work we present an alternative method for creating synthetic conversations that resemble real ones by using statistics about distributions of pauses and overlaps estimated on genuine conversations. Furthermore, we analyze the effect of the source of the statistics, different augmentations and amounts of data. We demonstrate that our approach performs substantially better than the original one, while reducing the dependence on the fine-tuning stage. Experiments are carried out on 2-speaker telephone conversations of Callhome and DIHARD 3. Together with this publication, we release our implementations of EEND and the method for creating simulated conversations.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/DY2QIKEV/Landini et al. - 2022 - From Simulated Mixtures to Simulated Conversations.pdf;/Users/brono/Zotero/storage/QTCM22VJ/2204.html}
}

@online{landiniSystemDescriptionDIHARD2019,
  title = {{{BUT System Description}} for {{DIHARD Speech Diarization Challenge}} 2019},
  author = {Landini, Federico and Wang, Shuai and Diez, Mireia and Burget, Lukáš and Matějka, Pavel and Žmolíková, Kateřina and Mošner, Ladislav and Plchot, Oldřich and Novotný, Ondřej and Zeinali, Hossein and Rohdin, Johan},
  date = {2019-10-19},
  eprint = {1910.08847},
  eprinttype = {arxiv},
  eprintclass = {eess},
  url = {http://arxiv.org/abs/1910.08847},
  urldate = {2023-03-12},
  abstract = {This paper describes the systems developed by the BUT team for the four tracks of the second DIHARD speech diarization challenge. For tracks 1 and 2 the systems were based on performing agglomerative hierarchical clustering (AHC) over x-vectors, followed by the Bayesian Hidden Markov Model (HMM) with eigenvoice priors applied at x-vector level followed by the same approach applied at frame level. For tracks 3 and 4, the systems were based on performing AHC using x-vectors extracted on all channels.},
  pubstate = {preprint},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing,x-vector},
  file = {/Users/brono/Zotero/storage/5UQG5B6S/Landini et al. - 2019 - BUT System Description for DIHARD Speech Diarizati.pdf;/Users/brono/Zotero/storage/VGQZ4CBA/1910.html}
}

@article{landiniSystemDescriptionThird,
  title = {{{BUT System Description}} for {{The Third DIHARD Speech Diarization Challenge}}},
  author = {Landini, Federico and Lozano-Diez, Alicia and Burget, Lukasˇ and Diez, Mireia and Silnova, Anna},
  abstract = {This is the system description corresponding to the systems developed by the BUT team for The Third DIHARD Speech Diarization Challenge. The systems for both tracks consist of a DOVERlap fusion of an end-to-end NN system with xvector based clustering systems in the form of spectral clustering and VBx. Given that the x-vector clustering systems do not provide overlapping speakers, overlapped speech is detected by a TasNet-based detector before the final fusion with the end-to-end approach.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/H47L749K/Landini et al. - BUT System Description for The Third DIHARD Speech.pdf}
}

@inproceedings{larcherSpeakerEmbeddingsDiarization2021,
  title = {Speaker {{Embeddings}} for {{Diarization}} of {{Broadcast Data In The Allies Challenge}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Larcher, Anthony and Mehrish, Ambuj and Tahon, Marie and Meignier, Sylvain and Carrive, Jean and Doukhan, David and Galibert, Olivier and Evans, Nicholas},
  date = {2021-06-06},
  pages = {5799--5803},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9414215},
  url = {https://ieeexplore.ieee.org/document/9414215/},
  urldate = {2023-03-12},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  file = {/Users/brono/Zotero/storage/8Y39ZF27/Larcher et al. - 2021 - Speaker Embeddings for Diarization of Broadcast Da.pdf}
}

@online{leeSpectralClusteringawareLearning2022,
  title = {Spectral {{Clustering-aware Learning}} of {{Embeddings}} for {{Speaker Diarisation}}},
  author = {Lee, Evonne P. C. and Sun, Guangzhi and Zhang, Chao and Woodland, Philip C.},
  date = {2022-10-24},
  eprint = {2210.13576},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2210.13576},
  urldate = {2023-02-27},
  abstract = {In speaker diarisation, speaker embedding extraction models often suffer from the mismatch between their training loss functions and the speaker clustering method. In this paper, we propose the method of spectral clustering-aware learning of embeddings (SCALE) to address the mismatch. Specifically, besides an angular prototype cal (AP) loss, SCALE uses a novel affinity matrix loss which directly minimises the error between the affinity matrix estimated from speaker embeddings and the reference. SCALE also includes p-percentile thresholding and Gaussian blur as two important hyper-parameters for spectral clustering in training. Experiments on the AMI dataset showed that speaker embeddings obtained with SCALE achieved over 50\% relative speaker error rate reductions using oracle segmentation, and over 30\% relative diarisation error rate reductions using automatic segmentation when compared to a strong baseline with the AP-loss-based speaker embeddings.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,TODO},
  file = {/Users/brono/Zotero/storage/ZA3XK9PD/Lee et al. - 2022 - Spectral Clustering-aware Learning of Embeddings f.pdf;/Users/brono/Zotero/storage/R6USGJ9X/2210.html}
}

@inproceedings{lefrancACLEWDiViMeEasytouse2018,
  title = {The {{ACLEW DiViMe}}: {{An Easy-to-use Diarization Tool}}},
  shorttitle = {The {{ACLEW DiViMe}}},
  booktitle = {Interspeech 2018},
  author = {Le Franc, Adrien and Riebling, Eric and Karadayi, Julien and Wang, Yun and Scaff, Camila and Metze, Florian and Cristia, Alejandrina},
  date = {2018-09-02},
  pages = {1383--1387},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-2324},
  url = {https://www.isca-speech.org/archive/interspeech_2018/lefranc18_interspeech.html},
  urldate = {2023-03-30},
  abstract = {We present “DiViMe”, an open-source virtual machine aimed at packaging speech technology for real-life data, and developed in the context of the “Analyzing Children’s Language Environments across the World” Project. This first release focuses on Speech Activity Detection, Speaker Diarization, and their evaluation. The present paper introduces the set of included tools and the current workflow, which is focused on making minimal assumptions regarding users’ technical skills. Additionally, we show how the current DiViMe tools fare against three sets of challenging data. In a first experiment, we look at performance with samples extracted from daylong recordings gathered using the LENATM system from English-learning children. We find that the performance of the tools currently in DiViMe is not far from that achieved by the LENATM proprietary software. In a second experiment, we generalize to other samples of child-centered daylong files, gathered with non-LENATM hardware from non-English-learning children, showing that performance does not degrade in this condition. Finally, we report on performance in the DiHARD 2018 Challenge Test Data. Originally conceived in the “Speech Recognition Virtual Kitchen”, DiViMe is a promising platform for packaging speech technology tools for widespread re-use, with potential impact on both fundamental and applied speech and language research.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/8M8KJV6A/Le Franc et al. - 2018 - The ACLEW DiViMe An Easy-to-use Diarization Tool.pdf}
}

@inproceedings{lefrancACLEWDiViMeEasytouse2018a,
  title = {The {{ACLEW DiViMe}}: {{An Easy-to-use Diarization Tool}}},
  shorttitle = {The {{ACLEW DiViMe}}},
  booktitle = {Interspeech 2018},
  author = {Le Franc, Adrien and Riebling, Eric and Karadayi, Julien and Wang, Yun and Scaff, Camila and Metze, Florian and Cristia, Alejandrina},
  date = {2018-09-02},
  pages = {1383--1387},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-2324},
  url = {https://www.isca-speech.org/archive/interspeech_2018/lefranc18_interspeech.html},
  urldate = {2023-04-19},
  abstract = {We present “DiViMe”, an open-source virtual machine aimed at packaging speech technology for real-life data, and developed in the context of the “Analyzing Children’s Language Environments across the World” Project. This first release focuses on Speech Activity Detection, Speaker Diarization, and their evaluation. The present paper introduces the set of included tools and the current workflow, which is focused on making minimal assumptions regarding users’ technical skills. Additionally, we show how the current DiViMe tools fare against three sets of challenging data. In a first experiment, we look at performance with samples extracted from daylong recordings gathered using the LENATM system from English-learning children. We find that the performance of the tools currently in DiViMe is not far from that achieved by the LENATM proprietary software. In a second experiment, we generalize to other samples of child-centered daylong files, gathered with non-LENATM hardware from non-English-learning children, showing that performance does not degrade in this condition. Finally, we report on performance in the DiHARD 2018 Challenge Test Data. Originally conceived in the “Speech Recognition Virtual Kitchen”, DiViMe is a promising platform for packaging speech technology tools for widespread re-use, with potential impact on both fundamental and applied speech and language research.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/KVWUFVPV/Le Franc et al. - 2018 - The ACLEW DiViMe An Easy-to-use Diarization Tool.pdf}
}

@article{leungEndtoEndSpeakerDiarization,
  title = {End-to-{{End Speaker Diarization System}} for the {{Third DIHARD Challenge System Description}}},
  author = {Leung, Tsun-Yat and Samarakoon, Lahiru},
  abstract = {This work aims to improve the recently proposed self-attentive end-to-end diarization model with encoder-decoder based attractors (EDA-EEND) for the third DIHARD Challenge. We propose to (1) replace the transformer encoders with conformer encoders to capture local information; (2) use convolutional upsampling to increase result resolution; (3) incorporate the attention mechanism into the attractor calculation; (4) add the additive margin penalty to increase the robustness; (5) shuffle chunks in each recording to increase combinations. In DIHARD III track 2, our final system achieved 23.86\% and 20.05\% diarization error rate (DER) on core evaluation set and full evaluation set, respectively, while the strong DIHARD III conventional baseline achieved 27.34\% and 25.36\% DER.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/8RHUWEN2/Leung and Samarakoon - End-to-End Speaker Diarization System for the Thir.pdf}
}

@article{liCIAICSystemDescriptions,
  title = {{{CIAIC System Descriptions}}},
  author = {Li, Meng-Zhen and Gong, Yi-jun},
  abstract = {We make use of a x-vector extractor model which was trained by Brno University of Technology during the Second DIHARD Diarization Challenge. The clustering methods we used was agglomerative hierarchical clustering (AHC) and variational Bayes based clustering(VBx). Experimental results show that the VBx systems are better than the AHC systems in both develapment set(DEV) and evaluation set(EVAL).},
  langid = {english},
  file = {/Users/brono/Zotero/storage/8R9X4UQX/Li and Gong - CIAIC System Descriptions.pdf}
}

@online{liCrosslingualSpeakerVerification2017,
  title = {Cross-Lingual {{Speaker Verification}} with {{Deep Feature Learning}}},
  author = {Li, Lantian and Wang, Dong and Rozi, Askar and Zheng, Thomas Fang},
  date = {2017-06-22},
  eprint = {1706.07861},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1706.07861},
  urldate = {2023-06-14},
  abstract = {Existing speaker verification (SV) systems often suffer from performance degradation if there is any language mismatch between model training, speaker enrollment, and test. A major cause of this degradation is that most existing SV methods rely on a probabilistic model to infer the speaker factor, so any significant change on the distribution of the speech signal will impact the inference. Recently, we proposed a deep learning model that can learn how to extract the speaker factor by a deep neural network (DNN). By this feature learning, an SV system can be constructed with a very simple back-end model. In this paper, we investigate the robustness of the feature-based SV system in situations with language mismatch. Our experiments were conducted on a complex cross-lingual scenario, where the model training was in English, and the enrollment and test were in Chinese or Uyghur. The experiments demonstrated that the feature-based system outperformed the i-vector system with a large margin, particularly with language mismatch between enrollment and test.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound},
  file = {/Users/brono/Zotero/storage/SR93T8LF/Li et al. - 2017 - Cross-lingual Speaker Verification with Deep Featu.pdf;/Users/brono/Zotero/storage/YWL427R7/1706.html}
}

@online{liDiscriminativeNeuralClustering2020,
  title = {Discriminative {{Neural Clustering}} for {{Speaker Diarisation}}},
  author = {Li, Qiujia and Kreyssig, Florian L. and Zhang, Chao and Woodland, Philip C.},
  date = {2020-11-23},
  eprint = {1910.09703},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1910.09703},
  urldate = {2023-03-01},
  abstract = {In this paper, we propose Discriminative Neural Clustering (DNC) that formulates data clustering with a maximum number of clusters as a supervised sequence-to-sequence learning problem. Compared to traditional unsupervised clustering algorithms, DNC learns clustering patterns from training data without requiring an explicit definition of a similarity measure. An implementation of DNC based on the Transformer architecture is shown to be effective on a speaker diarisation task using the challenging AMI dataset. Since AMI contains only 147 complete meetings as individual input sequences, data scarcity is a significant issue for training a Transformer model for DNC. Accordingly, this paper proposes three data augmentation schemes: sub-sequence randomisation, input vector randomisation, and Diaconis augmentation, which generates new data samples by rotating the entire input sequence of L2-normalised speaker embeddings. Experimental results on AMI show that DNC achieves a reduction in speaker error rate (SER) of 29.4\% relative to spectral clustering.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/7ER9IL65/Li et al. - 2020 - Discriminative Neural Clustering for Speaker Diari.pdf;/Users/brono/Zotero/storage/PNZFR3J2/1910.html}
}

@online{liImprovingCodeswitchingLanguage2021,
  title = {Improving {{Code-switching Language Modeling}} with {{Artificially Generated Texts}} Using {{Cycle-consistent Adversarial Networks}}},
  author = {Li, Chia-Yu and Vu, Ngoc Thang},
  date = {2021-12-12},
  eprint = {2112.06327},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.06327},
  urldate = {2023-06-09},
  abstract = {This paper presents our latest effort on improving Code-switching language models that suffer from data scarcity. We investigate methods to augment Code-switching training text data by artificially generating them. Concretely, we propose a cycle-consistent adversarial networks based framework to transfer monolingual text into Code-switching text, considering Code-switching as a speaking style. Our experimental results on the SEAME corpus show that utilising artificially generated Code-switching text data improves consistently the language model as well as the automatic speech recognition performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/CCUEBGEQ/Li and Vu - 2021 - Improving Code-switching Language Modeling with Ar.pdf;/Users/brono/Zotero/storage/9TYZ3ATH/2112.html}
}

@article{liMandarinEnglishCodeSwitchingCorpus,
  title = {A {{Mandarin-English Code-Switching Corpus}}},
  author = {Li, Ying and Yu, Yue and Fung, Pascale},
  abstract = {Generally the existing monolingual corpora are not suitable for large vocabulary continuous speech recognition (LVCSR) of codeswitching speech. The motivation of this paper is to study the rules and constraints code-switching follows and design a corpus for code-switching LVCSR task. This paper presents the development of a Mandarin-English code-switching corpus. This corpus consists of four parts: 1) conversational meeting speech and its data; 2) project meeting speech data; 3) student interviews speech; 4) text data of on-line news. The speech was transcribed by an annotator and verified by Mandarin-English bilingual speakers manually. We propose an approach for automatically downloading from the web text data that contains code-switching. The corpus includes both intra-sentential code-switching (switch in the middle of a sentence) and inter-sentential code-switching (switch at the end of the sentence). The distribution of part-of-speech (POS) tags and code-switching reasons are reported.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/DM362447/Li et al. - A Mandarin-English Code-Switching Corpus.pdf}
}

@inproceedings{linLSTMBasedSimilarity2019,
  title = {{{LSTM}} Based {{Similarity Measurement}} with {{Spectral Clustering}} for {{Speaker Diarization}}},
  booktitle = {Interspeech 2019},
  author = {Lin, Qingjian and Yin, Ruiqing and Li, Ming and Bredin, Hervé and Barras, Claude},
  date = {2019-09-15},
  eprint = {1907.10393},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  pages = {366--370},
  doi = {10.21437/Interspeech.2019-1388},
  url = {http://arxiv.org/abs/1907.10393},
  urldate = {2023-03-01},
  abstract = {More and more neural network approaches have achieved considerable improvement upon submodules of speaker diarization system, including speaker change detection and segment-wise speaker embedding extraction. Still, in the clustering stage, traditional algorithms like probabilistic linear discriminant analysis (PLDA) are widely used for scoring the similarity between two speech segments. In this paper, we propose a supervised method to measure the similarity matrix between all segments of an audio recording with sequential bidirectional long short-term memory networks (Bi-LSTM). Spectral clustering is applied on top of the similarity matrix to further improve the performance. Experimental results show that our system significantly outperforms the state-of-the-art methods and achieves a diarization error rate of 6.63\% on the NIST SRE 2000 CALLHOME database.},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,LSTM,Statistics - Machine Learning,TODO},
  file = {/Users/brono/Zotero/storage/QXFFKWDF/Lin et al. - 2019 - LSTM based Similarity Measurement with Spectral Cl.pdf;/Users/brono/Zotero/storage/X4KETZWZ/1907.html}
}

@article{linPolyUSubmissionThird,
  title = {{{PolyU Submission}} to the {{Third DIHARD Challenge}}},
  author = {Lin, Weiwei and Mak, Man-Wai},
  abstract = {This paper describes the system developed by HK PolyU for the Third DIHARD Speech Diarization challenge. Unlike the official baseline, which employs a very sophisticated pipeline including probabilistic linear discriminant analysis (PLDA) and variational Bayes hidden Markov model (VB-HMM) re-segmentation, our system relies entirely on the speaker embeddings obtained from a DenseNet. For each fixed-length speech segment, we computed the cosine distance scores between its speaker embedding and the speaker embeddings of the adjacent speech segments to identify the speaker turns. Then we used cosine distance again as a metric for agglomerative hierarchical clustering (AHC). Despite the straightforward approach, we produce competitive results.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/W2A7GANW/Lin and Mak - PolyU Submission to the Third DIHARD Challenge.pdf}
}

@inproceedings{linSelfAttentiveSimilarityMeasurement2020,
  title = {Self-{{Attentive Similarity Measurement Strategies}} in {{Speaker Diarization}}},
  booktitle = {Interspeech 2020},
  author = {Lin, Qingjian and Hou, Yu and Li, Ming},
  date = {2020-10-25},
  pages = {284--288},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2020-1908},
  url = {https://www.isca-speech.org/archive/interspeech_2020/lin20_interspeech.html},
  urldate = {2023-04-03},
  abstract = {In this paper, we present the submitted system for the third DIHARD Speech Diarization Challenge from the DKUDuke-Lenovo team. Our system consists of several modules: voice activity detection (VAD), segmentation, speaker embedding extraction, attentive similarity scoring, agglomerative hierarchical clustering. In addition, the target speaker VAD (TSVAD) is used for the phone call data to further improve the performance. Our final submitted system achieves a DER of 15.43\% for the core evaluation set and 13.39\% for the full evaluation set on task 1, and we also get a DER of 21.63\% for core evaluation set and 18.90\% for full evaluation set on task 2.},
  eventtitle = {Interspeech 2020},
  langid = {english},
  file = {/Users/brono/Zotero/storage/TX68TE8N/Lin et al. - 2020 - Self-Attentive Similarity Measurement Strategies i.pdf}
}

@online{liTALCSOpenSourceMandarinEnglish2022,
  title = {{{TALCS}}: {{An Open-Source Mandarin-English Code-Switching Corpus}} and a {{Speech Recognition Baseline}}},
  shorttitle = {{{TALCS}}},
  author = {Li, Chengfei and Deng, Shuhao and Wang, Yaoping and Wang, Guangjing and Gong, Yaguang and Chen, Changbin and Bai, Jinfeng},
  date = {2022-06-27},
  eprint = {2206.13135},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2206.13135},
  urldate = {2023-06-10},
  abstract = {This paper introduces a new corpus of Mandarin-English code-switching speech recognition--TALCS corpus, suitable for training and evaluating code-switching speech recognition systems. TALCS corpus is derived from real online one-to-one English teaching scenes in TAL education group, which contains roughly 587 hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the largest well labeled Mandarin-English code-switching open source automatic speech recognition (ASR) dataset in the world. In this paper, we will introduce the recording procedure in detail, including audio capturing devices and corpus environments. And the TALCS corpus is freely available for download under the permissive license1. Using TALCS corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet and Wenet. The Mixture Error Rate (MER) performance in the two speech recognition toolkits is compared in TALCS corpus. The experimental results implies that the quality of audio recordings and transcriptions are promising and the baseline system is workable.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/9Y9G2TER/Li et al. - 2022 - TALCS An Open-Source Mandarin-English Code-Switch.pdf;/Users/brono/Zotero/storage/J96YGR9E/Li et al. - 2022 - TALCS An Open-Source Mandarin-English Code-Switch.pdf;/Users/brono/Zotero/storage/NXEEL8WH/2206.html}
}

@inproceedings{liuEndtoEndLanguageDiarization2021,
  title = {End-to-{{End Language Diarization}} for {{Bilingual Code-Switching Speech}}},
  booktitle = {Interspeech 2021},
  author = {Liu, Hexin and Perera, Leibny Paola García and Zhang, Xinyi and Dauwels, Justin and Khong, Andy W.H. and Khudanpur, Sanjeev and Styles, Suzy J.},
  date = {2021-08-30},
  pages = {1489--1493},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-82},
  url = {https://www.isca-speech.org/archive/interspeech_2021/liu21d_interspeech.html},
  urldate = {2023-04-02},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/Users/brono/Zotero/storage/S4QSAI6K/Liu et al. - 2021 - End-to-End Language Diarization for Bilingual Code.pdf}
}

@inproceedings{liuEndtoendNeuralDiarization2021,
  title = {End-to-End {{Neural Diarization}}: {{From Transformer}} to {{Conformer}}},
  shorttitle = {End-to-End {{Neural Diarization}}},
  booktitle = {Interspeech 2021},
  author = {Liu, Yi Chieh and Han, Eunjung and Lee, Chul and Stolcke, Andreas},
  date = {2021-08-30},
  eprint = {2106.07167},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {3081--3085},
  doi = {10.21437/Interspeech.2021-1909},
  url = {http://arxiv.org/abs/2106.07167},
  urldate = {2023-03-13},
  abstract = {We propose a new end-to-end neural diarization (EEND) system that is based on Conformer, a recently proposed neural architecture that combines convolutional mappings and Transformer to model both local and global dependencies in speech. We first show that data augmentation and convolutional subsampling layers enhance the original self-attentive EEND in the Transformer-based EEND, and then Conformer gives an additional gain over the Transformer-based EEND. However, we notice that the Conformer-based EEND does not generalize as well from simulated to real conversation data as the Transformer-based model. This leads us to quantify the mismatch between simulated data and real speaker behavior in terms of temporal statistics reflecting turn-taking between speakers, and investigate its correlation with diarization error. By mixing simulated and real data in EEND training, we mitigate the mismatch further, with Conformer-based EEND achieving 24\% error reduction over the baseline SA-EEND system, and 10\% improvement over the best augmented Transformer-based system, on two-speaker CALLHOME data.},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/MTTWVGV4/Liu et al. - 2021 - End-to-end Neural Diarization From Transformer to.pdf;/Users/brono/Zotero/storage/ASZT5I2M/2106.html}
}

@online{liuReducingLanguageConfusion2022,
  title = {Reducing {{Language}} Confusion for {{Code-switching Speech Recognition}} with {{Token-level Language Diarization}}},
  author = {Liu, Hexin and Xu, Haihua and Garcia, Leibny Paola and Khong, Andy W. H. and He, Yi and Khudanpur, Sanjeev},
  date = {2022-10-26},
  eprint = {2210.14567},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2210.14567},
  urldate = {2023-04-02},
  abstract = {Code-switching (CS) refers to the phenomenon that languages switch within a speech signal and leads to language confusion for automatic speech recognition (ASR). This paper aims to address language confusion for improving CSASR from two perspectives: incorporating and disentangling language information. We incorporate language information in the CS-ASR model by dynamically biasing the model with token-level language posteriors which are outputs of a sequence-to-sequence auxiliary language diarization module. In contrast, the disentangling process reduces the difference between languages via adversarial training so as to normalize two languages. We conduct the experiments on the SEAME dataset. Compared to the baseline model, both the joint optimization with LD and the language posterior bias achieve performance improvement. The comparison of the proposed methods indicates that incorporating language information is more effective than disentangling for reducing language confusion in CS speech.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/34PXLFS6/Liu et al. - 2022 - Reducing Language confusion for Code-switching Spe.pdf}
}

@article{liuSpeakerDiarizationSystem,
  title = {Speaker {{Diarization System Based}} on {{GMM}} and {{BIC}}},
  author = {Liu, Tantan and Liu, Xiaoxing and Yan, Yonghong},
  abstract = {This paper presents an approach for speaker diarization based on a novel combination of Gaussian mixture model (GMM) and standard Bayesian information criterion (BIC). Gaussian mixture model provides a good description of feature vector distribution and BIC enables a proper merging and stopping criterion. Our system combines the advantage of these two method and yields favorable performance. Experiments carried out on mandarin broadcast news data demonstrate the advantage of the proposed approach, which shows better performance than the approach only based on GMM clustering.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/P9EFWSKF/Liu et al. - Speaker Diarization System Based on GMM and BIC.pdf}
}

@inproceedings{liWhyAutomaticRecognition2001,
  title = {Why Is Automatic Recognition of Children's Speech Difficult?},
  author = {Li, Qun and Russell, Martin},
  date = {2001-09-03},
  pages = {2671--2674},
  doi = {10.21437/Eurospeech.2001-625},
  file = {/Users/brono/Zotero/storage/TC6UR8MB/Li and Russell - 2001 - Why is automatic recognition of children's speech .ocr.pdf}
}

@article{lleidaAlbayzin2018Evaluation2019,
  title = {Albayzin 2018 {{Evaluation}}: {{The IberSpeech-RTVE Challenge}} on {{Speech Technologies}} for {{Spanish Broadcast Media}}},
  shorttitle = {Albayzin 2018 {{Evaluation}}},
  author = {Lleida, Eduardo and Ortega, Alfonso and Miguel, Antonio and Bazán-Gil, Virginia and Pérez, Carmen and Gómez, Manuel and family=Prada, given=Alberto, prefix=de, useprefix=true},
  date = {2019-01},
  journaltitle = {Applied Sciences},
  volume = {9},
  number = {24},
  pages = {5412},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app9245412},
  url = {https://www.mdpi.com/2076-3417/9/24/5412},
  urldate = {2023-04-03},
  abstract = {The IberSpeech-RTVE Challenge presented at IberSpeech 2018 is a new Albayzin evaluation series supported by the Spanish Thematic Network on Speech Technologies (Red Temática en Tecnologías del Habla (RTTH)). That series was focused on speech-to-text transcription, speaker diarization, and multimodal diarization of television programs. For this purpose, the Corporacion Radio Television Española (RTVE), the main public service broadcaster in Spain, and the RTVE Chair at the University of Zaragoza made more than 500 h of broadcast content and subtitles available for scientists. The dataset included about 20 programs of different kinds and topics produced and broadcast by RTVE between 2015 and 2018. The programs presented different challenges from the point of view of speech technologies such as: the diversity of Spanish accents, overlapping speech, spontaneous speech, acoustic variability, background noise, or specific vocabulary. This paper describes the database and the evaluation process and summarizes the results obtained.},
  issue = {24},
  langid = {english},
  keywords = {Albayzin evaluation,IberSpeech Challenge,multimodal diarization,RTVE2018 database,speaker diarization,speech-to-text transcription},
  file = {/Users/brono/Zotero/storage/E9TNBHM8/Lleida et al. - 2019 - Albayzin 2018 Evaluation The IberSpeech-RTVE Chal.pdf}
}

@article{longAcousticDataAugmentation2020,
  title = {Acoustic Data Augmentation for {{Mandarin-English}} Code-Switching Speech Recognition},
  author = {Long, Yanhua and Li, Yijie and Zhang, Qiaozheng and Wei, Shuang and Ye, Hong and Yang, Jichen},
  date = {2020-04},
  journaltitle = {Applied Acoustics},
  shortjournal = {Applied Acoustics},
  volume = {161},
  pages = {107175},
  issn = {0003682X},
  doi = {10.1016/j.apacoust.2019.107175},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X19309442},
  urldate = {2023-04-18},
  abstract = {Code-switching (CS) is a multilingual phenomenon where a speaker uses different languages in an utterance or between alternating utterances. Developing large-scale datasets for training code-switching acoustic and language models is challenging and extremely expensive. In this paper, we focus on the acoustic data augmentation for the Mandarin-English CS speech recognition task. Effectiveness of conventional acoustic data augmentation approaches are examined. More importantly, we propose a CS acoustic event detection system based on the deep neural network to extract real code-switching speech segments automatically. Then, the semi-supervised and active learning techniques are investigated to generate transcriptions of these segments. Finally, code-switching speech synthesis system is introduced to further enhance the acoustic modeling. Experimental results on the OC16-CE80 data, a MandarinEnglish mixlingual speech corpus, demonstrate the effectiveness of the proposed methods.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/ACKAJQAE/Long et al. - 2020 - Acoustic data augmentation for Mandarin-English co.pdf}
}

@online{loveniaASCENDSpontaneousChineseEnglish2022,
  title = {{{ASCEND}}: {{A Spontaneous Chinese-English Dataset}} for {{Code-switching}} in {{Multi-turn Conversation}}},
  shorttitle = {{{ASCEND}}},
  author = {Lovenia, Holy and Cahyawijaya, Samuel and Winata, Genta Indra and Xu, Peng and Yan, Xu and Liu, Zihan and Frieske, Rita and Yu, Tiezheng and Dai, Wenliang and Barezi, Elham J. and Chen, Qifeng and Ma, Xiaojuan and Shi, Bertram E. and Fung, Pascale},
  date = {2022-05-03},
  eprint = {2112.06223},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2112.06223},
  urldate = {2023-06-09},
  abstract = {Code-switching is a speech phenomenon occurring when a speaker switches language during a conversation. Despite the spontaneous nature of code-switching in conversational spoken language, most existing works collect code-switching data from read speech instead of spontaneous speech. ASCEND (A Spontaneous Chinese-English Dataset) is a high-quality Mandarin Chinese-English code-switching corpus built on spontaneous multi-turn conversational dialogue sources collected in Hong Kong. We report ASCEND's design and procedure for collecting the speech data, including annotations. ASCEND consists of 10.62 hours of clean speech, collected from 23 bilingual speakers of Chinese and English. Furthermore, we conduct baseline experiments using pre-trained wav2vec 2.0 models, achieving a best performance of 22.69\textbackslash\% character error rate and 27.05\% mixed error rate.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/XU8YRDGY/Lovenia et al. - 2022 - ASCEND A Spontaneous Chinese-English Dataset for .pdf;/Users/brono/Zotero/storage/WXNE6QG2/2112.html}
}

@online{luoEndtoEndCodeSwitchingSpeech2018,
  title = {Towards {{End-to-End Code-Switching Speech Recognition}}},
  author = {Luo, Ne and Jiang, Dongwei and Zhao, Shuaijiang and Gong, Caixia and Zou, Wei and Li, Xiangang},
  date = {2018-11-01},
  eprint = {1810.13091},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1810.13091},
  urldate = {2023-04-16},
  abstract = {Code-switching speech recognition has attracted an increasing interest recently, but the need for expert linguistic knowledge has always been a big issue. End-to-end automatic speech recognition (ASR) simplifies the building of ASR systems considerably by predicting graphemes or characters directly from acoustic input. In the mean time, the need of expert linguistic knowledge is also eliminated, which makes it an attractive choice for code-switching ASR. This paper presents a hybrid CTC-Attention based end-to-end Mandarin-English code-switching (CS) speech recognition system and studies the effect of hybrid CTC-Attention based models, different modeling units, the inclusion of language identification and different decoding strategies on the task of code-switching ASR. On the SEAME corpus, our system achieves a mixed error rate (MER) of 34.24\%.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/2HXCLU6K/Luo et al. - 2018 - Towards End-to-End Code-Switching Speech Recogniti.pdf;/Users/brono/Zotero/storage/KC4V94B3/1810.html}
}

@article{lyudovykCODESWITCHINGSPEECHRECOGNITION2014,
  title = {{{CODE-SWITCHING SPEECH RECOGNITION FOR CLOSELY RELATED LANGUAGES}}},
  author = {Lyudovyk, Tetyana and Pylypenko, Valeriy},
  date = {2014},
  journaltitle = {St. Petersburg},
  abstract = {This work presents an approach to recognition of multispeaker conversational speech with code-switching between Ukrainian and Russian languages. Both inter-sentential and intra-sentential code-switching is handled. The approach takes into account peculiarities of phonetic systems of the closely related Russian and Ukrainian languages. A crosslingual LVCSR system is developed. The acoustic model and pronunciation lexicon are based on Ukrainian phone set. Modeling of pronunciation variation in lexicons helps to cope not only with code-switching speech but also with accented speech. Results of code-switching speech recognition are presented. The approach is suitable especially in cases of intra-sentential code-switching where language identification is problematic.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/5ESU3J8D/Lyudovyk and Pylypenko - 2014 - CODE-SWITCHING SPEECH RECOGNITION FOR CLOSELY RELA.pdf}
}

@inproceedings{lyuLanguageDiarizationCodeswitch2013,
  title = {Language Diarization for Code-Switch Conversational Speech},
  booktitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lyu, Dau-Cheng and Chng, Eng-Siong and Li, Haizhou},
  date = {2013-05},
  pages = {7314--7318},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2013.6639083},
  abstract = {This paper examines the process of language diarization, the process to perform language segmentation and recognition, in a code-switched speech. Towards this task, we have developed a 63 hours conversational code-switch corpus recorded from Singapore/Malaysia speakers. We show that code-switching can occur frequently and the average language interval may be as short as one second. As such, language diarization is challenging task. To process such short segments, we propose a language diarization system using long term context feature across several phone-based segments and the combination of acoustics and phonotactic information. We achieved a frame error rate of 14.7\% for language diarization performance on a Mandarin-English code-switch corpus. To evaluate our system, we measured the language recognition performance on monolingual segments extracted from the code-switch corpus against published techniques of LID systems - we obtained a relative equal error rate reduction of 5.2\%, 13.8\%, 15.1\% and 17.9\% on speech durations of 0.1 to 0.5 sec., 0.5 to 1 sec., 1 to 3 sec. and 3 to 9 sec respectively.},
  eventtitle = {2013 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  keywords = {Acoustics,code-switch,conversational speech,Error analysis,Feature extraction,language diarization,language recognition,Speech,Speech coding,Speech processing,Speech recognition},
  file = {/Users/brono/Zotero/storage/QWS2ZW28/Lyu et al. - 2013 - Language diarization for code-switch conversationa.pdf;/Users/brono/Zotero/storage/SEEFIGAN/Language_diarization_for_code-switch_conversational_speech.pdf;/Users/brono/Zotero/storage/R7Y49GCT/stamp.html}
}

@inproceedings{lyuLanguageDiarizationConversational2013,
  title = {Language Diarization for Conversational Code-Switch Speech with Pronunciation Dictionary Adaptation},
  booktitle = {2013 {{IEEE China Summit}} and {{International Conference}} on {{Signal}} and {{Information Processing}}},
  author = {Lyu, Dau-Cheng and Chng, Eng-Siong and Li, Haizhou},
  date = {2013-07},
  pages = {147--150},
  doi = {10.1109/ChinaSIP.2013.6625316},
  abstract = {Language diarization is the task to perform automatic language segmentation and recognition in a code-switch speech. Towards this task, we developed a conversational Mandarin-English code-switch corpus spoken by Singaporean/Malaysian speakers. We also developed a Singapore accent specific pronunciation dictionary, with which we built a Singapore accent phone recognizer to extract long term context phonotactic feature. Our experiment shows that accent-specific phone recognizer is essential to improve language diarization performance. Specifically, the language diarization experiment, the phonotactic features generated by the Singapore accent phone recognizer has a 6.5\% relative frame error rate reduction over the phone recognizer using the CMU dictionary. In addition, the ASR performance using this dictionary on the Singapore English corpus achieved 21\% relative word error rate reduction over the system using the American accent CMU dictionary.},
  eventtitle = {2013 {{IEEE China Summit}} and {{International Conference}} on {{Signal}} and {{Information Processing}}},
  keywords = {code-switched speech,Dictionaries,Educational institutions,Error analysis,Feature extraction,language diarization,language recognition,pronunciation adaptation,Speech,Speech coding,Speech recognition},
  file = {/Users/brono/Zotero/storage/BGP3WHUI/Lyu et al. - 2013 - Language diarization for conversational code-switc.pdf;/Users/brono/Zotero/storage/SHUZM7M5/stamp.html}
}

@book{lyuLanguageIdentificationCodeswitching2008,
  title = {Language Identification on Code-Switching Utterances Using Multiple Cues},
  author = {Lyu, Dau-Cheng and Lyu, Ren-Yuan},
  date = {2008-09-22},
  pages = {714},
  doi = {10.21437/Interspeech.2008-223},
  abstract = {Code-switching speech is an utterance containing two or more languages. Usually, the switching linguistic unit is in clause or word levels. In this paper, a two-stage framework is proposed, containing a language identifier and then a speech recognizer, to evaluate on a Mandarin-Taiwanese code- switching utterance. In the language identifier, we use multiple cues including acoustic, prosodic and phonetic features. In order to integrate the cues to distinguish one language from another, we used a maximum a posteriori decision rule to connect an acoustic model, a duration model and a language model. In the experiments, we have achieved 34.5\% (LID) and 17.7\% (ASR) error rate reduction comparing with one stage LVCSR-based system.},
  pagetotal = {711},
  file = {/Users/brono/Zotero/storage/XDM7GTR5/Lyu and Lyu - 2008 - Language identification on code-switching utteranc.pdf}
}

@article{lyuMandarinEnglishCodeswitching2015,
  title = {Mandarin–{{English}} Code-Switching Speech Corpus in {{South-East Asia}}: {{SEAME}}},
  shorttitle = {Mandarin–{{English}} Code-Switching Speech Corpus in {{South-East Asia}}},
  author = {Lyu, Dau-Cheng and Tan, Tien-Ping and Chng, Eng-Siong and Li, Haizhou},
  date = {2015-09},
  journaltitle = {Language Resources and Evaluation},
  shortjournal = {Lang Resources \& Evaluation},
  volume = {49},
  number = {3},
  pages = {581--600},
  issn = {1574-020X, 1574-0218},
  doi = {10.1007/s10579-015-9303-x},
  url = {http://link.springer.com/10.1007/s10579-015-9303-x},
  urldate = {2023-04-16},
  abstract = {This paper introduces the South East Asia Mandarin–English corpus, a 63-h spontaneous Mandarin–English code-switching transcribed speech corpus suitable for LVCSR and language change detection/identification research. The corpus is recorded under unscripted interview and conversational settings from 157 Singaporean and Malaysian speakers who spoke a mixture of Mandarin and English within a single sentence. About 82 \% of the transcribed utterances are intra-sentential code-switching speech and the corpus will be release by LDC in 2015. This paper presents an analysis of the code-switching statistics of the corpus, such as the duration of monolingual segments and the frequency of language turns in codeswitch utterances. We also summarize the development effort, details such as the processing time for transcription, validation and language boundary labelling.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/QMFUTI9V/Lyu et al. - 2015 - Mandarin–English code-switching speech corpus in S.pdf}
}

@inproceedings{lyuSEAMEMandarinEnglishCodeswitching2010,
  title = {{{SEAME}}: A {{Mandarin-English}} Code-Switching Speech Corpus in South-East Asia},
  shorttitle = {{{SEAME}}},
  booktitle = {Interspeech 2010},
  author = {Lyu, Dau-Cheng and Tan, Tien-Ping and Chng, Eng Siong and Li, Haizhou},
  date = {2010-09-26},
  pages = {1986--1989},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2010-563},
  url = {https://www.isca-speech.org/archive/interspeech_2010/lyu10_interspeech.html},
  urldate = {2023-04-03},
  abstract = {In Singapore and Malaysia, people often speak a mixture of Mandarin and English within a single sentence. We call such sentences intra-sentential code-switch sentences. In this paper, we report on the development of a Mandarin-English codeswitching spontaneous speech corpus: SEAME. The corpus is developed as part of a multilingual speech recognition project and will be used to examine how Mandarin-English codeswitch speech occurs in the spoken language in South-East Asia. Additionally, it can provide insights into the development of large vocabulary continuous speech recognition (LVCSR) for code-switching speech. The corpus collected consists of intra-sentential code-switching utterances that are recorded under both interview and conversational settings. This paper describes the corpus design and the analysis of collected corpus.},
  eventtitle = {Interspeech 2010},
  langid = {english},
  file = {/Users/brono/Zotero/storage/WXW4M3LI/Lyu et al. - 2010 - SEAME a Mandarin-English code-switching speech co.pdf}
}

@article{macwhinneyToolsAnalyzingTalk2019,
  title = {Tools for {{Analyzing Talk Part}} 1: {{The CHAT Transcription Format}}},
  author = {MacWhinney, Brian},
  date = {2019},
  publisher = {{TalkBank}},
  doi = {10.21415/3MHN-0Z89},
  url = {https://talkbank.org/manuals/CHAT.pdf},
  urldate = {2023-06-27},
  langid = {english},
  file = {/Users/brono/Zotero/storage/H48H54MM/MacWhinney - 2019 - CHAT Manual.pdf}
}

@online{maitiEndtoEndDiarizationVariable2021,
  title = {End-to-{{End Diarization}} for {{Variable Number}} of {{Speakers}} with {{Local-Global Networks}} and {{Discriminative Speaker Embeddings}}},
  author = {Maiti, Soumi and Erdogan, Hakan and Wilson, Kevin and Wisdom, Scott and Watanabe, Shinji and Hershey, John R.},
  date = {2021-05-05},
  eprint = {2105.02096},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2105.02096},
  urldate = {2023-03-03},
  abstract = {We present an end-to-end deep network model that performs meeting diarization from single-channel audio recordings. End-to-end diarization models have the advantage of handling speaker overlap and enabling straightforward handling of discriminative training, unlike traditional clustering-based diarization methods. The proposed system is designed to handle meetings with unknown numbers of speakers, using variable-number permutation-invariant cross-entropy based loss functions. We introduce several components that appear to help with diarization performance, including a local convolutional network followed by a global self-attention module, multi-task transfer learning using a speaker identification component, and a sequential approach where the model is refined with a second stage. These are trained and validated on simulated meeting data based on LibriSpeech and LibriTTS datasets; final evaluations are done using LibriCSS, which consists of simulated meetings recorded using real acoustics via loudspeaker playback. The proposed model performs better than previously proposed end-to-end diarization models on these data.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,E2E,Electrical Engineering and Systems Science - Audio and Speech Processing,overlap},
  file = {/Users/brono/Zotero/storage/PDTV6QV7/Maiti et al. - 2021 - End-to-End Diarization for Variable Number of Spea.pdf;/Users/brono/Zotero/storage/EE995TWT/2105.html}
}

@online{maoSpeechRecognitionMultiSpeaker2020,
  title = {Speech {{Recognition}} and {{Multi-Speaker Diarization}} of {{Long Conversations}}},
  author = {Mao, Huanru Henry and Li, Shuyang and McAuley, Julian and Cottrell, Garrison},
  date = {2020-11-04},
  eprint = {2005.08072},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2005.08072},
  urldate = {2023-03-13},
  abstract = {Speech recognition (ASR) and speaker diarization (SD) models have traditionally been trained separately to produce rich conversation transcripts with speaker labels. Recent advances have shown that joint ASR and SD models can learn to leverage audio-lexical inter-dependencies to improve word diarization performance. We introduce a new benchmark of hour-long podcasts collected from the weekly This American Life radio program to better compare these approaches when applied to extended multi-speaker conversations. We find that training separate ASR and SD models perform better when utterance boundaries are known but otherwise joint models can perform better. To handle long conversations with unknown utterance boundaries, we introduce a striding attention decoding algorithm and data augmentation techniques which, combined with model pre-training, improves ASR and SD.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/ULQHVHP5/Mao et al. - 2020 - Speech Recognition and Multi-Speaker Diarization o.pdf;/Users/brono/Zotero/storage/NBVE65YU/2005.html}
}

@online{marreddyNoteMFCCsDelta,
  title = {A Note on {{MFCCs}} and Delta Features},
  author = {Marreddy, Mounika},
  url = {https://mounikamarreddy.github.io/2019-07-26-delta-feats/},
  urldate = {2023-04-14},
  abstract = {What are MFCCs and how are they computed? Feature extraction is the first step in any automatic speech recognition (ASR) pipeline. The objective is to compute features from speech waveforms which contain relevant information about the linguistic content of the speech, and ignore information about the background noise, emotions, etc....},
  langid = {english},
  file = {/Users/brono/Zotero/storage/F68VSC3N/2019-07-26-delta-feats.html}
}

@article{matejuSpeechActivitySpeaker,
  title = {Speech {{Activity}} and {{Speaker Change Point Detection}} for {{Online Streams}}},
  author = {Matějů, Ing Lukáš},
  abstract = {The main focus of the thesis lies on two closely interrelated tasks, speech activity detection and speaker change point detection, and their applications in online processing. These tasks commonly play a crucial role of speech preprocessors utilized in speech-processing applications, such as automatic speech recognition or speaker diarization. While their use in offline systems is extensively covered in literature, the number of published works focusing on online use is limited. This is unfortunate, as many speech-processing applications (e.g., monitoring systems) are required to be run in real time. The thesis begins with a three-chapter opening part, where the first introductory chapter explains the basic concepts and outlines the practical use of both tasks. It is followed by a chapter, which reviews the current state of the art and lists the existing toolkits. That part is concluded by a chapter explaining the motivation behind the thesis and the practical use in monitoring systems; ultimately, this chapter sets the main goals of the thesis. In the thesis, the next two chapters cover the theoretical background of both tasks. They present selected approaches relevant to the work (used for result comparisons) or focused on online use. The following chapter proposes the final speech activity detection approach for online use. Within this chapter, a detailed description of the development of this approach is available as well as its thorough experimental evaluation. This approach yields state-of-the-art results under low- and medium-noise conditions on the standardized QUT-NOISE-TIMIT corpus. It is also integrated into a monitoring system, where it supplements a speech recognition system. The final speaker change point detection approach is proposed in the following chapter. It was designed in a series of consecutive experiments, which are extensively detailed in this chapter. An experimental evaluation of this approach on the COST278 database shows the performance of approaching the offline reference system while operating in online mode with low latency. Finally, the last chapter summarizes all the results of the thesis.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/YAWHBQXD/Matějů - Speech Activity and Speaker Change Point Detection.pdf}
}

@inproceedings{mccreeSpeakerDiarizationUsing2019,
  title = {Speaker {{Diarization Using Leave-One-Out Gaussian PLDA Clustering}} of {{DNN Embeddings}}},
  booktitle = {Interspeech 2019},
  author = {McCree, Alan and Sell, Gregory and Garcia-Romero, Daniel},
  date = {2019-09-15},
  pages = {381--385},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2912},
  url = {https://www.isca-speech.org/archive/interspeech_2019/mccree19_interspeech.html},
  urldate = {2023-03-16},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/Users/brono/Zotero/storage/3IBSNR3Y/McCree et al. - 2019 - Speaker Diarization Using Leave-One-Out Gaussian P.pdf}
}

@inproceedings{mcknightAnalysisPhoneticDependence2021,
  title = {Analysis of {{Phonetic Dependence}} of {{Segmentation Errors}} in {{Speaker Diarization}}},
  booktitle = {2020 28th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {McKnight, Simon W. and Hogg, Aidan O. T. and Naylor, Patrick A.},
  date = {2021-01},
  pages = {381--385},
  issn = {2076-1465},
  doi = {10.23919/Eusipco47968.2020.9287552},
  abstract = {Evaluation of speaker segmentation and diarization normally makes use of forgiveness collars around ground truth speaker segment boundaries such that estimated speaker segment boundaries with such collars are considered completely correct. This paper shows that the popular recent approach of removing forgiveness collars from speaker diarization evaluation tools can unfairly penalize speaker diarization systems that correctly estimate speaker segment boundaries. The uncertainty in identifying the start and/or end of a particular phoneme means that the ground truth segmentation is not perfectly accurate, and even trained human listeners are unable to identify phoneme boundaries with full consistency. This research analyses the phoneme dependence of this uncertainty, and shows that it depends on (i) whether the phoneme being detected is at the start or end of an utterance and (ii) what the phoneme is, so that the use of a uniform forgiveness collar is inadequate. This analysis is expected to point the way towards more indicative and repeatable assessment of the performance of speaker diarization systems.},
  eventtitle = {2020 28th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  keywords = {diarization scoring,Europe,forgiveness collar,Labeling,phoneme boundary,Phonetics,Signal processing,Speaker diarization,Tools,Uncertainty},
  file = {/Users/brono/Zotero/storage/8GRQFRSV/McKnight et al. - 2021 - Analysis of Phonetic Dependence of Segmentation Er.pdf;/Users/brono/Zotero/storage/3DK46TE4/stamp.html}
}

@inproceedings{mcknightStudyingHumanBasedSpeaker2022,
  title = {Studying {{Human-Based Speaker Diarization}} and {{Comparing}} to {{State-of-the-Art Systems}}},
  booktitle = {2022 {{Asia-Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  author = {McKnight, Simon W. and Hogg, Aidan O. T. and Neo, Vincent W. and Naylor, Patrick A.},
  date = {2022-11-07},
  pages = {1--8},
  publisher = {{IEEE}},
  location = {{Chiang Mai, Thailand}},
  doi = {10.23919/APSIPAASC55919.2022.9979811},
  url = {http://www.apsipa.org/proceedings/2022/APSIPA%202022/TuPM1-3/1570839565.pdf},
  urldate = {2023-03-26},
  eventtitle = {2022 {{Asia Pacific Signal}} and {{Information Processing Association Annual Summit}} and {{Conference}} ({{APSIPA ASC}})},
  isbn = {978-616-590-477-3},
  file = {/Users/brono/Zotero/storage/ABR2WSWF/McKnight et al. - 2022 - Studying Human-Based Speaker Diarization and Compa.pdf}
}

@inproceedings{medennikovTargetSpeakerVoiceActivity2020,
  title = {Target-{{Speaker Voice Activity Detection}}: A {{Novel Approach}} for {{Multi-Speaker Diarization}} in a {{Dinner Party Scenario}}},
  shorttitle = {Target-{{Speaker Voice Activity Detection}}},
  booktitle = {Interspeech 2020},
  author = {Medennikov, Ivan and Korenevsky, Maxim and Prisyach, Tatiana and Khokhlov, Yuri and Korenevskaya, Mariya and Sorokin, Ivan and Timofeeva, Tatiana and Mitrofanov, Anton and Andrusenko, Andrei and Podluzhny, Ivan and Laptev, Aleksandr and Romanenko, Aleksei},
  date = {2020-10-25},
  eprint = {2005.07272},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {274--278},
  doi = {10.21437/Interspeech.2020-1602},
  url = {http://arxiv.org/abs/2005.07272},
  urldate = {2023-03-02},
  abstract = {Speaker diarization for real-life scenarios is an extremely challenging problem. Widely used clustering-based diarization approaches perform rather poorly in such conditions, mainly due to the limited ability to handle overlapping speech. We propose a novel Target-Speaker Voice Activity Detection (TS-VAD) approach, which directly predicts an activity of each speaker on each time frame. TS-VAD model takes conventional speech features (e.g., MFCC) along with i-vectors for each speaker as inputs. A set of binary classification output layers produces activities of each speaker. I-vectors can be estimated iteratively, starting with a strong clustering-based diarization. We also extend the TS-VAD approach to the multi-microphone case using a simple attention mechanism on top of hidden representations extracted from the single-channel TS-VAD model. Moreover, post-processing strategies for the predicted speaker activity probabilities are investigated. Experiments on the CHiME-6 unsegmented data show that TS-VAD achieves state-of-the-art results outperforming the baseline x-vector-based system by more than 30\% Diarization Error Rate (DER) abs.},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,TODO},
  file = {/Users/brono/Zotero/storage/EJ5JZ7CN/Medennikov et al. - 2020 - Target-Speaker Voice Activity Detection a Novel A.pdf;/Users/brono/Zotero/storage/VVP6BE2M/2005.html}
}

@article{meierLearningNeuralModels2018,
  title = {Learning Neural Models for End-to-End Clustering},
  author = {Meier, Benjamin Bruno and Elezi, Ismail and Amirian, Mohammadreza and Dürr, Oliver and Stadelmann, Thilo},
  date = {2018},
  publisher = {{IAPR}},
  doi = {10.21256/ZHAW-3850},
  url = {https://digitalcollection.zhaw.ch/handle/11475/7727},
  urldate = {2023-03-17},
  langid = {english},
  keywords = {006: Spezielle Computerverfahren,Learning to cluster,Perceptual grouping,Speech \&amp; image clustering},
  file = {/Users/brono/Zotero/storage/XMCMAYKJ/Meier et al. - 2018 - Learning neural models for end-to-end clustering.pdf}
}

@online{MERLIonCCSChallenge,
  title = {{{MERLIon CCS Challenge Evaluation Plan}} v1.2.Pdf},
  url = {https://drive.google.com/file/d/1xvWuV-p4ooMSeZlgW2cot5P1eKioYXJA/view?usp=share_link&usp=embed_facebook},
  urldate = {2023-04-03},
  organization = {{Google Docs}},
  file = {/Users/brono/Zotero/storage/LD9WQ8SX/MERLIon CCS Challenge Evaluation Plan v1.2.pdf.pdf;/Users/brono/Zotero/storage/8W3P6CN4/view.html}
}

@online{MERLIonCCSInterspeech,
  title = {{{MERLIon CCS Interspeech}} 2023},
  url = {https://sites.google.com/view/merlion-ccs-challenge/},
  urldate = {2023-04-04},
  abstract = {About},
  langid = {english},
  file = {/Users/brono/Zotero/storage/UWYFVVNI/merlion-ccs-challenge.html}
}

@online{MERLIonCCSInterspeecha,
  title = {{{MERLIon CCS Interspeech}} 2023 - {{Task}} 2},
  url = {https://sites.google.com/view/merlion-ccs-challenge/task-2},
  urldate = {2023-04-10},
  abstract = {The second task of the MERLIon CCS challenge is language diarization. During development, systems are provided with audio recordings where ground-truth timestamps and language labels have been annotated. During evaluation, only audio is provided. The datasets for development and evaluation are},
  langid = {english},
  file = {/Users/brono/Zotero/storage/5Q6GFHIQ/task-2.html}
}

@software{MERLIonChallengeMerlionccs20232023,
  title = {{{MERLIon-Challenge}}/Merlion-Ccs-2023},
  date = {2023-02-13T14:58:01Z},
  origdate = {2022-12-29T07:42:34Z},
  url = {https://github.com/MERLIon-Challenge/merlion-ccs-2023/blob/d64b89a8c76235ace5687dfcad8df4e97b59d9e9/readme.md},
  urldate = {2023-04-10},
  organization = {{MERLIon-Challenge}}
}

@article{michelsantiOverviewDeepLearningBasedAudioVisual2021,
  title = {An {{Overview}} of {{Deep-Learning-Based Audio-Visual Speech Enhancement}} and {{Separation}}},
  author = {Michelsanti, Daniel and Tan, Zheng-Hua and Zhang, Shi-Xiong and Xu, Yong and Yu, Meng and Yu, Dong and Jensen, Jesper},
  date = {2021},
  journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  shortjournal = {IEEE/ACM Trans. Audio Speech Lang. Process.},
  volume = {29},
  pages = {1368--1396},
  issn = {2329-9290, 2329-9304},
  doi = {10.1109/TASLP.2021.3066303},
  url = {https://ieeexplore.ieee.org/document/9380418/},
  urldate = {2023-03-20},
  abstract = {Speech enhancement and speech separation are two related tasks, whose purpose is to extract either one or more target speech signals, respectively, from a mixture of sounds generated by several sources. Traditionally, these tasks have been tackled using signal processing and machine learning techniques applied to the available acoustic signals. Since the visual aspect of speech is essentially unaffected by the acoustic environment, visual information from the target speakers, such as lip movements and facial expressions, has also been used for speech enhancement and speech separation systems. In order to efficiently fuse acoustic and visual information, researchers have exploited the flexibility of data-driven approaches, specifically deep learning, achieving strong performance. The ceaseless proposal of a large number of techniques to extract features and fuse multimodal information has highlighted the need for an overview that comprehensively describes and discusses audio-visual speech enhancement and separation based on deep learning. In this paper, we provide a systematic survey of this research topic, focusing on the main elements that characterise the systems in the literature: acoustic features; visual features; deep learning methods; fusion techniques; training targets and objective functions. In addition, we review deep-learning-based methods for speech reconstruction from silent videos and audiovisual sound source separation for non-speech signals, since these methods can be more or less directly applied to audio-visual speech enhancement and separation. Finally, we survey commonly employed audio-visual speech datasets, given their central role in the development of data-driven approaches, and evaluation methods, because they are generally used to compare different systems and determine their performance.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/PAL5RH58/Michelsanti et al. - 2021 - An Overview of Deep-Learning-Based Audio-Visual Sp.pdf}
}

@inproceedings{mishraChallengesSpokenLanguage2023,
  title = {Challenges in Spoken Language Diarization in Code-Switched Scenario},
  booktitle = {2023 {{National Conference}} on {{Communications}} ({{NCC}})},
  author = {Mishra, Jagabandhu and Mahadeva Prasanna, S. R.},
  date = {2023-02},
  pages = {1--6},
  doi = {10.1109/NCC56989.2023.10068088},
  abstract = {Spoken language diarization (SLD) is a task of automatically annotating the monolingual segments in a given code-switched (CS) speech signal. Most of the SLD methods in the literature are linked to the task like automatic speech recognition and no independent frameworks exist. One attractive direction to evolving an independent framework is to explore speaker diarization (SD). However, there are several challenges in doing the same due to the production of the CS speech by a single speaker. This work uses the speaker change detection (SCD) framework of SD to illustrate the challenges. The initial experiments demonstrate that the performance of language change detection (LCD) is significantly poor compared to SCD. The degradation is mostly due to the significant overlapping of the features belonging to two languages in the feature space. Trained adaptation models are used to compute the Gaussian mixture model posterior (GP) features and observed that the overlapping is significantly lower compared to the acoustic features. Hence the GP feature provides a relative improvement of 20\% for LCD in terms of identification rate (IDR). Thus using existing SD frameworks, to begin with, and modifying them to increase the discrimination between languages in the CS scenario seems to be a good way to evolve independent frameworks.},
  eventtitle = {2023 {{National Conference}} on {{Communications}} ({{NCC}})},
  keywords = {Acoustics,Adaptation models,Code-switched speech,Degradation,language change detection,Liquid crystal displays,posteriors,Production,speaker diarization,Speech coding,spoken language diarization,Superluminescent diodes},
  file = {/Users/brono/Zotero/storage/CQMZG5PU/Mishra and Mahadeva Prasanna - 2023 - Challenges in spoken language diarization in code-.pdf;/Users/brono/Zotero/storage/NQG8WWLR/stamp.html}
}

@online{mishraLanguageVsSpeaker2022,
  title = {Language vs {{Speaker Change}}: {{A Comparative Study}}},
  shorttitle = {Language vs {{Speaker Change}}},
  author = {Mishra, Jagabandhu and Prasanna, S. R. Mahadeva},
  date = {2022-03-05},
  eprint = {2203.02680},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2203.02680},
  urldate = {2023-04-18},
  abstract = {Spoken language change detection (LCD) refers to detecting language switching points in a multilingual speech signal. Speaker change detection (SCD) refers to locating the speaker change points in a multispeaker speech signal. The objective of this work is to understand the challenges in LCD task by comparing it with SCD task. Human subjective study for change detection is performed for LCD and SCD. This study demonstrates that LCD requires larger duration spectro-temporal information around the change point compared to SCD. Based on this, the work explores automatic distance based and model based LCD approaches. The model based ones include Gaussian mixture model and universal background model (GMM-UBM), attention, and Generative adversarial network (GAN) based approaches. Both the human and automatic LCD tasks infer that the performance of the LCD task improves by incorporating more and more spectro-temporal duration.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing},
  file = {/Users/brono/Zotero/storage/YV4NYLC6/Mishra and Prasanna - 2022 - Language vs Speaker Change A Comparative Study.pdf;/Users/brono/Zotero/storage/LY4QYD6J/2203.html}
}

@article{modipaImplicationsSepediEnglish,
  title = {Implications of {{Sepedi}}/{{English}} Code Switching for {{ASR}} Systems},
  author = {Modipa, Thipe I and Davel, Marelie H and family=Wet, given=Febe, prefix=de, useprefix=true},
  abstract = {Code switching (the process of switching from one language to another during a conversation) is a common phenomenon in multilingual environments. Where a minority and dominant language coincide, code switching from the minority language to the dominant language can become particularly frequent. We analyse one such scenario: Sepedi spoken in South Africa, where English is the dominant language; and determine the frequency and mechanisms of code switching through the analysis of radio broadcasts. We also perform an initial acoustic analysis to determine the impact of such code switching on speech recognition performance. We find that the frequency of code switching is unexpectedly high, and that the continuum of code switching (from unmodified embedded words to loan words absorbed in the matrix language) makes this a particularly challenging task for speech recognition systems.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/V7IVI43X/Modipa et al. - Implications of SepediEnglish code switching for .pdf}
}

@article{modipaImplicationsSepediEnglish2013,
  title = {Implications of {{Sepedi}}/{{English}} Code Switching for {{ASR}} Systems},
  author = {Modipa, Thipe I and Davel, Marelie H and family=Wet, given=Febe, prefix=de, useprefix=true},
  date = {2013},
  abstract = {Code switching (the process of switching from one language to another during a conversation) is a common phenomenon in multilingual environments. Where a minority and dominant language coincide, code switching from the minority language to the dominant language can become particularly frequent. We analyse one such scenario: Sepedi spoken in South Africa, where English is the dominant language; and determine the frequency and mechanisms of code switching through the analysis of radio broadcasts. We also perform an initial acoustic analysis to determine the impact of such code switching on speech recognition performance. We find that the frequency of code switching is unexpectedly high, and that the continuum of code switching (from unmodified embedded words to loan words absorbed in the matrix language) makes this a particularly challenging task for speech recognition systems.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/K4KRDPEE/Modipa et al. - Implications of SepediEnglish code switching for .pdf}
}

@article{muraliDevelopingSpeakerDiarization2022,
  title = {Towards Developing Speaker Diarization for Parent-Child Interactions},
  author = {Murali, Abhejay and Dutta, Satwik and Chandra Shekar, Meena and Irvin, Dwight and Buzhardt, Jay and Hansen, John H.},
  date = {2022-10},
  journaltitle = {The Journal of the Acoustical Society of America},
  shortjournal = {The Journal of the Acoustical Society of America},
  volume = {152},
  number = {4},
  pages = {A61-A61},
  issn = {0001-4966},
  doi = {10.1121/10.0015551},
  url = {https://asa.scitation.org/doi/10.1121/10.0015551},
  urldate = {2023-04-17},
  abstract = {Daily interactions of children with their parents are crucial for spoken language skills and overall development. Capturing such interactions can help to provide meaningful feedback to parents as well as practitioners. Naturalistic audio capture and developing further speech processing pipeline for parent-child interactions is a challenging problem. One of the first important steps in the speech processing pipeline is Speaker Diarization—to identify who spoke when. Speaker Diarization is the method of separating a captured audio stream into analogous segments that are differentiated by the speaker’s (child or parent’s) identity. Following ongoing COVID-19 restrictions and human subjects research IRB protocols, an unsupervised data collection approach was formulated to collect parent-child interactions (of consented families) using LENA device—a light weight audio recorder. Different interaction scenarios were explored: book reading activity at home and spontaneous interactions in a science museum. To identify child’s speech from a parent, we train the Diarization models on open-source adult speech data and children speech data acquired from LDC (Linguistic Data Consortium). Various speaker embeddings (e.g., x-vectors, i-vectors, resnets) will be explored. Results will be reported using Diarization Error Rate. [Work sponsored by NSF via Grant Nos. 1918032 and 1918012.]},
  langid = {english}
}

@inproceedings{najafianAutomaticMeasurementAnalysis2016,
  title = {Automatic Measurement and Analysis of the Child Verbal Communication Using Classroom Acoustics within a Child Care Center},
  booktitle = {5th {{Workshop}} on {{Child Computer Interaction}} ({{WOCCI}} 2016)},
  author = {Najafian, Maryam and Irvin, Dwight and Luo, Ying and Rous, Beth and Hansen, John Hl},
  date = {2016-09-06},
  pages = {56--61},
  publisher = {{ISCA}},
  doi = {10.21437/WOCCI.2016-10},
  url = {https://www.isca-speech.org/archive/wocci_2016/najafian16_wocci.html},
  urldate = {2023-04-19},
  abstract = {Understanding the language environment of early learners is a challenging task for both human and machine, and it is critical in facilitating effective language development among young children. This papers presents a new application for the existing diarization systems and investigates the language environment of young children using a turn taking strategy employing an i-vector based baseline that captures adult-to-child or child-tochild conversational turns across different classrooms in a child care center. Detecting speaker turns is necessary before more in depth subsequent analysis of audio such as word count, speech recognition, and keyword spotting which can contribute to the design of future learning spaces specifically designed for typically developing children, or those at-risk with communication limitations. Experimental results using naturalistic childteacher classroom settings indicate the proposed rapid childadult speech turn taking scheme is highly effective under noisy classroom conditions and results in 27.3\% relative error rate reduction compared to the baseline results produced by the LIUM diarization toolkit.},
  eventtitle = {5th {{Workshop}} on {{Child Computer Interaction}} ({{WOCCI}} 2016)},
  langid = {english},
  file = {/Users/brono/Zotero/storage/GAA3K2LB/Najafian et al. - 2016 - Automatic measurement and analysis of the child ve.pdf}
}

@inproceedings{najafianSpeakerIndependentDiarization2016,
  title = {Speaker Independent Diarization for Child Language Environment Analysis Using Deep Neural Networks},
  booktitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Najafian, Maryam and Hansen, John H. L.},
  date = {2016-12},
  pages = {114--120},
  doi = {10.1109/SLT.2016.7846253},
  abstract = {Large-scale monitoring of the child language environment through measuring the amount of speech directed to the child by other children and adults during a vocal communication is an important task. Using the audio extracted from a recording unit worn by a child within a childcare center, at each point in time our proposed diarization system can determine the content of the child's language environment, by categorizing the audio content into one of the four major categories, namely (1) speech initiated by the child wearing the recording unit, speech originated by other (2) children or (3) adults and directed at the primary child, and (4) non-speech contents. In this study, we exploit complex Hidden Markov Models (HMMs) with multiple states to model the temporal dependencies between different sources of acoustic variability and estimate the HMM state output probabilities using deep neural networks as a discriminative modeling approach. The proposed system is robust against common diarization errors caused by rapid turn takings, between class similarities, and background noise without the need to prior clustering techniques. The experimental results confirm that this approach outperforms the state-of-the-art Gaussian mixture model based diarization without the need for bottom-up clustering and leads to 22.24\% relative error reduction.},
  eventtitle = {2016 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  keywords = {Acoustics,Child Speech,Decoding,Hidden Markov models,Language Development,Speaker Diarization,Speech,Speech Analysis,Speech recognition,Training,Viterbi algorithm},
  file = {/Users/brono/Zotero/storage/3SNCZSTF/Najafian and Hansen - 2016 - Speaker independent diarization for child language.pdf;/Users/brono/Zotero/storage/QEU2GQ2C/stamp.html}
}

@inproceedings{narayanaswamyDesigningEffectiveMetric2019,
  title = {Designing an {{Effective Metric Learning Pipeline}} for {{Speaker Diarization}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Narayanaswamy, Vivek Sivaraman and Thiagarajan, Jayaraman J. and Song, Huan and Spanias, Andreas},
  date = {2019-05},
  pages = {5806--5810},
  publisher = {{IEEE}},
  location = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8682255},
  url = {https://ieeexplore.ieee.org/document/8682255/},
  urldate = {2023-03-26},
  eventtitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-8131-1},
  file = {/Users/brono/Zotero/storage/VV7GKLRJ/Narayanaswamy et al. - 2019 - Designing an Effective Metric Learning Pipeline fo.pdf}
}

@software{NeuralSpeakerDiarization2023,
  title = {Neural Speaker Diarization with Pyannote.Audio},
  date = {2023-04-10T07:53:55Z},
  origdate = {2016-03-07T17:26:15Z},
  url = {https://github.com/pyannote/pyannote-audio/blob/74939acbfa830521a434cb4068176196dd9612dc/tutorials/adapting_pretrained_pipeline.ipynb},
  urldate = {2023-04-10},
  abstract = {Neural building blocks for speaker diarization: speech activity detection, speaker change detection, overlapped speech detection, speaker embedding},
  organization = {{pyannote}}
}

@online{nevilleThirdDIHARDSpeech,
  title = {The {{Third DIHARD Speech Diarization Challenge}}},
  author = {Neville, Ryant},
  url = {https://dihardchallenge.github.io/dihard3/results.html},
  urldate = {2023-04-04},
  abstract = {The Third DIHARD Speech Diarization Challenge},
  langid = {english},
  organization = {{The Third DIHARD Speech Diarization Challenge}},
  file = {/Users/brono/Zotero/storage/YYM7ZCSK/results.html}
}

@inproceedings{ngShefceCantoneseEnglishBilingual2017,
  title = {Shefce: {{A Cantonese-English}} Bilingual Speech Corpus for Pronunciation Assessment},
  shorttitle = {Shefce},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ng, Raymond W. M. and Kwan, Alvin C.M. and Lee, Tan and Hain, Thomas},
  date = {2017-03},
  pages = {5825--5829},
  publisher = {{IEEE}},
  location = {{New Orleans, LA}},
  doi = {10.1109/ICASSP.2017.7953273},
  url = {http://ieeexplore.ieee.org/document/7953273/},
  urldate = {2023-06-13},
  abstract = {This paper introduces the development of ShefCE: a CantoneseEnglish bilingual speech corpus from L2 English speakers in Hong Kong. Bilingual parallel recording materials were chosen from TED online lectures. Script selection were carried out according to bilingual consistency (evaluated using a machine translation system) and the distribution balance of phonemes. 31 undergraduate to postgraduate students in Hong Kong aged 20-30 were recruited and recorded a 25-hour speech corpus (12 hours in Cantonese and 13 hours in English). Baseline phoneme/syllable recognition systems were trained on background data with and without the ShefCE training data. The final syllable error rate (SER) for Cantonese is 17.3\% and final phoneme error rate (PER) for English is 34.5\%. The automatic speech recognition performance on English showed a significant mismatch when applying L1 models on L2 data, suggesting the need for explicit accent adaptation. ShefCE and the corresponding baseline models will be made openly available for academic research.},
  eventtitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5090-4117-6},
  langid = {english},
  file = {/Users/brono/Zotero/storage/X5LV5VKB/Ng et al. - 2017 - Shefce A Cantonese-English bilingual speech corpu.pdf}
}

@inproceedings{panayotovLibrispeechASRCorpus2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  shorttitle = {Librispeech},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2015-04},
  pages = {5206--5210},
  publisher = {{IEEE}},
  location = {{South Brisbane, Queensland, Australia}},
  doi = {10.1109/ICASSP.2015.7178964},
  url = {http://ieeexplore.ieee.org/document/7178964/},
  urldate = {2023-03-16},
  eventtitle = {{{ICASSP}} 2015 - 2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4673-6997-8}
}

@online{panEndtoendSpeakerDiarization2022,
  title = {Towards {{End-to-end Speaker Diarization}} in the {{Wild}}},
  author = {Pan, Zexu and Wichern, Gordon and Germain, François G. and Subramanian, Aswin and Roux, Jonathan Le},
  date = {2022-11-02},
  eprint = {2211.01299},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {https://arxiv.org/pdf/2211.01299.pdf},
  urldate = {2023-04-09},
  abstract = {Speaker diarization algorithms address the "who spoke when" problem in audio recordings. Algorithms trained end-to-end have proven superior to classical modular-cascaded systems in constrained scenarios with a small number of speakers. However, their performance for in-the-wild recordings containing more speakers with shorter utterance lengths remains to be investigated. In this paper, we address this gap, showing that an attractor-based end-to-end system can also perform remarkably well in the latter scenario when first pre-trained on a carefully-designed simulated dataset that matches the distribution of in-the-wild recordings. We also propose to use an attention mechanism to increase the network capacity in decoding more speaker attractors, and to jointly train the attractors on a speaker recognition task to improve the speaker attractor representation. Even though the model we propose is audio-only, we find it significantly outperforms both audio-only and audio-visual baselines on the AVA-AVD benchmark dataset, achieving state-of-the-art results with an absolute reduction in diarization error of 23.3\%.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/7ESHKR8Q/2211.01299.pdf;/Users/brono/Zotero/storage/9H9GT47T/2211.html}
}

@article{parkAutoTuningSpectralClustering2020,
  title = {Auto-{{Tuning Spectral Clustering}} for {{Speaker Diarization Using Normalized Maximum Eigengap}}},
  author = {Park, Tae Jin and Han, Kyu J. and Kumar, Manoj and Narayanan, Shrikanth},
  date = {2020},
  journaltitle = {IEEE Signal Processing Letters},
  shortjournal = {IEEE Signal Process. Lett.},
  volume = {27},
  pages = {381--385},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2019.2961071},
  url = {https://ieeexplore.ieee.org/document/8937487/},
  urldate = {2023-03-13},
  file = {/Users/brono/Zotero/storage/5AG9SCWP/Park et al. - 2020 - Auto-Tuning Spectral Clustering for Speaker Diariz.pdf}
}

@inproceedings{parkMultiScaleSpeakerDiarization2021,
  title = {Multi-{{Scale Speaker Diarization}} with {{Neural Affinity Score Fusion}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Park, Tae Jin and Kumar, Manoj and Narayanan, Shrikanth},
  date = {2021-06-06},
  pages = {7173--7177},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9414578},
  url = {https://ieeexplore.ieee.org/document/9414578/},
  urldate = {2023-03-07},
  abstract = {Predicting the speaker’s identity of short speech segments in human dialogue has been considered one of the most challenging problems in speech signal processing. Speaker representations of short speech segments tend to be unreliable, resulting in poor fidelity of speaker representations in tasks requiring speaker recognition. In this paper, we propose an unconventional method that tackles the trade-off between temporal resolution and the quality of the speaker representations. To find a set of weights that balance the scores from multiple temporal scales of segments, a neural affinity score fusion model is presented. Using the CALLHOME dataset, we show that our proposed multi-scale segmentation and integration approach can achieve a state-of-the-art diarization performance.},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  langid = {english},
  file = {/Users/brono/Zotero/storage/8868K8E7/Park et al. - 2021 - Multi-Scale Speaker Diarization with Neural Affini.pdf}
}

@online{parkMultiscaleSpeakerDiarization2022,
  title = {Multi-Scale {{Speaker Diarization}} with {{Dynamic Scale Weighting}}},
  author = {Park, Tae Jin and Koluguri, Nithin Rao and Balam, Jagadeesh and Ginsburg, Boris},
  date = {2022-03-29},
  eprint = {2203.15974},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2203.15974},
  urldate = {2023-04-14},
  abstract = {Speaker diarization systems are challenged by a trade-off between the temporal resolution and the fidelity of the speaker representation. By obtaining a superior temporal resolution with an enhanced accuracy, a multi-scale approach is a way to cope with such a trade-off. In this paper, we propose a more advanced multi-scale diarization system based on a multi-scale diarization decoder. There are two main contributions in this study that significantly improve the diarization performance. First, we use multi-scale clustering as an initialization to estimate the number of speakers and obtain the average speaker representation vector for each speaker and each scale. Next, we propose the use of 1-D convolutional neural networks that dynamically determine the importance of each scale at each time step. To handle a variable number of speakers and overlapping speech, the proposed system can estimate the number of existing speakers. Our proposed system achieves a state-of-art performance on the CALLHOME and AMI MixHeadset datasets, with 3.92\% and 1.05\% diarization error rates, respectively.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/QKP5TQKW/Park et al. - 2022 - Multi-scale Speaker Diarization with Dynamic Scale.pdf;/Users/brono/Zotero/storage/VAX7P8IQ/Park et al. - 2022 - Multi-scale Speaker Diarization with Dynamic Scale.pdf;/Users/brono/Zotero/storage/76GT5SD7/2203.html}
}

@online{parkReviewSpeakerDiarization2021,
  title = {A {{Review}} of {{Speaker Diarization}}: {{Recent Advances}} with {{Deep Learning}}},
  shorttitle = {A {{Review}} of {{Speaker Diarization}}},
  author = {Park, Tae Jin and Kanda, Naoyuki and Dimitriadis, Dimitrios and Han, Kyu J. and Watanabe, Shinji and Narayanan, Shrikanth},
  date = {2021-11-26},
  eprint = {2101.09624},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2101.09624},
  urldate = {2023-02-01},
  abstract = {Speaker diarization is a task to label audio or video recordings with classes that correspond to speaker identity, or in short, a task to identify “who spoke when”. In the early years, speaker diarization algorithms were developed for speech recognition on multispeaker audio recordings to enable speaker adaptive processing. These algorithms also gained their own value as a standalone application over time to provide speaker-specific metainformation for downstream tasks such as audio retrieval. More recently, with the emergence of deep learning technology, which has driven revolutionary changes in research and practices across speech application domains, rapid advancements have been made for speaker diarization. In this paper, we review not only the historical development of speaker diarization technology but also the recent advancements in neural speaker diarization approaches. Furthermore, we discuss how speaker diarization systems have been integrated with speech recognition applications and how the recent surge of deep learning is leading the way of jointly modeling these two components to be complementary to each other. By considering such exciting technical trends, we believe that this paper is a valuable contribution to the community to provide a survey work by consolidating the recent developments with neural methods and thus facilitating further progress toward a more efficient speaker diarization.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/9Y85SIJ2/Park et al. - 2021 - A Review of Speaker Diarization Recent Advances w.pdf;/Users/brono/Zotero/storage/VZZXP4WX/Speaker_Diarization_based_on_Deep_Learning_Techniques_A_Review.pdf}
}

@article{parkUSCSAILSystemDIHARD,
  title = {{{USC-SAIL System}} for {{DIHARD III}}: {{Domain Adaptive Diarization System}}},
  author = {Park, Tae Jin and Peri, Raghuveer},
  abstract = {DIHARD challenge focuses on the hard diarization problem and the DIHARD dataset includes a number of challenging domains that are hard to obtain low diarization error rates. We propose a novel approach to deal with domain mismatch problems by estimating the domain of the given input session. We take advantage of three different embedding extractors trained on different datasets. Based on these multiple embedding extractors, our domain adaptive speaker diarization system employs two different approaches: Hard decision and soft decision. In the hard decision method, we estimate the given session into one of the three categories and select an embedding extractor suited to that category. On the other hand, in the soft decision method, we train our proposed neural affinity score fusion network that estimates the desirable weights for the affinity scores we obtain from the three embedding extractors. We show the performance gain from each method and how our domain estimator models are trained to obtain such improvement. In addition, we introduce the auto-tuning spectral clustering method to develop a parameter-free diarization system.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/XKVHJEFE/Park and Peri - USC-SAIL System for DIHARD III Domain Adaptive Di.pdf}
}

@article{pawarDissertationPresentedAcademic2019,
  title = {A {{Dissertation Presented}} to {{The Academic Faculty}}},
  author = {Pawar, Rahul Shivaji},
  date = {2019},
  pages = {66},
  url = {https://smartech.gatech.edu/bitstream/handle/1853/62285/PAWAR-DISSERTATION-2019.pdf?sequence=1&isAllowed=y},
  langid = {english},
  file = {/Users/brono/Zotero/storage/MUN9ICNK/Pawar - A Dissertation Presented to The Academic Faculty.pdf}
}

@inproceedings{piccininiProsodicCuesMonolingual2014,
  title = {Prosodic {{Cues}} to {{Monolingual}} versus {{Code-switching Sentences}} in {{English}} and {{Spanish}}},
  booktitle = {Speech {{Prosody}} 2014},
  author = {Piccinini, Page and Garellek, Marc},
  date = {2014-05-20},
  pages = {885--889},
  publisher = {{ISCA}},
  doi = {10.21437/SpeechProsody.2014-166},
  url = {https://www.isca-speech.org/archive/speechprosody_2014/piccinini14_speechprosody.html},
  urldate = {2023-04-23},
  abstract = {Code-switching offers an interesting methodology to examine what happens when two linguistic systems come into contact. In the present study, two experiments were conducted to see if (1) listeners were able to anticipate code-switches in speech-innoise, and (2) prosodic cues were present in the signal as potential cues to an upcoming code-switch. A speech-in-noise perception experiment with early Spanish-English bilinguals found that listeners were indeed able to accurately identify words in code-switching sentences with the same accuracy as in monolingual sentences, even in highly-degraded listening conditions. We then analyzed the stimuli used in the perception experiment, and found that the speaker used different prosodic contours for code-switching productions compared to monolingual productions. We propose that listeners use specific code-switching prosody to anticipate code-switches, and thus ease processing costs in word identification.},
  eventtitle = {Speech {{Prosody}} 2014},
  langid = {english},
  file = {/Users/brono/Zotero/storage/76MHZ36F/Piccinini and Garellek - 2014 - Prosodic Cues to Monolingual versus Code-switching.pdf}
}

@book{potamianosAutomaticSpeechRecognition1997,
  title = {Automatic Speech Recognition for Children},
  author = {Potamianos, Alexandros and Narayanan, Shrikanth and Lee, Sungbok},
  date = {1997-09-22},
  doi = {10.21437/Eurospeech.1997-623}
}

@inproceedings{poveySemiOrthogonalLowRankMatrix2018,
  title = {Semi-{{Orthogonal Low-Rank Matrix Factorization}} for {{Deep Neural Networks}}},
  booktitle = {Interspeech 2018},
  author = {Povey, Daniel and Cheng, Gaofeng and Wang, Yiming and Li, Ke and Xu, Hainan and Yarmohammadi, Mahsa and Khudanpur, Sanjeev},
  date = {2018-09-02},
  pages = {3743--3747},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1417},
  url = {https://www.isca-speech.org/archive/interspeech_2018/povey18_interspeech.html},
  urldate = {2023-04-03},
  abstract = {The LEAP submission for DIHARD-III challenge is described in this report. The LEAP system involves the use of End-to-End speaker diarization system for the two-speaker conversational telephone speech recordings. For the wideband multi-speaker recordings, the proposed approach for diarization uses embeddings from a time-delay neural network (called xvectors) followed by a graph based clustering approach called the path integral clustering. The LEAP system showed 24\% and 18\% relative improvements for track1 and track2 respectively over the baseline system provided by the organizers. This report provides details of the model and the experimental results on the DIHARD-III dataset.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/Y2GN2UPB/Povey et al. - 2018 - Semi-Orthogonal Low-Rank Matrix Factorization for .pdf}
}

@inproceedings{prazakSpeakerDiarizationUsing2011,
  title = {Speaker Diarization Using {{PLDA-based}} Speaker Clustering},
  booktitle = {Proceedings of the 6th {{IEEE International Conference}} on {{Intelligent Data Acquisition}} and {{Advanced Computing Systems}}},
  author = {Prazak, Jan and Silovsky, Jan},
  date = {2011-09},
  pages = {347--350},
  publisher = {{IEEE}},
  location = {{Prague, Czech Republic}},
  doi = {10.1109/IDAACS.2011.6072771},
  url = {http://ieeexplore.ieee.org/document/6072771/},
  urldate = {2023-02-01},
  abstract = {This paper investigates application of the Probabilistic Linear Discriminant Analysis (PLDA) for speaker clustering within a speaker diarization framework. Factor analysis is employed to extract low-dimensional representation of a sequence of acoustic feature vectors - so called i-vectors - and these i-vectors are modeled using the PLDA. Experiments were carried out using the COST278 broadcast news database. We achieved 33.7\% relative improvement of the Diarization Error Rate (DER) and 43.8\% relative improvement of the speaker error rate compared to the baseline system using clustering based on the Bayesian Information Criterion (BIC).},
  eventtitle = {2011 {{IEEE}} 6th {{International Conference}} on {{Intelligent Data Acquisition}} and {{Advanced Computing Systems}}: {{Technology}} and {{Applications}} ({{IDAACS}})},
  isbn = {978-1-4577-1426-9},
  langid = {english},
  keywords = {TODO},
  file = {/Users/brono/Zotero/storage/PNYS9USU/Prazak and Silovsky - 2011 - Speaker diarization using PLDA-based speaker clust.pdf}
}

@inproceedings{rajDOVERLapMethodCombining2021,
  title = {{{DOVER-Lap}}: {{A Method}} for {{Combining Overlap-Aware Diarization Outputs}}},
  shorttitle = {{{DOVER-Lap}}},
  booktitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Raj, Desh and Paola Garcia-Perera, Leibny and Huang, Zili and Watanabe, Shinji and Povey, Daniel and Stolcke, Andreas and Khudanpur, Sanjeev},
  date = {2021-01-19},
  pages = {881--888},
  publisher = {{IEEE}},
  location = {{Shenzhen, China}},
  doi = {10.1109/SLT48900.2021.9383490},
  url = {https://ieeexplore.ieee.org/document/9383490/},
  urldate = {2023-03-13},
  eventtitle = {2021 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  isbn = {978-1-72817-066-4},
  file = {/Users/brono/Zotero/storage/J7TEGFQF/Raj et al. - 2021 - DOVER-Lap A Method for Combining Overlap-Aware Di.pdf}
}

@online{rajIntegrationSpeechSeparation2020,
  title = {Integration of Speech Separation, Diarization, and Recognition for Multi-Speaker Meetings: {{System}} Description, Comparison, and Analysis},
  shorttitle = {Integration of Speech Separation, Diarization, and Recognition for Multi-Speaker Meetings},
  author = {Raj, Desh and Denisov, Pavel and Chen, Zhuo and Erdogan, Hakan and Huang, Zili and He, Maokui and Watanabe, Shinji and Du, Jun and Yoshioka, Takuya and Luo, Yi and Kanda, Naoyuki and Li, Jinyu and Wisdom, Scott and Hershey, John R.},
  date = {2020-11-03},
  eprint = {2011.02014},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2011.02014},
  urldate = {2023-03-01},
  abstract = {Multi-speaker speech recognition of unsegmented recordings has diverse applications such as meeting transcription and automatic subtitle generation. With technical advances in systems dealing with speech separation, speaker diarization, and automatic speech recognition (ASR) in the last decade, it has become possible to build pipelines that achieve reasonable error rates on this task. In this paper, we propose an end-to-end modular system for the LibriCSS meeting data, which combines independently trained separation, diarization, and recognition components, in that order. We study the effect of different state-of-the-art methods at each stage of the pipeline, and report results using task-specific metrics like SDR and DER, as well as downstream WER. Experiments indicate that the problem of overlapping speech for diarization and ASR can be effectively mitigated with the presence of a well-trained separation module. Our best system achieves a speaker-attributed WER of 12.7\%, which is close to that of a non-overlapping ASR.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,E2E,Electrical Engineering and Systems Science - Audio and Speech Processing,Overlap,TODO},
  file = {/Users/brono/Zotero/storage/H7UMG5BB/Raj et al. - 2020 - Integration of speech separation, diarization, and.pdf;/Users/brono/Zotero/storage/2K89REQ9/2011.html}
}

@article{rajMissingfeatureApproachesSpeech2005,
  title = {Missing-Feature Approaches in Speech Recognition},
  author = {Raj, B. and Stern, R.M.},
  date = {2005-09},
  journaltitle = {IEEE Signal Processing Magazine},
  shortjournal = {IEEE Signal Process. Mag.},
  volume = {22},
  number = {5},
  pages = {101--116},
  issn = {1053-5888},
  doi = {10.1109/MSP.2005.1511828},
  url = {http://ieeexplore.ieee.org/document/1511828/},
  urldate = {2023-03-20},
  langid = {english},
  file = {/Users/brono/Zotero/storage/3J8IAHUW/Raj and Stern - 2005 - Missing-feature approaches in speech recognition.pdf}
}

@inproceedings{rallabandiBuildingMixedLingual2017,
  title = {On {{Building Mixed Lingual Speech Synthesis Systems}}},
  booktitle = {Interspeech 2017},
  author = {Rallabandi, SaiKrishna and Black, Alan W.},
  date = {2017-08-20},
  pages = {52--56},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-1244},
  url = {https://www.isca-speech.org/archive/interspeech_2017/rallabandi17_interspeech.html},
  urldate = {2023-04-16},
  abstract = {Codemixing - phenomenon where lexical items from one language are embedded in the utterance of another- is relatively frequent in multilingual communities. However, TTS systems today are not fully capable of effectively handling such mixed content despite achieving high quality in the monolingual case. In this paper, we investigate various mechanisms for building mixed lingual systems which are built using a mixture of monolingual corpora and are capable of synthesizing such content. First, we explore the possibility of manipulating the phoneme representation: using target word to source phone mapping with the aim of emulating the native speaker intuition. We then present experiments at the acoustic stage investigating training techniques at both spectral and prosodic levels. Subjective evaluation shows that our systems are capable of generating high quality synthesis in codemixed scenarios.},
  eventtitle = {Interspeech 2017},
  langid = {english},
  file = {/Users/brono/Zotero/storage/AJPYV6TT/Rallabandi and Black - 2017 - On Building Mixed Lingual Speech Synthesis Systems.pdf}
}

@online{ramonSpeakerDiarizationKaldi2019,
  title = {Speaker {{Diarization}} with {{Kaldi}}},
  author = {Ramon, Yoav},
  date = {2019-02-28T10:51:37},
  url = {https://towardsdatascience.com/speaker-diarization-with-kaldi-e30301b05cc8},
  urldate = {2023-04-15},
  abstract = {the ability to process audio of multiple speakers is crucial. This article is a basic tutorial for that process with Kaldi X-Vectors.},
  langid = {english},
  organization = {{Medium}},
  file = {/Users/brono/Zotero/storage/U89DYEBM/speaker-diarization-with-kaldi-e30301b05cc8.html}
}

@inproceedings{raoStudyLexicalProsodic2018,
  title = {A {{Study}} of {{Lexical}} and {{Prosodic Cues}} to {{Segmentation}} in a {{Hindi-English Code-switched Discourse}}},
  booktitle = {Interspeech 2018},
  author = {Rao, Preeti and Pandya, Mugdha and Sabu, Kamini and Kumar, Kanhaiya and Bondale, Nandini},
  date = {2018-09-02},
  pages = {1918--1922},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1600},
  url = {https://www.isca-speech.org/archive/interspeech_2018/rao18_interspeech.html},
  urldate = {2023-04-23},
  abstract = {Bilingualism, almost universal in India, routinely appears in communication in many forms. Code-switching with English is common among city dwellers with the matrix language typically being the speaker’s native tongue. While a number of English words have made their way into the lexicon of Indian languages, also prevalent is insertional code-switching, i.e. switching at sentence or clause level. We consider an interesting and widely encountered variety of code-switched speech in the form of public discourses by a popular motivational speaker who uses English, probably for effect, in her Hindi language speeches. We effectively observe three categories of segments in the discourse: Hindi, Hindi with embedded English words and English. In this work, we present the characteristics of our data, and investigate the discrimination potential of lexical and prosodic cues on manually segmented fragments. Lexical cues are obtained via Google Speech API for Indian English recognition. Prosodic cues computed from pitch, intensity and syllable duration estimates are found to demonstrate significant differences between Hindi and English segments, indicating more careful articulation of the embedded language.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/8IL7APHB/Rao et al. - 2018 - A Study of Lexical and Prosodic Cues to Segmentati.pdf}
}

@online{ReproducibleResearchVoxConverse2022,
  title = {Reproducible\_research/2.1.1/{{VoxConverse}}.Test.Eval · Pyannote/Speaker-Diarization at Main},
  date = {2022-11-17},
  url = {https://huggingface.co/pyannote/speaker-diarization/blob/main/reproducible_research/2.1.1/VoxConverse.test.eval},
  urldate = {2023-04-10},
  abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/brono/Zotero/storage/GEGV5868/VoxConverse.test.html}
}

@online{ReviewSpeakerDiarization,
  title = {A Review on Speaker Diarization Systems and Approaches | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.specom.2012.05.002},
  url = {https://reader.elsevier.com/reader/sd/pii/S0167639312000696?token=78031BC07FEE8CE619BD046BC060D8309F942E356115C8EE241DA6B3B4529F4477FE41D7A497C2955DE4183FBDC9ED26&originRegion=us-east-1&originCreation=20230312133329},
  urldate = {2023-03-12},
  langid = {english},
  file = {/Users/brono/Zotero/storage/LQPS4X3L/A review on speaker diarization systems and approa.pdf}
}

@inproceedings{reynoldsApproachesApplicationsAudio2005,
  title = {Approaches and {{Applications}} of {{Audio Diarization}}},
  booktitle = {Proceedings. ({{ICASSP}} '05). {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2005.},
  author = {Reynolds, D.A. and Torres-Carrasquillo, P.},
  date = {2005},
  volume = {5},
  pages = {953--956},
  publisher = {{IEEE}},
  location = {{Philadelphia, Pennsylvania, USA}},
  doi = {10.1109/ICASSP.2005.1416463},
  url = {http://ieeexplore.ieee.org/document/1416463/},
  urldate = {2023-03-20},
  eventtitle = {({{ICASSP}} '05). {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}, 2005.},
  isbn = {978-0-7803-8874-1},
  file = {/Users/brono/Zotero/storage/QLIV8SDM/Reynolds and Torres-Carrasquillo - 2005 - Approaches and Applications of Audio Diarization.pdf}
}

@online{rezazadeganAutomaticSpeechSummarisation2020,
  title = {Automatic {{Speech Summarisation}}: {{A Scoping Review}}},
  shorttitle = {Automatic {{Speech Summarisation}}},
  author = {Rezazadegan, Dana and Berkovsky, Shlomo and Quiroz, Juan C. and Kocaballi, A. Baki and Wang, Ying and Laranjo, Liliana and Coiera, Enrico},
  date = {2020-08-26},
  eprint = {2008.11897},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2008.11897},
  urldate = {2023-06-10},
  abstract = {Speech summarisation techniques take human speech as input and then output an abridged version as text or speech. Speech summarisation has applications in many domains from information technology to health care, for example improving speech archives or reducing clinical documentation burden. This scoping review maps the speech summarisation literature, with no restrictions on time frame, language summarised, research method, or paper type. We reviewed a total of 110 papers out of a set of 153 found through a literature search and extracted speech features used, methods, scope, and training corpora. Most studies employ one of four speech summarisation architectures: (1) Sentence extraction and compaction; (2) Feature extraction and classification or rank-based sentence selection; (3) Sentence compression and compression summarisation; and (4) Language modelling. We also discuss the strengths and weaknesses of these different methods and speech features. Overall, supervised methods (e.g. Hidden Markov support vector machines, Ranking support vector machines, Conditional random fields) performed better than unsupervised methods. As supervised methods require manually annotated training data which can be costly, there was more interest in unsupervised methods. Recent research into unsupervised methods focusses on extending language modelling, for example by combining Uni-gram modelling with deep neural networks. Protocol registration: The protocol for this scoping review is registered at https://osf.io.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/brono/Zotero/storage/X2UB63EN/Rezazadegan et al. - 2020 - Automatic Speech Summarisation A Scoping Review.pdf;/Users/brono/Zotero/storage/NTIQSNWM/2008.html}
}

@inproceedings{ryantaEnhancementAnalysisConversational2018,
  title = {Enhancement and {{Analysis}} of {{Conversational Speech}}: {{JSALT}} 2017},
  shorttitle = {Enhancement and {{Analysis}} of {{Conversational Speech}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Ryanta, Neville and Bergelson, Elika and Church, Kenneth and Cristia, Alejandrina and Du, Jun and Ganapathy, Sriram and Khudanpur, Sanjeev and Kowalski, Diana and Krishnamoorthy, Mahesh and Kulshreshta, Rajat and Liberman, Mark and Lu, Yu-Ding and Maciejewski, Matthew and Metze, Florian and Profant, Jan and Sun, Lei and Tsao, Yu and Yu, Zhou},
  date = {2018-04},
  pages = {5154--5158},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2018.8462468},
  abstract = {Automatic speech recognition is more and more widely and effectively used. Nevertheless, in some automatic speech analysis tasks the state of the art is surprisingly poor. One of these is “diarization”, the task of determining who spoke when. Diarization is key to processing meeting audio and clinical interviews, extended recordings such as police body cam or child language acquisition data, and any other speech data involving multiple speakers whose voices are not cleanly separated into individual channels. Overlapping speech, environmental noise and suboptimal recording techniques make the problem harder. During the JSALT Summer Workshop at CMU in 2017, an international team of researchers worked on several aspects of this problem, including calibration of the state of the art, detection of overlaps, enhancement of noisy recordings, and classification of shorter speech segments. This paper sketches the workshop's results, and announces plans for a “Diarization Challenge” to encourage further progress.},
  eventtitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  keywords = {automatic speech recognition,Conferences,diarization,Interviews,Measurement,overlap detection,speech enhancement,Speech enhancement,Task analysis,Training},
  file = {/Users/brono/Zotero/storage/S2SISPYM/Ryanta et al. - 2018 - Enhancement and Analysis of Conversational Speech.pdf;/Users/brono/Zotero/storage/FFKL9LDM/stamp.html}
}

@article{ryantFirstDIHARDChallenge,
  title = {First {{DIHARD Challenge Evaluation Plan}}},
  author = {Ryant, Neville and Church, Kenneth and Cieri, Christopher and Cristia, Alejandrina and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  langid = {english},
  file = {/Users/brono/Zotero/storage/GCSMGWMV/Ryant et al. - First DIHARD Challenge Evaluation Plan.pdf}
}

@article{ryantSecondDIHARDChallenge,
  title = {Second {{DIHARD Challenge Evaluation Plan}}},
  author = {Ryant, Neville and Church, Kenneth and Cieri, Christopher and Cristia, Alejandrina and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  langid = {english},
  file = {/Users/brono/Zotero/storage/P66RGT49/Ryant et al. - Second DIHARD Challenge Evaluation Plan.pdf}
}

@online{ryantSecondDIHARDDiarization2019,
  title = {The {{Second DIHARD Diarization Challenge}}: {{Dataset}}, Task, and Baselines},
  shorttitle = {The {{Second DIHARD Diarization Challenge}}},
  author = {Ryant, Neville and Church, Kenneth and Cieri, Christopher and Cristia, Alejandrina and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  date = {2019-06-18},
  eprint = {1906.07839},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1906.07839},
  urldate = {2023-03-11},
  abstract = {This paper introduces the second DIHARD challenge, the second in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. The challenge comprises four tracks evaluating diarization performance under two input conditions (single channel vs. multi-channel) and two segmentation conditions (diarization from a reference speech segmentation vs. diarization from scratch). In order to prevent participants from overtuning to a particular combination of recording conditions and conversational domain, recordings are drawn from a variety of sources ranging from read audiobooks to meeting speech, to child language acquisition recordings, to dinner parties, to web video. We describe the task and metrics, challenge design, datasets, and baseline systems for speech enhancement, speech activity detection, and diarization.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/8ED97KVI/Ryant et al. - 2019 - The Second DIHARD Diarization Challenge Dataset, .pdf;/Users/brono/Zotero/storage/G2BHSSGG/1906.html}
}

@online{ryantSecondDIHARDDiarization2019a,
  title = {The {{Second DIHARD Diarization Challenge}}: {{Dataset}}, Task, and Baselines},
  shorttitle = {The {{Second DIHARD Diarization Challenge}}},
  author = {Ryant, Neville and Church, Kenneth and Cieri, Christopher and Cristia, Alejandrina and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  date = {2019-06-18},
  eprint = {1906.07839},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1906.07839},
  urldate = {2023-03-19},
  abstract = {This paper introduces the second DIHARD challenge, the second in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. The challenge comprises four tracks evaluating diarization performance under two input conditions (single channel vs. multi-channel) and two segmentation conditions (diarization from a reference speech segmentation vs. diarization from scratch). In order to prevent participants from overtuning to a particular combination of recording conditions and conversational domain, recordings are drawn from a variety of sources ranging from read audiobooks to meeting speech, to child language acquisition recordings, to dinner parties, to web video. We describe the task and metrics, challenge design, datasets, and baseline systems for speech enhancement, speech activity detection, and diarization.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/QWRBQT3S/Ryant et al. - 2019 - The Second DIHARD Diarization Challenge Dataset, .pdf;/Users/brono/Zotero/storage/KYUFTI9Y/1906.html}
}

@article{ryantThirdDIHARDChallenge,
  title = {Third {{DIHARD Challenge Evaluation Plan}}},
  author = {Ryant, Neville and Church, Kenneth and Cieri, Christopher and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  langid = {english},
  file = {/Users/brono/Zotero/storage/FHLR78QZ/Ryant et al. - Third DIHARD Challenge Evaluation Plan.pdf}
}

@dataset{ryantThirdDIHARDChallenge2022,
  title = {Third {{DIHARD Challenge Development}}},
  author = {Ryant, Neville and Liberman, Mark and Fiumara, James and Cieri, Christopher},
  date = {2022},
  pages = {1853231 KB},
  publisher = {{Linguistic Data Consortium}},
  doi = {10.35111/BS6W-W186},
  url = {https://catalog.ldc.upenn.edu/LDC2022S12},
  urldate = {2023-04-10},
  langid = {english}
}

@online{ryantThirdDIHARDDiarization2021,
  title = {The {{Third DIHARD Diarization Challenge}}},
  author = {Ryant, Neville and Singh, Prachi and Krishnamohan, Venkat and Varma, Rajat and Church, Kenneth and Cieri, Christopher and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  date = {2021-04-05},
  eprint = {2012.01477},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2012.01477},
  urldate = {2023-03-11},
  abstract = {DIHARD III was the third in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variability in recording equipment, noise conditions, and conversational domain. Speaker diarization was evaluated under two speech activity conditions (diarization from a reference speech activity vs. diarization from scratch) and 11 diverse domains. The domains span a range of recording conditions and interaction types, including read audio-books, meeting speech, clinical interviews, web videos, and, for the first time, conversational telephone speech. A total of 30 organizations (forming 21teams) from industry and academia submitted 499 valid system outputs. The evaluation results indicate that speaker diarization has improved markedly since DIHARD I, particularly for two-party interactions, but that for many domains (e.g., web video) the problem remains far from solved.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/LQRFZBZH/Ryant et al. - 2021 - The Third DIHARD Diarization Challenge.pdf;/Users/brono/Zotero/storage/C7D5HJMJ/2012.html}
}

@inproceedings{ryantThirdDIHARDDiarization2021a,
  title = {The {{Third DIHARD Diarization Challenge}}},
  booktitle = {Interspeech 2021},
  author = {Ryant, Neville and Singh, Prachi and Krishnamohan, Venkat and Varma, Rajat and Church, Kenneth and Cieri, Christopher and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  date = {2021-08-30},
  pages = {3570--3574},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1208},
  url = {https://www.isca-speech.org/archive/interspeech_2021/ryant21_interspeech.html},
  urldate = {2023-03-19},
  abstract = {This paper introduces the third DIHARD challenge, the third in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. Speaker diarization is evaluated under two segmentation conditions (diarization from a reference speech segmentation vs. diarization from scratch) and 11 diverse domains. The domains span a range of recording conditions and interaction types, including read audiobooks, meeting speech, clinical interviews, web videos, and, for the first time, conversational telephone speech. We describe the task and metrics, challenge design, datasets, and baseline systems for speech speech activity detection and diarization.},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/Users/brono/Zotero/storage/A44NB2LB/Ryant et al. - 2021 - The Third DIHARD Diarization Challenge.pdf}
}

@inproceedings{ryantThirdDIHARDDiarization2021b,
  title = {The {{Third DIHARD Diarization Challenge}}},
  booktitle = {Interspeech 2021},
  author = {Ryant, Neville and Singh, Prachi and Krishnamohan, Venkat and Varma, Rajat and Church, Kenneth and Cieri, Christopher and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  date = {2021-08-30},
  pages = {3570--3574},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-1208},
  url = {https://www.isca-speech.org/archive/interspeech_2021/ryant21_interspeech.html},
  urldate = {2023-04-03},
  abstract = {This paper introduces the third DIHARD challenge, the third in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variation in recording equipment, noise conditions, and conversational domain. Speaker diarization is evaluated under two segmentation conditions (diarization from a reference speech segmentation vs. diarization from scratch) and 11 diverse domains. The domains span a range of recording conditions and interaction types, including read audiobooks, meeting speech, clinical interviews, web videos, and, for the first time, conversational telephone speech. We describe the task and metrics, challenge design, datasets, and baseline systems for speech speech activity detection and diarization.},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/Users/brono/Zotero/storage/2GMNS2AI/Ryant et al. - 2021 - The Third DIHARD Diarization Challenge.pdf}
}

@online{ryantThirdDIHARDDiarization2021c,
  title = {The {{Third DIHARD Diarization Challenge}}},
  author = {Ryant, Neville and Singh, Prachi and Krishnamohan, Venkat and Varma, Rajat and Church, Kenneth and Cieri, Christopher and Du, Jun and Ganapathy, Sriram and Liberman, Mark},
  date = {2021-04-05},
  eprint = {2012.01477},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2012.01477},
  urldate = {2023-04-04},
  abstract = {DIHARD III was the third in a series of speaker diarization challenges intended to improve the robustness of diarization systems to variability in recording equipment, noise conditions, and conversational domain. Speaker diarization was evaluated under two speech activity conditions (diarization from a reference speech activity vs. diarization from scratch) and 11 diverse domains. The domains span a range of recording conditions and interaction types, including read audio-books, meeting speech, clinical interviews, web videos, and, for the first time, conversational telephone speech. A total of 30 organizations (forming 21teams) from industry and academia submitted 499 valid system outputs. The evaluation results indicate that speaker diarization has improved markedly since DIHARD I, particularly for two-party interactions, but that for many domains (e.g., web video) the problem remains far from solved.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/3Z4T6QZJ/Ryant et al. - 2021 - The Third DIHARD Diarization Challenge.pdf;/Users/brono/Zotero/storage/9DRUNXKT/2012.html}
}

@online{saeedCrossmodalSpeakerVerification2021,
  title = {Cross-Modal {{Speaker Verification}} and {{Recognition}}: {{A Multilingual Perspective}}},
  shorttitle = {Cross-Modal {{Speaker Verification}} and {{Recognition}}},
  author = {Saeed, Muhammad Saad and Nawaz, Shah and Morerio, Pietro and Mahmood, Arif and Gallo, Ignazio and Yousaf, Muhammad Haroon and Del Bue, Alessio},
  date = {2021-04-22},
  eprint = {2004.13780},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2004.13780},
  urldate = {2023-06-13},
  abstract = {Recent years have seen a surge in finding association between faces and voices within a cross-modal biometric application along with speaker recognition. Inspired from this, we introduce a challenging task in establishing association between faces and voices across multiple languages spoken by the same set of persons. The aim of this paper is to answer two closely related questions: "Is face-voice association language independent?" and "Can a speaker be recognised irrespective of the spoken language?". These two questions are very important to understand effectiveness and to boost development of multilingual biometric systems. To answer them, we collected a Multilingual Audio-Visual dataset, containing human speech clips of \$154\$ identities with \$3\$ language annotations extracted from various videos uploaded online. Extensive experiments on the three splits of the proposed dataset have been performed to investigate and answer these novel research questions that clearly point out the relevance of the multilingual problem.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/EC7EVTFF/Saeed et al. - 2021 - Cross-modal Speaker Verification and Recognition .pdf;/Users/brono/Zotero/storage/4F4KCKTF/2004.html}
}

@article{schmittBagofAudioWordsApproachSnore,
  title = {A {{Bag-of-Audio-Words Approach}} for {{Snore Sounds}}’ {{Excitation Localisation}}},
  author = {Schmitt, Maximilian and Janott, Christoph and Pandit, Vedhas and Qian, Kun and Heiser, Clemens and Hemmert, Werner and Schuller, Björn},
  abstract = {Habitual snoring and Obstructive Sleep Apnea are serious conditions that can affect the health of the snorer. For a targeted surgical treatment, it is crucial to identify the exact location of the vibration within the upper airways. As opposed to earlier work, we present the first unsupervised feature learning approach to this task based on bags-ofaudio-words. Likewise, we cluster feature values within a given time-segment into acoustic ‘words’. The frequency of occurrence per such word is then represented in a histogram per sound chunk to classify between four excitation locations. In extensive test runs based on snore sound data of 24 patients labelled by experts, we evaluated several feature sets as basis for audio word creation. In the result, we find audio words based on wavelet features, formants, and MFCC to be highly suited and outperform previous experiments based on the same data set.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/BHGPFF6A/Schmitt et al. - A Bag-of-Audio-Words Approach for Snore Sounds’ Ex.pdf}
}

@inproceedings{schullerINTERSPEECH2017Computational2017,
  title = {The {{INTERSPEECH}} 2017 {{Computational Paralinguistics Challenge}}: {{Addressee}}, {{Cold}} \& {{Snoring}}},
  shorttitle = {The {{INTERSPEECH}} 2017 {{Computational Paralinguistics Challenge}}},
  booktitle = {Interspeech 2017},
  author = {Schuller, Björn and Steidl, Stefan and Batliner, Anton and Bergelson, Elika and Krajewski, Jarek and Janott, Christoph and Amatuni, Andrei and Casillas, Marisa and Seidl, Amanda and Soderstrom, Melanie and Warlaumont, Anne S. and Hidalgo, Guillermo and Schnieder, Sebastian and Heiser, Clemens and Hohenhorst, Winfried and Herzog, Michael and Schmitt, Maximilian and Qian, Kun and Zhang, Yue and Trigeorgis, George and Tzirakis, Panagiotis and Zafeiriou, Stefanos},
  date = {2017-08-20},
  pages = {3442--3446},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-43},
  url = {https://www.isca-speech.org/archive/interspeech_2017/schuller17_interspeech.html},
  urldate = {2023-04-04},
  abstract = {The INTERSPEECH 2017 Computational Paralinguistics Challenge addresses three different problems for the first time in research competition under well-defined conditions: In the Addressee sub-challenge, it has to be determined whether speech produced by an adult is directed towards another adult or towards a child; in the Cold sub-challenge, speech under cold has to be told apart from ‘healthy’ speech; and in the Snoring sub-challenge, four different types of snoring have to be classified. In this paper, we describe these sub-challenges, their conditions, and the baseline feature extraction and classifiers, which include data-learnt feature representations by end-to-end learning with convolutional and recurrent neural networks, and bag-of-audio-words for the first time in the challenge series.},
  eventtitle = {Interspeech 2017},
  langid = {english},
  file = {/Users/brono/Zotero/storage/VHQ39D4M/Schuller et al. - 2017 - The INTERSPEECH 2017 Computational Paralinguistics.pdf}
}

@article{schultzMultilingualCrosslingualSpeech1998,
  title = {Multilingual and {{Crosslingual Speech Recognition}}},
  author = {Schultz, T and Waibel, A},
  date = {1998},
  abstract = {This paper describes the design of a multilingual speech recognizer using an LVCSR dictation database which has been collected under the project GlobalPhone. This project at the University of Karlsruhe investigates LVCSR systems in 15 languages of the world, namely Arabic, Chinese, Croatian, English, French, German, Italian, Japanese, Korean, Portuguese, Russian, Spanish, Swedish, Tamil, and Turkish. For our experiments we used six of these languages to train and test several recognition engines in monolingual, multilingual and crosslingual setups. Based on a global phoneme set we built a multilingual speech recognition system which can handle five different languages. The acoustic models of the five languages are combined into a monolithic system and context dependent phoneme models are created using language questions.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/AWE978TV/Schultz and Waibel - Multilingual and Crosslingual Speech Recognition.pdf}
}

@inproceedings{schusterSpeakerindependentDetectionChilddirected2014,
  title = {Speaker-Independent Detection of Child-Directed Speech},
  booktitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Schuster, Sebastian and Pancoast, Stephanie and Ganjoo, Milind and Frank, Michael C. and Jurafsky, Dan},
  date = {2014-12},
  pages = {366--371},
  doi = {10.1109/SLT.2014.7078602},
  abstract = {Identifying the distinct register that adults use when speaking to children is an important task for child development research. We present a fully automatic, speaker-independent system that detects child-directed speech. The two-stage system uses diarization-style voice activation techniques to extract speech segments followed by a supervised ν-SVM classifier trained on 1582 prosodic and log Mel energy features. The system significantly improves the state of the art, detecting child-directed speech with F1 of .66 (exact boundary) and .83 (within 1 second). A feature analysis confirms the importance of F0 features (especially 3rd quartile and range) as well as new features like the variance, kurtosis, and min of log Mel energy within a frequency band.},
  eventtitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  keywords = {Accuracy,Child-directed Speech,Gold,Language Development,Measurement,Noise,Prosody,Speech,Speech Analysis,Support vector machines,Training},
  file = {/Users/brono/Zotero/storage/TZFVS2MD/Schuster et al. - 2014 - Speaker-independent detection of child-directed sp.pdf;/Users/brono/Zotero/storage/5ZPQ7CXA/stamp.html}
}

@inproceedings{schusterSpeakerindependentDetectionChilddirected2014a,
  title = {Speaker-Independent Detection of Child-Directed Speech},
  booktitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  author = {Schuster, Sebastian and Pancoast, Stephanie and Ganjoo, Milind and Frank, Michael C. and Jurafsky, Dan},
  date = {2014-12},
  pages = {366--371},
  doi = {10.1109/SLT.2014.7078602},
  abstract = {Identifying the distinct register that adults use when speaking to children is an important task for child development research. We present a fully automatic, speaker-independent system that detects child-directed speech. The two-stage system uses diarization-style voice activation techniques to extract speech segments followed by a supervised ν-SVM classifier trained on 1582 prosodic and log Mel energy features. The system significantly improves the state of the art, detecting child-directed speech with F1 of .66 (exact boundary) and .83 (within 1 second). A feature analysis confirms the importance of F0 features (especially 3rd quartile and range) as well as new features like the variance, kurtosis, and min of log Mel energy within a frequency band.},
  eventtitle = {2014 {{IEEE Spoken Language Technology Workshop}} ({{SLT}})},
  keywords = {Accuracy,Child-directed Speech,Gold,Language Development,Measurement,Noise,Prosody,Speech,Speech Analysis,Support vector machines,Training},
  file = {/Users/brono/Zotero/storage/69KJJNVV/Schuster et al. - 2014 - Speaker-independent detection of child-directed sp.pdf;/Users/brono/Zotero/storage/Z2ZM66JX/stamp.html}
}

@article{schwabLanguageLearningSocioeconomic2016,
  title = {Language Learning, Socioeconomic Status, and Child-Directed Speech},
  author = {Schwab, Jessica F. and Lew-Williams, Casey},
  date = {2016},
  journaltitle = {WIREs Cognitive Science},
  volume = {7},
  number = {4},
  pages = {264--275},
  issn = {1939-5086},
  doi = {10.1002/wcs.1393},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1393},
  urldate = {2023-04-09},
  abstract = {Young children's language experiences and language outcomes are highly variable. Research in recent decades has focused on understanding the extent to which family socioeconomic status (SES) relates to parents’ language input to their children and, subsequently, children's language learning. Here, we first review research demonstrating differences in the quantity and quality of language that children hear across low-, mid-, and high-SES groups, but also—and perhaps more importantly—research showing that differences in input and learning also exist within SES groups. Second, in order to better understand the defining features of ‘high-quality’ input, we highlight findings from laboratory studies examining specific characteristics of the sounds, words, sentences, and social contexts of child-directed speech (CDS) that influence children's learning. Finally, after narrowing in on these particular features of CDS, we broaden our discussion by considering family and community factors that may constrain parents’ ability to participate in high-quality interactions with their young children. A unification of research on SES and CDS will facilitate a more complete understanding of the specific means by which input shapes learning, as well as generate ideas for crafting policies and programs designed to promote children's language outcomes. WIREs Cogn Sci 2016, 7:264–275. doi: 10.1002/wcs.1393 This article is categorized under: Linguistics {$>$} Language Acquisition Psychology {$>$} Language},
  langid = {english},
  file = {/Users/brono/Zotero/storage/W7H5569L/Schwab and Lew-Williams - 2016 - Language learning, socioeconomic status, and child.pdf;/Users/brono/Zotero/storage/HBI3TZ29/wcs.html}
}

@software{SCTKNISTScoring2023,
  title = {{{SCTK}}, the {{NIST Scoring Toolkit}}},
  date = {2023-03-22T18:32:40Z},
  origdate = {2016-05-03T19:00:47Z},
  url = {https://github.com/usnistgov/SCTK},
  urldate = {2023-04-21},
  organization = {{National Institute of Standards and Technology}}
}

@inproceedings{sellDiarizationHardExperiences2018,
  title = {Diarization Is {{Hard}}: {{Some Experiences}} and {{Lessons Learned}} for the {{JHU Team}} in the {{Inaugural DIHARD Challenge}}},
  shorttitle = {Diarization Is {{Hard}}},
  booktitle = {Interspeech 2018},
  author = {Sell, Gregory and Snyder, David and McCree, Alan and Garcia-Romero, Daniel and Villalba, Jesús and Maciejewski, Matthew and Manohar, Vimal and Dehak, Najim and Povey, Daniel and Watanabe, Shinji and Khudanpur, Sanjeev},
  date = {2018-09-02},
  pages = {2808--2812},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1893},
  url = {https://www.isca-speech.org/archive/interspeech_2018/sell18_interspeech.html},
  urldate = {2023-03-12},
  eventtitle = {Interspeech 2018},
  langid = {english},
  keywords = {x-vector},
  file = {/Users/brono/Zotero/storage/6WJXE9CI/Sell et al. - 2018 - Diarization is Hard Some Experiences and Lessons .pdf}
}

@article{shahinAutomaticDetectionSpeech2020,
  title = {The {{Automatic Detection}} of {{Speech Disorders}} in {{Children}}: {{Challenges}}, {{Opportunities}}, and {{Preliminary Results}}},
  shorttitle = {The {{Automatic Detection}} of {{Speech Disorders}} in {{Children}}},
  author = {Shahin, Mostafa and Zafar, Usman and Ahmed, Beena},
  date = {2020-02},
  journaltitle = {IEEE Journal of Selected Topics in Signal Processing},
  shortjournal = {IEEE J. Sel. Top. Signal Process.},
  volume = {14},
  number = {2},
  pages = {400--412},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2019.2959393},
  url = {https://ieeexplore.ieee.org/document/8931568/},
  urldate = {2023-02-01},
  abstract = {Given the limited accessibility to Speech and Language Pathologists (SLPs) children in need often have, pediatric Computer-Aided Speech Therapy (CAST) tools can play an important role in the early diagnosis and treatment of speech disorders. However, various challenges impede the implementation of accurate automated analysis of speech disorders in children. In this article, we first discuss three key challenges in processing child disordered speech: 1) the unreliability of low-level annotation and scarcity of speech corpora, 2) speaker diarization of therapy sessions and 3) inaccurate children’s acoustic models. We next explore opportunities to overcome some of these challenges. First, we investigate the effectiveness of high-level paralinguistic features in disordered speech detection to reduce the dependency on annotated data. A binary classifier trained using paralinguistic features extracted from both typically developing children and those suffering from Speech Sound Disorders (SSD) achieved 87\% subject-level classification accuracy. Second, we tackle the speech disorder detection problem as an anomaly detection problem where models are trained merely on typically developing speech, reducing the need for disordered training data. A phoneme-level F1 score of 0.77 was obtained from an anomaly detection-based system trained on speech attribute features to classify between typical and atypical phoneme pronunciations of children with speech disorder. Finally, we test the efficiency of an x-vector based speaker diarization technique in pediatric therapy sessions. The method successfully distinguished between therapist and child speech with a Diarization Error Rate (DER) of 10\%.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/FLZSBNCP/Shahin et al. - 2020 - The Automatic Detection of Speech Disorders in Chi.pdf}
}

@article{shahinDeepLearningInsomnia2017,
  title = {Deep {{Learning}} and {{Insomnia}}: {{Assisting Clinicians With Their Diagnosis}}},
  shorttitle = {Deep {{Learning}} and {{Insomnia}}},
  author = {Shahin, Mostafa and Ahmed, Beena and Hamida, Sana Tmar-Ben and Mulaffer, Fathima Lamana and Glos, Martin and Penzel, Thomas},
  date = {2017-11},
  journaltitle = {IEEE Journal of Biomedical and Health Informatics},
  shortjournal = {IEEE J. Biomed. Health Inform.},
  volume = {21},
  number = {6},
  pages = {1546--1553},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2017.2650199},
  url = {https://ieeexplore.ieee.org/document/7811237/},
  urldate = {2023-02-01},
  abstract = {Effective sleep analysis is hampered by the lack of automated tools catering to disordered sleep patterns and cumbersome monitoring hardware. In this paper, we apply deep learning on a set of 57 EEG features extracted from a maximum of two EEG channels to accurately differentiate between patients with insomnia or controls with no sleep complaints. We investigated two different approaches to achieve this. The first approach used EEG data from the whole sleep recording irrespective of the sleep stage (stage-independent classification), while the second used only EEG data from insomnia-impacted specific sleep stages (stage-dependent classification). We trained and tested our system using both healthy and disordered sleep collected from 41 controls and 42 primary insomnia patients. When compared with manual assessments, an NREM + REM based classifier had an overall discrimination accuracy of 92\% and 86\% between two groups using both two and one EEG channels, respectively. These results demonstrate that deep learning can be used to assist in the diagnosis of sleep disorders such as insomnia.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/EU984RQE/Shahin et al. - 2017 - Deep Learning and Insomnia Assisting Clinicians W.pdf}
}

@inproceedings{shahinOneClassSVMsBased2018,
  title = {One-{{Class SVMs Based Pronunciation Verification Approach}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Shahin, Mostafa and Ji, Jim X and Ahmed, Beena},
  date = {2018-08},
  pages = {2881--2886},
  publisher = {{IEEE}},
  location = {{Beijing}},
  doi = {10.1109/ICPR.2018.8545687},
  url = {https://ieeexplore.ieee.org/document/8545687/},
  urldate = {2023-02-01},
  abstract = {The automatic assessment of speech plays an important role in Computer Aided Pronunciation Learning systems. However, modeling both the correct and incorrect pronunciation of each phoneme to achieve accurate pronunciation verification is unfeasible due to the lack of sufficient mispronounced samples in training datasets. In this paper, we propose a novel approach that handles this unbalanced data distribution by building multiple one-class SVMs to evaluate each phoneme as correct or incorrect. We model the correct pronunciation of each individual phoneme with a one-class SVM trained using a set of speech attributes features, namely the manner and place of articulation. These features are extracted from a bank of pre-trained DNN speech attributes classifiers. The one-class SVM model measures the similarity between the new data and the training set and then classifies it as normal (correct) or an anomaly (incorrect). We evaluated the system using native speech corpus and disordered speech corpus and compared it with the conventional Goodness of Pronunciation (GOP) algorithm. The results show that our approach reduces the false-acceptance and false-rejection rates by around 26\% and 39\% respectively.},
  eventtitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  isbn = {978-1-5386-3788-3},
  langid = {english},
  file = {/Users/brono/Zotero/storage/WCUFAFBD/Shahin et al. - 2018 - One-Class SVMs Based Pronunciation Verification Ap.pdf}
}

@inproceedings{shenCECOSChineseEnglishCodeswitching2011,
  title = {{{CECOS}}: {{A Chinese-English}} Code-Switching Speech Database},
  shorttitle = {{{CECOS}}},
  booktitle = {2011 {{International Conference}} on {{Speech Database}} and {{Assessments}} ({{Oriental COCOSDA}})},
  author = {Shen, Han-Ping and Wu, Chung-Hsien and Yang, Yan-Ting and Hsu, Chun-Shan},
  date = {2011-10},
  pages = {120--123},
  publisher = {{IEEE}},
  location = {{Hsinchu City, Taiwan}},
  doi = {10.1109/ICSDA.2011.6085992},
  url = {http://ieeexplore.ieee.org/document/6085992/},
  urldate = {2023-06-13},
  abstract = {With the increase on the demands for code-switching automatic speech recognition (ASR), the design and development of a code-switching speech database becomes highly desirable. However, it is not easy to collect sufficient code-switched utterances for model training for codeswitching ASR. This study presents the procedure and experience for the design and development of a ChineseEnglish COde-switching Speech database (CECOS). Two different methods for collecting Chinese-English codeswitched utterances are employed in this work. The applications of the collected database are also introduced. The CECOS database not only contains the speech data with code-switch properties but also accents due to non-native speakers. This database can be applied to several applications, such as code-switching speech recognition, language identification, named entity detection, etc.},
  eventtitle = {2011 {{Oriental COCOSDA}} 2011 - {{International Conference}} on {{Speech Database}} and {{Assessments}}},
  isbn = {978-1-4577-0931-9 978-1-4577-0930-2 978-1-4577-0929-6},
  langid = {english},
  file = {/Users/brono/Zotero/storage/KNLIXF7J/Shen et al. - 2011 - CECOS A Chinese-English code-switching speech dat.pdf}
}

@online{shiASRU2019MandarinEnglish2020,
  title = {The {{ASRU}} 2019 {{Mandarin-English Code-Switching Speech Recognition Challenge}}: {{Open Datasets}}, {{Tracks}}, {{Methods}} and {{Results}}},
  shorttitle = {The {{ASRU}} 2019 {{Mandarin-English Code-Switching Speech Recognition Challenge}}},
  author = {Shi, Xian and Feng, Qiangze and Xie, Lei},
  date = {2020-07-12},
  eprint = {2007.05916},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2007.05916},
  urldate = {2023-04-18},
  abstract = {Code-switching (CS) is a common phenomenon and recognizing CS speech is challenging. But CS speech data is scarce and there' s no common testbed in relevant research. This paper describes the design and main outcomes of the ASRU 2019 Mandarin-English code-switching speech recognition challenge, which aims to improve the ASR performance in Mandarin-English code-switching situation. 500 hours Mandarin speech data and 240 hours Mandarin-English intra-sentencial CS data are released to the participants. Three tracks were set for advancing the AM and LM part in traditional DNN-HMM ASR system, as well as exploring the E2E models' performance. The paper then presents an overview of the results and system performance in the three tracks. It turns out that traditional ASR system benefits from pronunciation lexicon, CS text generating and data augmentation. In E2E track, however, the results highlight the importance of using language identification, building-up a rational set of modeling units and spec-augment. The other details in model training and method comparsion are discussed.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/6YFGH9ZR/Shi et al. - 2020 - The ASRU 2019 Mandarin-English Code-Switching Spee.pdf;/Users/brono/Zotero/storage/R5IREIMW/2007.html}
}

@video{shortmoviezonlineDivorceHinglishShort2011,
  entrysubtype = {video},
  title = {Divorce- {{A Hinglish Short Film}}},
  editor = {{ShortMoviezonline}},
  editortype = {director},
  date = {2011-12-09},
  url = {https://www.youtube.com/watch?v=nswIUw2y8P4&t=282s},
  urldate = {2023-04-21},
  abstract = {On the brink of a divorce this couple has to find out why exactly they want a divorce from their love marriage. Will they find out the reason or will they find a reason to re-live...  Watch the Hinglish short film  Divorce to find out what exactly happens. Watch here -}
}

@inproceedings{singhLEAPDiarizationSystem2019,
  title = {{{LEAP Diarization System}} for the {{Second DIHARD Challenge}}},
  booktitle = {Interspeech 2019},
  author = {Singh, Prachi and M.A., Harsha Vardhan and Ganapathy, Sriram and Kanagasundaram, A.},
  date = {2019-09-15},
  pages = {983--987},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2716},
  url = {https://www.isca-speech.org/archive/interspeech_2019/singh19_interspeech.html},
  urldate = {2023-04-03},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/Users/brono/Zotero/storage/B3T36TAI/Singh et al. - 2019 - LEAP Diarization System for the Second DIHARD Chal.pdf}
}

@inproceedings{singhLEAPDiarizationSystem2019a,
  title = {{{LEAP Diarization System}} for the {{Second DIHARD Challenge}}},
  booktitle = {Interspeech 2019},
  author = {Singh, Prachi and M.A., Harsha Vardhan and Ganapathy, Sriram and Kanagasundaram, A.},
  date = {2019-09-15},
  pages = {983--987},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2716},
  url = {https://www.isca-speech.org/archive/interspeech_2019/singh19_interspeech.html},
  urldate = {2023-04-17},
  abstract = {This paper presents the LEAP System, developed for the Second DIHARD diarization Challenge. The evaluation data in the challenge is composed of multi-talker speech in restaurants, doctor-patient conversations, child language acquisition recordings in home environments and audio extracted YouTube videos. The LEAP system is developed using two types of embeddings, one based on i-vector representations and the other one based on x-vector representations. The initial diarization output obtained using agglomerative hierarchical clustering (AHC) done on the probabilistic linear discriminant analysis (PLDA) scores is refined using the Variational-Bayes hidden Markov model (VB-HMM) model. We propose a modified VB-HMM model with posterior scaling which provides significant improvements in the final diarization error rate (DER). We also use a domain compensation on the i-vector features to reduce the mis-match between training and evaluation conditions.N(s)TN(s)TN(s)T Using the proposed approaches, we obtain relative improvements in DER of about 7.1\% relative for the best individual system over the DIHARD baseline system and about 13.7\% relative for the final system combination on evaluation set. An analysis performed using the proposed posterior scaling method shows that scaling results in improved discrimination among the HMM states in the VB-HMM.},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/Users/brono/Zotero/storage/CR4775GG/Singh et al. - 2019 - LEAP Diarization System for the Second DIHARD Chal.pdf}
}

@online{singhSupervisedHierarchicalClustering2023,
  title = {Supervised {{Hierarchical Clustering}} Using {{Graph Neural Networks}} for {{Speaker Diarization}}},
  author = {Singh, Prachi and Kaul, Amrit and Ganapathy, Sriram},
  date = {2023-02-24},
  eprint = {2302.12716},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2302.12716},
  urldate = {2023-03-01},
  abstract = {Conventional methods for speaker diarization involve windowing an audio file into short segments to extract speaker embeddings, followed by an unsupervised clustering of the embeddings. This multi-step approach generates speaker assignments for each segment. In this paper, we propose a novel Supervised HierArchical gRaph Clustering algorithm (SHARC) for speaker diarization where we introduce a hierarchical structure using Graph Neural Network (GNN) to perform supervised clustering. The supervision allows the model to update the representations and directly improve the clustering performance, thus enabling a single-step approach for diarization. In the proposed work, the input segment embeddings are treated as nodes of a graph with the edge weights corresponding to the similarity scores between the nodes. We also propose an approach to jointly update the embedding extractor and the GNN model to perform end-to-end speaker diarization (E2E-SHARC). During inference, the hierarchical clustering is performed using node densities and edge existence probabilities to merge the segments until convergence. In the diarization experiments, we illustrate that the proposed E2E-SHARC approach achieves 53\% and 44\% relative improvements over the baseline systems on benchmark datasets like AMI and Voxconverse, respectively.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,TODO},
  file = {/Users/brono/Zotero/storage/LR3HABGZ/Singh et al. - 2023 - Supervised Hierarchical Clustering using Graph Neu.pdf;/Users/brono/Zotero/storage/LLHXHWI7/2302.html}
}

@online{sitaramSurveyCodeswitchedSpeech2020,
  title = {A {{Survey}} of {{Code-switched Speech}} and {{Language Processing}}},
  author = {Sitaram, Sunayana and Chandu, Khyathi Raghavi and Rallabandi, Sai Krishna and Black, Alan W.},
  date = {2020-07-22},
  eprint = {1904.00784},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1904.00784},
  urldate = {2023-04-02},
  abstract = {Code-switching, the alternation of languages within a conversation or utterance, is a common communicative phenomenon that occurs in multilingual communities across the world. This survey reviews computational approaches for code-switched Speech and Natural Language Processing. We motivate why processing code-switched text and speech is essential for building intelligent agents and systems that interact with users in multilingual communities. As code-switching data and resources are scarce, we list what is available in various code-switched language pairs with the language processing tasks they can be used for. We review code-switching research in various Speech and NLP applications, including language processing tools and end-to-end systems. We conclude with future directions and open problems in the field.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/brono/Zotero/storage/TDM96VUI/Sitaram et al. - 2020 - A Survey of Code-switched Speech and Language Proc.pdf;/Users/brono/Zotero/storage/HSCHFWPD/1904.html}
}

@article{sivaramrajeyyagariAutomaticSpeakerDiarization2020,
  title = {Automatic {{Speaker Diarization}} Using {{Deep LSTM}} in {{Audio Lecturing}} of E-{{Khool Platform}}},
  author = {{Sivaram Rajeyyagari}},
  date = {2020-10},
  journaltitle = {Journal of Networking and Communication Systems (JNACS)},
  shortjournal = {JNACS},
  volume = {3},
  number = {4},
  pages = {17--25},
  issn = {2582-3817},
  doi = {10.46253/jnacs.v3i4.a3},
  url = {https://publisher.resbee.org/jnacs/archive/v3i4/a3.html},
  urldate = {2023-03-13},
  file = {/Users/brono/Zotero/storage/MEJPVNTB/Sivaram Rajeyyagari - 2020 - Automatic Speaker Diarization using Deep LSTM in A.pdf}
}

@inproceedings{snyderDeepNeuralNetwork2017,
  title = {Deep {{Neural Network Embeddings}} for {{Text-Independent Speaker Verification}}},
  booktitle = {Interspeech 2017},
  author = {Snyder, David and Garcia-Romero, Daniel and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2017-08-20},
  pages = {999--1003},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2017-620},
  url = {https://www.isca-speech.org/archive/interspeech_2017/snyder17_interspeech.html},
  urldate = {2023-04-14},
  abstract = {This paper investigates replacing i-vectors for text-independent speaker verification with embeddings extracted from a feedforward deep neural network. Long-term speaker characteristics are captured in the network by a temporal pooling layer that aggregates over the input speech. This enables the network to be trained to discriminate between speakers from variablelength speech segments. After training, utterances are mapped directly to fixed-dimensional speaker embeddings and pairs of embeddings are scored using a PLDA-based backend. We compare performance with a traditional i-vector baseline on NIST SRE 2010 and 2016. We find that the embeddings outperform i-vectors for short speech segments and are competitive on long duration test conditions. Moreover, the two representations are complementary, and their fusion improves on the baseline at all operating points. Similar systems have recently shown promising results when trained on very large proprietary datasets, but to the best of our knowledge, these are the best results reported for speaker-discriminative neural networks when trained and tested on publicly available corpora.},
  eventtitle = {Interspeech 2017},
  langid = {english},
  file = {/Users/brono/Zotero/storage/NABB56L7/Snyder et al. - 2017 - Deep Neural Network Embeddings for Text-Independen.pdf}
}

@online{snyderMUSANMusicSpeech2015,
  title = {{{MUSAN}}: {{A Music}}, {{Speech}}, and {{Noise Corpus}}},
  shorttitle = {{{MUSAN}}},
  author = {Snyder, David and Chen, Guoguo and Povey, Daniel},
  date = {2015-10-28},
  eprint = {1510.08484},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1510.08484},
  urldate = {2023-03-17},
  abstract = {This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound},
  file = {/Users/brono/Zotero/storage/YAKIWZUW/Snyder et al. - 2015 - MUSAN A Music, Speech, and Noise Corpus.pdf;/Users/brono/Zotero/storage/WFGG98IH/1510.html}
}

@inproceedings{snyderXVectorsRobustDNN2018,
  title = {X-{{Vectors}}: {{Robust DNN Embeddings}} for {{Speaker Recognition}}},
  shorttitle = {X-{{Vectors}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2018-04},
  pages = {5329--5333},
  publisher = {{IEEE}},
  location = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8461375},
  url = {https://ieeexplore.ieee.org/document/8461375/},
  urldate = {2023-03-17},
  abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
  eventtitle = {{{ICASSP}} 2018 - 2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5386-4658-8},
  langid = {english},
  file = {/Users/brono/Zotero/storage/M63YEQMI/Snyder et al. - 2018 - X-Vectors Robust DNN Embeddings for Speaker Recog.pdf}
}

@inproceedings{snyderXVectorsRobustDNN2018b,
  title = {X-{{Vectors}}: {{Robust DNN Embeddings}} for {{Speaker Recognition}}},
  shorttitle = {X-{{Vectors}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Snyder, David and Garcia-Romero, Daniel and Sell, Gregory and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2018-04},
  pages = {5329--5333},
  publisher = {{IEEE}},
  location = {{Calgary, AB}},
  doi = {10.1109/ICASSP.2018.8461375},
  url = {https://ieeexplore.ieee.org/document/8461375/},
  urldate = {2023-03-31},
  abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
  eventtitle = {{{ICASSP}} 2018 - 2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-5386-4658-8},
  langid = {english},
  file = {/Users/brono/Zotero/storage/PEPTRZZA/Snyder et al. - 2018 - X-Vectors Robust DNN Embeddings for Speaker Recog.pdf}
}

@book{SpeechCommunication132018,
  title = {Speech Communication: 13. {{ITG-FachtagungSprachkommunikation}} ; 10.-12. {{Oktober}} 2018 in {{Oldenburg}}},
  shorttitle = {Speech Communication},
  date = {2018},
  series = {{{ITG-Fachberichte}}},
  number = {282},
  publisher = {{VDE Verlag GmbH}},
  location = {{Berlin}},
  isbn = {978-3-8007-4767-2},
  pagetotal = {1}
}

@online{SpeechMultilingualNatural,
  title = {Speech and Multilingual Natural Language Framework for Speaker Change Detection and Diarization | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.eswa.2022.119238},
  url = {https://reader.elsevier.com/reader/sd/pii/S0957417422022564?token=AB90A6FB9336FE0AEEC930F98E1B0D694581D42CF02184CFFE41E1BEFC2E14ED1501F3E39DEF8941C8D3875747433F76&originRegion=us-east-1&originCreation=20230404125456},
  urldate = {2023-04-04},
  langid = {english},
  file = {/Users/brono/Zotero/storage/L499JWBJ/Speech and multilingual natural language framework.pdf}
}

@online{SpeechMultilingualNaturala,
  title = {Speech and Multilingual Natural Language Framework for Speaker Change Detection and Diarization | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.eswa.2022.119238},
  url = {https://reader.elsevier.com/reader/sd/pii/S0957417422022564?token=87367E6DC492956FBB1A42657807FA32EB93CDE4955E37C245318AAE51EE7AB12CA0925C748D7BA6CA16ED331B071DB6&originRegion=us-east-1&originCreation=20230416081833},
  urldate = {2023-04-16},
  langid = {english}
}

@inproceedings{stolckeDoverMethodCombining2019,
  title = {Dover: {{A Method}} for {{Combining Diarization Outputs}}},
  shorttitle = {Dover},
  booktitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Stolcke, Andreas and Yoshioka, Takuya},
  date = {2019-12},
  pages = {757--763},
  publisher = {{IEEE}},
  location = {{SG, Singapore}},
  doi = {10.1109/ASRU46091.2019.9004031},
  url = {https://ieeexplore.ieee.org/document/9004031/},
  urldate = {2023-03-13},
  eventtitle = {2019 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-72810-306-8},
  file = {/Users/brono/Zotero/storage/9AUAYWW4/Stolcke and Yoshioka - 2019 - Dover A Method for Combining Diarization Outputs.pdf}
}

@incollection{tadeleEffectLanguageMixture2022,
  title = {Effect of {{Language Mixture}} on {{Speaker Verification}}: {{An Investigation}} with {{Amharic}}, {{English}}, and {{Mandarin Chinese}}},
  shorttitle = {Effect of {{Language Mixture}} on {{Speaker Verification}}},
  booktitle = {Artificial {{Intelligence}} and {{Security}}},
  author = {Tadele, Firew and Wei, Jianguo and Honda, Kiyoshi and Zhang, Ruiteng and Yang, Wenhao},
  editor = {Sun, Xingming and Zhang, Xiaorui and Xia, Zhihua and Bertino, Elisa},
  date = {2022},
  volume = {13340},
  pages = {243--256},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-06791-4_20},
  url = {https://link.springer.com/10.1007/978-3-031-06791-4_20},
  urldate = {2023-06-13},
  abstract = {Speaker verification (SV) tasks with low-resource language corpora naturally face technical difficulties and often require language mixture processing. In this paper, the LibriSpeech ASR corpus, the AISHELL-I Mandarin Speech corpus, and the Yegna2021 corpus were used for training the x-vector model. The Yegna2021 is a bilingual speech corpus consisting of Amharic and English languages. We designed and collected the Yegna2021 corpus to facilitate SV experimentation. Over 200 native Ethiopian speakers who are bilingual in both languages have participated in the creation of the corpus. To the best of our knowledge, this is the first study of SV systems in Amharic language. This study proposes that improving SV performance degradation, caused by language mismatch between training and testing utterances, requires not only combining two or more languages for training, but also considering the phonetic similarities and differences between languages that impact on obtaining better SV performance. The varied effects of language combinations have been examined on Mandarin Chinese, Amharic, and English languages. In this paper, we investigate the impact of language mismatches between training and testing on SV performance using only the Yegna2021corpus. The experimental results show that a language variability between training and testing utterances significantly degrades SV performance (between 6.5\% to 9.0\%). The combination of Amharic and Mandarin yields better SV performance than English and Mandarin, achieving an Equal error rate (EER) of 8.3\% as compared to 9.8\%, with relative performance degradation of 17.1\%. To verify these results, we paired Mandarin with data from the LibriSpeech, and the result shows 18.2\% relative performance degradation, with an EER of 9.9\% for English and Mandarin.},
  isbn = {978-3-031-06790-7 978-3-031-06791-4},
  langid = {english},
  file = {/Users/brono/Zotero/storage/JE899EWI/Tadele et al. - 2022 - Effect of Language Mixture on Speaker Verification.pdf}
}

@article{tavarezNewBilingualSpeech,
  title = {New Bilingual Speech Databases for Audio Diarization},
  author = {Tavarez, David and Navas, Eva and Erro, Daniel and Saratxaga, Ibon and Hernaez, Inma},
  abstract = {This paper describes the process of collecting and recording two new bilingual speech databases in Spanish and Basque. They are designed primarily for speaker diarization in two different application domains: broadcast news audio and recorded meetings. First, both databases have been manually segmented. Next, several diarization experiments have been carried out in order to evaluate them. Our baseline speaker diarization system has been applied to both databases with around 30\% of DER for broadcast news audio and 40\% of DER for recorded meetings. Also, the behavior of the system when different languages are used by the same speaker has been tested.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/JXMYT2DH/Tavarez et al. - New bilingual speech databases for audio diarizati.pdf}
}

@book{tavastHumanLanguageTechnologies2012,
  title = {Human {{Language Technologies}}: {{The Baltic Perspective}} : {{Proceedings}} of the {{Fifth International Conference Baltic HLT}} 2012},
  shorttitle = {Human {{Language Technologies}}},
  author = {Tavast, Arvi and Muischnek, Kadri and Koit, Mare},
  date = {2012},
  eprint = {HHXK0WbgRO0C},
  eprinttype = {googlebooks},
  publisher = {{IOS Press}},
  abstract = {Human language technologies continue to play an important part in the modern information society.This book contains papers presented at the fifth international conference 'Human Language Technologies - The Baltic Perspective (Baltic HLT 2012)', held in Tartu, Estonia, in October 2012.Baltic HLT provides a special venue for new and ongoing work in computational linguistics and related disciplines, both in the Baltic states and in a broader geographical perspective. It brings together scientists, developers, providers and users of HLT, and is a forum for the sharing of new ideas and recent advances in human language processing, promoting cooperation between the research communities of computer science and linguistics from the Baltic countries and the rest of the world.Twenty long papers, as well as the posters or demos accepted for presentation at the conference, are published here. They cover a wide range of topics: morphological disambiguation, dependency syntax and valency, computational semantics, named entities, dialogue modeling, terminology extraction and management, machine translation, corpus and parallel corpus compiling, speech modeling and multimodal communication. Some of the papers also give a general overview of the state of the art of human language technology and language resources in the Baltic states.This book will be of interest to all those whose work involves the use and application of computational linguistics and related disciplines.},
  isbn = {978-1-61499-132-8},
  langid = {english},
  pagetotal = {313}
}

@online{tevissenMeasuringScoringSpeaker2023,
  title = {Towards {{Measuring}} and {{Scoring Speaker Diarization Fairness}}},
  author = {Tevissen, Yannis and Boudy, Jérôme and Chollet, Gérard and Petitpont, Frédéric},
  date = {2023-02-20},
  eprint = {2302.09991},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2302.09991},
  urldate = {2023-03-01},
  abstract = {Speaker diarization, or the task of finding "who spoke and when", is now used in almost every speech processing application. Nevertheless, its fairness has not yet been evaluated because there was no protocol to study its biases one by one. In this paper we propose a protocol and a scoring method designed to evaluate speaker diarization fairness. This protocol is applied on a large dataset of spoken utterances and report the performances of speaker diarization depending on the gender, the age, the accent of the speaker and the length of the spoken sentence. Some biases induced by the gender, or the accent of the speaker were identified when we applied a state-of-the-art speaker diarization method.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,TODO},
  file = {/Users/brono/Zotero/storage/K689FBFU/Tevissen et al. - 2023 - Towards Measuring and Scoring Speaker Diarization .pdf;/Users/brono/Zotero/storage/JW5ISMMN/2302.html}
}

@article{tranterOverviewAutomaticSpeaker2006,
  title = {An Overview of Automatic Speaker Diarization Systems},
  author = {Tranter, S.E. and Reynolds, D.A.},
  date = {2006-09},
  journaltitle = {IEEE Transactions on Audio, Speech and Language Processing},
  shortjournal = {IEEE Trans. Audio Speech Lang. Process.},
  volume = {14},
  number = {5},
  pages = {1557--1565},
  issn = {1558-7916},
  doi = {10.1109/TASL.2006.878256},
  url = {http://ieeexplore.ieee.org/document/1677976/},
  urldate = {2023-03-13}
}

@inproceedings{vandamFundamentalFrequencyChilddirected2014,
  title = {Fundamental Frequency of Child-Directed Speech Using Automatic Speech Recognition},
  booktitle = {2014 {{Joint}} 7th {{International Conference}} on {{Soft Computing}} and {{Intelligent Systems}} ({{SCIS}}) and 15th {{International Symposium}} on {{Advanced Intelligent Systems}} ({{ISIS}})},
  author = {VanDam, Mark and De Palma, Paul},
  date = {2014-12},
  pages = {1349--1353},
  publisher = {{IEEE}},
  location = {{Kita-Kyushu, Japan}},
  doi = {10.1109/SCIS-ISIS.2014.7044876},
  url = {http://ieeexplore.ieee.org/document/7044876/},
  urldate = {2023-04-17},
  abstract = {It has long been recognized that speech directed to infants and toddlers—child-directed speech (CDS) or motherese—is characterized by a collection of features including hyperarticulation, a distinctive lexicon, reduced structural complexity, and increased fundamental frequency (f0). This paper examines f0 in nearly 500 hours of recordings from 33 families, showing that mothers, but not fathers, distinguish CDS with consistently higher f0. We also found that, compared with parents of typically-developing children, parents of children who are hard-of-hearing do not differ in production of f0 to their children. Results are relevant for improved ASR application and better understanding of mothers' and fathers' speech to their children.},
  eventtitle = {2014 {{Joint}} 7th {{International Conference}} on {{Soft Computing}} and {{Intelligent Systems}} ({{SCIS}}) and 15th {{International Symposium}} on {{Advanced Intelligent Systems}} ({{ISIS}})},
  isbn = {978-1-4799-5955-6},
  langid = {english},
  file = {/Users/brono/Zotero/storage/JPCVDE33/VanDam and De Palma - 2014 - Fundamental frequency of child-directed speech usi.pdf}
}

@article{vanwykDomainAdaptationSpeaker,
  title = {Domain Adaptation for Speaker Diarisation in Low-Resource Environments},
  author = {family=Wyk, given=Lucas, prefix=van, useprefix=true},
  abstract = {In this study, we investigate methods with which to adapt a pre-trained diarisation system to a new target domain when only a small in-domain corpus is available and retraining is therefore not an option. We also develop a method for fine-tuning the adaptation process of a pre-trained speaker diarisation system using cluster analysis. Our domain adaptation process focuses on retraining and adapting the statistical components in a speaker diarisation pipeline, which are inherently domain specific, to the target domain. Lastly, we demonstrate this domain adaptation process in a real-world scenario by adapting a pre-trained diarisation system using a small in-domain dataset consisting of telephonic speech from South African call centres. We show that the adapted system can be used to provide metadata which aids the performance of automatic speech recognition systems through speaker-specific adaptations.},
  langid = {english},
  keywords = {TODO},
  file = {/Users/brono/Zotero/storage/N8CMP82T/van Wyk - Domain adaptation for speaker diarisation in low-r.pdf}
}

@inproceedings{varianiDeepNeuralNetworks2014,
  title = {Deep Neural Networks for Small Footprint Text-Dependent Speaker Verification},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Variani, Ehsan and Lei, Xin and McDermott, Erik and Moreno, Ignacio Lopez and Gonzalez-Dominguez, Javier},
  date = {2014-05},
  pages = {4052--4056},
  publisher = {{IEEE}},
  location = {{Florence, Italy}},
  doi = {10.1109/ICASSP.2014.6854363},
  url = {http://ieeexplore.ieee.org/document/6854363/},
  urldate = {2023-03-01},
  abstract = {In this paper we investigate the use of deep neural networks (DNNs) for a small footprint text-dependent speaker verification task. At development stage, a DNN is trained to classify speakers at the framelevel. During speaker enrollment, the trained DNN is used to extract speaker specific features from the last hidden layer. The average of these speaker features, or d-vector, is taken as the speaker model. At evaluation stage, a d-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision. Experimental results show the DNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task. In addition, the DNN based system is more robust to additive noise and outperforms the i-vector system at low False Rejection operating points. Finally the combined system outperforms the i-vector system by 14\% and 25\% relative in equal error rate (EER) for clean and noisy conditions respectively.},
  eventtitle = {{{ICASSP}} 2014 - 2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-4799-2893-4},
  langid = {english},
  keywords = {d-vector,TODO},
  file = {/Users/brono/Zotero/storage/NFAWKHWT/Variani et al. - 2014 - Deep neural networks for small footprint text-depe.pdf}
}

@inproceedings{vSVMBasedLanguage2018,
  title = {{{SVM Based Language Diarization}} for {{Code-Switched Bilingual Indian Speech Using Bottleneck Features}}},
  booktitle = {6th {{Workshop}} on {{Spoken Language Technologies}} for {{Under-Resourced Languages}} ({{SLTU}} 2018)},
  author = {V, Spoorthy and Thenkanidiyoor, Veena and D, Dileep A.},
  date = {2018-08-29},
  pages = {132--136},
  publisher = {{ISCA}},
  doi = {10.21437/SLTU.2018-28},
  url = {https://www.isca-speech.org/archive/sltu_2018/v18_sltu.html},
  urldate = {2023-04-16},
  eventtitle = {6th {{Workshop}} on {{Spoken Language Technologies}} for {{Under-Resourced Languages}} ({{SLTU}} 2018)},
  langid = {english}
}

@inproceedings{vuFirstSpeechRecognition2012,
  title = {A First Speech Recognition System for {{Mandarin-English}} Code-Switch Conversational Speech},
  booktitle = {2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Vu, Ngoc Thang and Lyu, Dau-Cheng and Weiner, Jochen and Telaar, Dominic and Schlippe, Tim and Blaicher, Fabian and Chng, Eng-Siong and Schultz, Tanja and Li, Haizhou},
  date = {2012-03},
  pages = {4889--4892},
  publisher = {{IEEE}},
  location = {{Kyoto, Japan}},
  doi = {10.1109/ICASSP.2012.6289015},
  url = {http://ieeexplore.ieee.org/document/6289015/},
  urldate = {2023-04-16},
  abstract = {This paper presents first steps toward a large vocabulary continuous speech recognition system (LVCSR) for conversational Mandarin-English code-switching (CS) speech. We applied state-of-the-art techniques such as speaker adaptive and discriminative training to build the first baseline system on the SEAME corpus [1] (South East Asia Mandarin-English). For acoustic modeling, we applied different phone merging approaches based on the International Phonetic Alphabet (IPA) and Bhattacharyya distance in combination with discriminative training to improve accuracy. On language model level, we investigated statistical machine translation (SMT) based text generation approaches for building code-switching language models. Furthermore, we integrated the provided information from a language identification system (LID) into the decoding process by using a multi-stream approach. Our best 2-pass system achieves a Mixed Error Rate (MER) of 36.6\% on the SEAME development set.},
  eventtitle = {{{ICASSP}} 2012 - 2012 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
  langid = {english},
  file = {/Users/brono/Zotero/storage/S5S6P6F5/Vu et al. - 2012 - A first speech recognition system for Mandarin-Eng.pdf}
}

@online{wangDKUDukeECELenovoSystemDiarization2021,
  title = {The {{DKU-DukeECE-Lenovo System}} for the {{Diarization Task}} of the 2021 {{VoxCeleb Speaker Recognition Challenge}}},
  author = {Wang, Weiqing and Cai, Danwei and Lin, Qingjian and Yang, Lin and Wang, Junjie and Wang, Jin and Li, Ming},
  date = {2021-09-06},
  eprint = {2109.02002},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2109.02002},
  urldate = {2023-03-13},
  abstract = {This report describes the submission of the DKU-DukeECE-Lenovo team to the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2021 track 4. Our system including a voice activity detection (VAD) model, a speaker embedding model, two clustering-based speaker diarization systems with different similarity measurements, two different overlapped speech detection (OSD) models, and a target-speaker voice activity detection (TS-VAD) model. Our final submission, consisting of 5 independent systems, achieves a DER of 5.07\% on the challenge test set.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/LFSSBZ6K/Wang et al. - 2021 - The DKU-DukeECE-Lenovo System for the Diarization .pdf;/Users/brono/Zotero/storage/DPE2W3X4/2109.html}
}

@online{wanGeneralizedEndtoEndLoss2020,
  title = {Generalized {{End-to-End Loss}} for {{Speaker Verification}}},
  author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  date = {2020-11-09},
  eprint = {1710.10467},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1710.10467},
  urldate = {2023-03-01},
  abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10\%, while reducing the training time by 60\% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e. "OK Google" and "Hey Google") as well as multiple dialects.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning,TODO},
  file = {/Users/brono/Zotero/storage/K9HJCK3X/Wan et al. - 2020 - Generalized End-to-End Loss for Speaker Verificati.pdf;/Users/brono/Zotero/storage/L9QGCR4E/1710.html}
}

@online{wanGeneralizedEndtoEndLoss2020a,
  title = {Generalized {{End-to-End Loss}} for {{Speaker Verification}}},
  author = {Wan, Li and Wang, Quan and Papir, Alan and Moreno, Ignacio Lopez},
  date = {2020-11-09},
  eprint = {1710.10467},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1710.10467},
  urldate = {2023-04-04},
  abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based endto-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10\%, while reducing the training time by 60\% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation —training a more accurate model that supports multiple keywords (i.e., “OK Google” and “Hey Google”) as well as multiple dialects.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  file = {/Users/brono/Zotero/storage/VG95ZN5U/Wan et al. - 2020 - Generalized End-to-End Loss for Speaker Verificati.pdf}
}

@online{wangOC16CE80ChineseEnglishMixlingual2016,
  title = {{{OC16-CE80}}: {{A Chinese-English Mixlingual Database}} and {{A Speech Recognition Baseline}}},
  shorttitle = {{{OC16-CE80}}},
  author = {Wang, Dong and Tang, Zhiyuan and Tang, Difei and Chen, Qing},
  date = {2016-09-27},
  eprint = {1609.08412},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1609.08412},
  urldate = {2023-06-12},
  abstract = {We present the OC16-CE80 Chinese-English mixlingual speech database which was released as a main resource for training, development and test for the Chinese-English mixlingual speech recognition (MixASR-CHEN) challenge on O-COCOSDA 2016. This database consists of 80 hours of speech signals recorded from more than 1,400 speakers, where the utterances are in Chinese but each involves one or several English words. Based on the database and another two free data resources (THCHS30 and the CMU dictionary), a speech recognition (ASR) baseline was constructed with the deep neural network-hidden Markov model (DNN-HMM) hybrid system. We then report the baseline results following the MixASR-CHEN evaluation rules and demonstrate that OC16-CE80 is a reasonable data resource for mixlingual research.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/74I4NPYT/Wang et al. - 2016 - OC16-CE80 A Chinese-English Mixlingual Database a.pdf;/Users/brono/Zotero/storage/UFXVY6QB/1609.html}
}

@inproceedings{wangScenarioDependentSpeakerDiarization2021,
  title = {Scenario-{{Dependent Speaker Diarization}} for {{DIHARD-III Challenge}}},
  booktitle = {Interspeech 2021},
  author = {Wang, Yu-Xuan and Du, Jun and He, Maokui and Niu, Shu-Tong and Sun, Lei and Lee, Chin-Hui},
  date = {2021-08-30},
  pages = {3106--3110},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2021-516},
  url = {https://www.isca-speech.org/archive/interspeech_2021/wang21y_interspeech.html},
  urldate = {2023-03-13},
  eventtitle = {Interspeech 2021},
  langid = {english},
  file = {/Users/brono/Zotero/storage/UDBRDDWV/Wang et al. - 2021 - Scenario-Dependent Speaker Diarization for DIHARD-.pdf}
}

@online{wangSpeakerDiarizationLSTM2018,
  title = {Speaker {{Diarization}} with {{LSTM}}},
  author = {Wang, Quan and Downey, Carlton and Wan, Li and Mansfield, Philip Andrew and Moreno, Ignacio Lopez},
  date = {2018-04-12},
  eprint = {1710.10468},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  url = {http://arxiv.org/abs/1710.10468},
  urldate = {2023-02-27},
  abstract = {For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and speaker diarization applications. However, mirroring the rise of deep learning in various domains, neural network based audio embeddings, also known as d-vectors, have consistently demonstrated superior speaker verification performance. In this paper, we build on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combine LSTM-based d-vector audio embeddings with recent work in non-parametric clustering to obtain a state-of-the-art speaker diarization system. Our system is evaluated on three standard public datasets, suggesting that d-vector based diarization systems offer significant advantages over traditional i-vector based systems. We achieved a 12.0\% diarization error rate on NIST SRE 2000 CALLHOME, while our model is trained with out-of-domain data from voice search logs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,d-vector,Electrical Engineering and Systems Science - Audio and Speech Processing,LSTM,RNN,Statistics - Machine Learning},
  file = {/Users/brono/Zotero/storage/GYHRCEJ2/Wang et al. - 2022 - Speaker Diarization with LSTM.pdf;/Users/brono/Zotero/storage/YCCPVLYC/1710.html}
}

@article{wangSpeakerDiarizationWho2012,
  title = {Speaker {{Diarization}} - “{{Who Spoke When}}”},
  author = {Wang, David I-Chung},
  date = {2012},
  journaltitle = {Queensland University of Technology},
  pages = {169},
  url = {https://eprints.qut.edu.au/59624/1/David_Wang_Thesis.pdf},
  abstract = {Speaker diarization is the process of annotating an input audio with information that attributes temporal regions of the audio signal to their respective sources, which may include both speech and non-speech events. For speech regions, the diarization system also specifies the locations of speaker boundaries and assign relative speaker labels to each homogeneous segment of speech. In short, speaker diarization systems effectively answer the question of ‘who spoke when’. There are several important applications for speaker diarization technology, such as facilitating speaker indexing systems to allow users to directly access the relevant segments of interest within a given audio, and assisting with other downstream processes such as summarizing and parsing. When combined with automatic speech recognition (ASR) systems, the metadata extracted from a speaker diarization system can provide complementary information for ASR transcripts including the location of speaker turns and relative speaker segment labels, making the transcripts more readable. Speaker diarization output can also be used to localize the instances of specific speakers to pool data for model adaptation, which in turn boosts transcription accuracies. Speaker diarization therefore plays an important role as a preliminary step in automatic transcription of audio data. The aim of this work is to improve the usefulness and practicality of speaker diarization technology, through the reduction of diarization error rates. In particular, this research is focused on the segmentation and clustering stages within a diarization system. Although particular emphasis is placed on the broadcast news audio domain and systems developed throughout this work are also trained and tested on broadcast news data, the techniques proposed in this dissertation are also applicable to other domains including telephone conversations and meetings audio. Three main research themes were pursued: heuristic rules for speaker segmentation, modelling uncertainty in speaker model estimates, and modelling uncertainty in eigenvoice speaker modelling. The use of heuristic approaches for the speaker segmentation task was first investigated, with emphasis placed on minimizing missed boundary detections. A set of heuristic rules was proposed, to govern the detection and heuristic selection of candidate speaker segment boundaries. A second pass, using the same heuristic algorithm with a smaller window, was also proposed with the aim of improving detection of boundaries around short speaker segments. Compared to single threshold based methods, the proposed heuristic approach was shown to provide improved segmentation performance, leading to a reduction in the overall diarization error rate. Methods to model the uncertainty in speaker model estimates were developed, to address the difficulties associated with making segmentation and clustering decisions with limited data in the speaker segments. The Bayes factor, derived specifically for multivariate Gaussian speaker modelling, was introduced to account for the uncertainty of the speaker model estimates. The use of the Bayes factor also enabled the incorporation of prior information regarding the audio to aid segmentation and clustering decisions. The idea of modelling uncertainty in speaker model estimates was also extended to the eigenvoice speaker modelling framework for the speaker clustering task. Building on the application of Bayesian approaches to the speaker diarization problem, the proposed approach takes into account the uncertainty associated with the explicit estimation of the speaker factors. The proposed decision criteria, based on Bayesian theory, was shown to generally outperform their nonBayesian counterparts.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/UCMNYA7M/Wang - Speaker Diarization - “Who Spoke When”.pdf}
}

@article{wangUSTCNELSLIPSystemDescription,
  title = {{{USTC-NELSLIP System Description}} for {{DIHARD-III Challenge}}},
  author = {Wang, Yuxuan and He, Maokui and Niu, Shutong and Sun, Lei and Gao, Tian and Fang, Xin and Pan, Jia},
  abstract = {This system description describes our submission system to the Third DIHARD Speech Diarization Challenge. Besides the traditional clustering based system, the innovation of our system lies in the combination of various front-end techniques to solve the diarization problem, including speech separation and target-speaker based voice activity detection (TS-VAD), combined with iterative data purification. We also adopted audio domain classification to design domain-dependent processing. Finally, we performed post processing to do system fusion and selection. Our best system achieved DERs of 11.30\% in track 1 and 16.78\% in track 2 on evaluation set, respectively.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/MH9EZXVG/Wang et al. - USTC-NELSLIP System Description for DIHARD-III Cha.pdf}
}

@online{wangWespeakerResearchProduction2022,
  title = {Wespeaker: {{A Research}} and {{Production}} Oriented {{Speaker Embedding Learning Toolkit}}},
  shorttitle = {Wespeaker},
  author = {Wang, Hongji and Liang, Chengdong and Wang, Shuai and Chen, Zhengyang and Zhang, Binbin and Xiang, Xu and Deng, Yanlei and Qian, Yanmin},
  date = {2022-11-01},
  eprint = {2210.17016},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2210.17016},
  urldate = {2023-04-14},
  abstract = {Speaker modeling is essential for many related tasks, such as speaker recognition and speaker diarization. The dominant modeling approach is fixed-dimensional vector representation, i.e., speaker embedding. This paper introduces a research and production oriented speaker embedding learning toolkit, Wespeaker. Wespeaker contains the implementation of scalable data management, state-of-the-art speaker embedding models, loss functions, and scoring back-ends, with highly competitive results achieved by structured recipes which were adopted in the winning systems in several speaker verification challenges. The application to other downstream tasks such as speaker diarization is also exhibited in the related recipe. Moreover, CPU- and GPU-compatible deployment codes are integrated for production-oriented development. The toolkit is publicly available at https://github.com/wenet-e2e/wespeaker.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/5INUB7DC/Wang et al. - 2022 - Wespeaker A Research and Production oriented Spea.pdf;/Users/brono/Zotero/storage/LE5PCP6W/2210.html}
}

@article{wangXMUSPEECHSystemThird,
  title = {{{XMUSPEECH System}} for the {{Third DIHARD Challenge}}},
  author = {Wang, Jie and Lu, Hao and Li, Lin and Hong, Qingyang},
  abstract = {The main task of speaker diarization is “who spoke when”. As the front-end of many speech systems, speaker diarization system plays an important role in the field of Automatic Speech Recognition. Speakers diarization system used to take ivector,d-vector,x-vector as embeddings, of which x-vectors were extracted by time-delay neural network (TDNN) extractor. Then, the resulting PLDA model is used to calculate log-likelihood ratio verification scores as a similarity metric for each pair of xvectors from the test records. According to the score metric, those x-vectors were clustered into many classes. In this paper, we used Residual Neural Network (ResNet) instead of time-delay neural network to extract ResNet vectors of segments. Testing on DIHARD3 corpus shows that the performance of the system is better than that of the system using time-delay neural network model as embeddings extractor.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/ISNNJVI7/Wang et al. - XMUSPEECH System for the Third DIHARD Challenge.pdf}
}

@online{watanabeCHiME6ChallengeTackling2020,
  title = {{{CHiME-6 Challenge}}:{{Tackling Multispeaker Speech Recognition}} for {{Unsegmented Recordings}}},
  shorttitle = {{{CHiME-6 Challenge}}},
  author = {Watanabe, Shinji and Mandel, Michael and Barker, Jon and Vincent, Emmanuel and Arora, Ashish and Chang, Xuankai and Khudanpur, Sanjeev and Manohar, Vimal and Povey, Daniel and Raj, Desh and Snyder, David and Subramanian, Aswin Shanmugam and Trmal, Jan and Yair, Bar Ben and Boeddeker, Christoph and Ni, Zhaoheng and Fujita, Yusuke and Horiguchi, Shota and Kanda, Naoyuki and Yoshioka, Takuya and Ryant, Neville},
  date = {2020-05-02},
  eprint = {2004.09249},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2004.09249},
  urldate = {2023-03-11},
  abstract = {Following the success of the 1st, 2nd, 3rd, 4th and 5th CHiME challenges we organize the 6th CHiME Speech Separation and Recognition Challenge (CHiME-6). The new challenge revisits the previous CHiME-5 challenge and further considers the problem of distant multi-microphone conversational speech diarization and recognition in everyday home environments. Speech material is the same as the previous CHiME-5 recordings except for accurate array synchronization. The material was elicited using a dinner party scenario with efforts taken to capture data that is representative of natural conversational speech. This paper provides a baseline description of the CHiME-6 challenge for both segmented multispeaker speech recognition (Track 1) and unsegmented multispeaker speech recognition (Track 2). Of note, Track 2 is the first challenge activity in the community to tackle an unsegmented multispeaker speech recognition scenario with a complete set of reproducible open source baselines providing speech enhancement, speaker diarization, and speech recognition modules.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/5VLZMYLA/Watanabe et al. - 2020 - CHiME-6 ChallengeTackling Multispeaker Speech Rec.pdf;/Users/brono/Zotero/storage/QDGYWQHY/2004.html}
}

@online{WelcomeSIDEKITDiarization,
  title = {Welcome to {{SIDEKIT}} for Diarization Documentation! — S4d 0.1.0 Documentation},
  url = {https://projets-lium.univ-lemans.fr/s4d/},
  urldate = {2023-04-07}
}

@online{winataDecadesProgressCodeSwitching2023,
  title = {The {{Decades Progress}} on {{Code-Switching Research}} in {{NLP}}: {{A Systematic Survey}} on {{Trends}} and {{Challenges}}},
  shorttitle = {The {{Decades Progress}} on {{Code-Switching Research}} in {{NLP}}},
  author = {Winata, Genta Indra and Aji, Alham Fikri and Yong, Zheng-Xin and Solorio, Thamar},
  date = {2023-05-24},
  eprint = {2212.09660},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.09660},
  urldate = {2023-06-12},
  abstract = {Code-Switching, a common phenomenon in written text and conversation, has been studied over decades by the natural language processing (NLP) research community. Initially, code-switching is intensively explored by leveraging linguistic theories and, currently, more machine-learning oriented approaches to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the code-switching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/SVI9KLT7/Winata et al. - 2023 - The Decades Progress on Code-Switching Research in.pdf;/Users/brono/Zotero/storage/KMNH9NV3/2212.html}
}

@online{winataDecadesProgressCodeSwitching2023a,
  title = {The {{Decades Progress}} on {{Code-Switching Research}} in {{NLP}}: {{A Systematic Survey}} on {{Trends}} and {{Challenges}}},
  shorttitle = {The {{Decades Progress}} on {{Code-Switching Research}} in {{NLP}}},
  author = {Winata, Genta Indra and Aji, Alham Fikri and Yong, Zheng-Xin and Solorio, Thamar},
  date = {2023-05-24},
  eprint = {2212.09660},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2212.09660},
  urldate = {2023-06-13},
  abstract = {Code-Switching, a common phenomenon in written text and conversation, has been studied over decades by the natural language processing (NLP) research community. Initially, code-switching is intensively explored by leveraging linguistic theories and, currently, more machine-learning oriented approaches to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the codeswitching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/7KURP4LD/Winata et al. - 2023 - The Decades Progress on Code-Switching Research in.pdf}
}

@inproceedings{winataMetaTransferLearningCodeSwitched2020,
  title = {Meta-{{Transfer Learning}} for {{Code-Switched Speech Recognition}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Winata, Genta Indra and Cahyawijaya, Samuel and Lin, Zhaojiang and Liu, Zihan and Xu, Peng and Fung, Pascale},
  date = {2020},
  pages = {3770--3776},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.348},
  url = {https://www.aclweb.org/anthology/2020.acl-main.348},
  urldate = {2023-04-16},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  file = {/Users/brono/Zotero/storage/UDAPZ6RT/Winata et al. - 2020 - Meta-Transfer Learning for Code-Switched Speech Re.pdf}
}

@article{woonCreatingCorpusMultilingual2021,
  title = {Creating a {{Corpus}} of {{Multilingual Parent-Child Speech Remotely}}: {{Lessons Learned}} in a {{Large-Scale Onscreen Picturebook Sharing Task}}},
  shorttitle = {Creating a {{Corpus}} of {{Multilingual Parent-Child Speech Remotely}}},
  author = {Woon, Fei Ting and Yogarrajah, Eshwaaree C. and Fong, Seraphina and Salleh, Nur Sakinah Mohd and Sundaray, Shamala and Styles, Suzy J.},
  date = {2021},
  journaltitle = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.734936},
  urldate = {2023-04-03},
  abstract = {With lockdowns and social distancing measures in place, research teams looking to collect naturalistic parent-child speech interactions have to develop alternatives to in-lab recordings and observational studies with long-stretch recordings. We designed a novel micro-longitudinal study, the Talk Together Study, which allowed us to create a rich corpus of parent-child speech interactions in a fully online environment (N participants = 142, N recordings = 410). In this paper, we discuss the methods we used, and the lessons learned during adapting and running the study. These lessons learned cover nine domains of research design, monitoring and feedback: Recruitment strategies, Surveys and Questionnaires, Video-call scheduling, Speech elicitation tools, Videocall protocols, Participant remuneration strategies, Project monitoring, Participant retention, and Data Quality, and may be used as a primer for teams planning to conduct remote studies in the future.},
  file = {/Users/brono/Zotero/storage/9K68QMH3/Woon et al. - 2021 - Creating a Corpus of Multilingual Parent-Child Spe.pdf}
}

@article{woonCreatingCorpusMultilingual2021a,
  title = {Creating a {{Corpus}} of {{Multilingual Parent-Child Speech Remotely}}: {{Lessons Learned}} in a {{Large-Scale Onscreen Picturebook Sharing Task}}},
  shorttitle = {Creating a {{Corpus}} of {{Multilingual Parent-Child Speech Remotely}}},
  author = {Woon, Fei Ting and Yogarrajah, Eshwaaree C. and Fong, Seraphina and Salleh, Nur Sakinah Mohd and Sundaray, Shamala and Styles, Suzy J.},
  date = {2021},
  journaltitle = {Frontiers in Psychology},
  volume = {12},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2021.734936},
  urldate = {2023-04-04},
  abstract = {With lockdowns and social distancing measures in place, research teams looking to collect naturalistic parent-child speech interactions have to develop alternatives to in-lab recordings and observational studies with long-stretch recordings. We designed a novel micro-longitudinal study, the Talk Together Study, which allowed us to create a rich corpus of parent-child speech interactions in a fully online environment (N participants = 142, N recordings = 410). In this paper, we discuss the methods we used, and the lessons learned during adapting and running the study. These lessons learned cover nine domains of research design, monitoring and feedback: Recruitment strategies, Surveys and Questionnaires, Video-call scheduling, Speech elicitation tools, Videocall protocols, Participant remuneration strategies, Project monitoring, Participant retention, and Data Quality, and may be used as a primer for teams planning to conduct remote studies in the future.},
  file = {/Users/brono/Zotero/storage/DA2MEXTJ/Woon et al. - 2021 - Creating a Corpus of Multilingual Parent-Child Spe.pdf}
}

@inproceedings{xiaoMicrosoftSpeakerDiarization2021,
  title = {Microsoft {{Speaker Diarization System}} for the {{Voxceleb Speaker Recognition Challenge}} 2020},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Xiao, Xiong and Kanda, Naoyuki and Chen, Zhuo and Zhou, Tianyan and Yoshioka, Takuya and Chen, Sanyuan and Zhao, Yong and Liu, Gang and Wu, Yu and Wu, Jian and Liu, Shujie and Li, Jinyu and Gong, Yifan},
  date = {2021-06-06},
  pages = {5824--5828},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9413832},
  url = {https://ieeexplore.ieee.org/document/9413832/},
  urldate = {2023-03-13},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  file = {/Users/brono/Zotero/storage/L5VB4H8S/Xiao et al. - 2021 - Microsoft Speaker Diarization System for the Voxce.pdf}
}

@article{xieCHILDRENCANBE,
  title = {{{CHILDREN CAN BE CHALLENGING}}: {{AUTOMATIC DIARIZATION OF CHILD-GUARDIAN INTERACTION}}},
  author = {Xie, Jiamin},
  langid = {english},
  file = {/Users/brono/Zotero/storage/L7NUZHBP/Xie - CHILDREN CAN BE CHALLENGING AUTOMATIC DIARIZATION.pdf}
}

@inproceedings{xieMultiPLDADiarizationChildren2019,
  title = {Multi-{{PLDA Diarization}} on {{Children}}’s {{Speech}}},
  booktitle = {Interspeech 2019},
  author = {Xie, Jiamin and García-Perera, Leibny Paola and Povey, Daniel and Khudanpur, Sanjeev},
  date = {2019-09-15},
  pages = {376--380},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2961},
  url = {https://www.isca-speech.org/archive/interspeech_2019/xie19_interspeech.html},
  urldate = {2023-03-23},
  abstract = {Children’s speech and other vocalizations pose challenges for speaker diarization. The spontaneity of kids causes rapid or delayed phonetic variations in an utterance, which makes speaker’s information difficult to extract. Fast speaker turns and long overlap in conversations between children and their guardians makes correct segmentation even harder compared to, say a business meeting. In this work, we explore diarization of child-guardian interactions. We investigate the effectiveness of adding children’s speech to adult data in Probabilistic Linear Discriminant Analysis (PLDA) training. We also train each of two PLDAs with separate objective to a coarse or fine classification of speakers. A fusion of the two PLDAs is examined. By performing this fusion, we expect to improve on children’s speech while preserving adult segmentations. Our experimental results show that including children’s speech helps reduce DER by 2.7\%, achieving a best overall DER of 33.1\% with the xvector system. A fusion system yields a reasonable 33.3\% DER that validates our concept.},
  eventtitle = {Interspeech 2019},
  langid = {english},
  file = {/Users/brono/Zotero/storage/XQE9HDLG/Xie et al. - 2019 - Multi-PLDA Diarization on Children’s Speech.pdf}
}

@article{xuSignalProcessingYoung,
  title = {Signal {{Processing}} for {{Young Child Speech Language Development}}},
  author = {Xu, Dongxin and Yapanel, Umit and Gray, Sharmi and Gilkerson, Jill and Richards, Jeff and Hansen, John},
  abstract = {Speech signal processing and other man-machine interaction technologies have been developed for improved child-computer interaction for education, entertainment, as well as other applications [1, 2]. However, for very young children (in the age range of 0 to 4 years old, and especially 0 to 2), such interaction is not encouraged [3, 4]. Instead, parent-child interaction is highly recommended [3, 4] since it promotes improved language development. In this study, a new system entitled LENATM (Language Environment Analysis) and its associate processing technologies will be introduced. LENA provides parents/caregivers with quantified statistical information concerning the language environment and development status of children in order to allow for the determination of what needs to improve and how to improve. The adult word count (AWC) estimation algorithm is shown to reduce the relative Root Mean Square Error from an initial 42\% to 7-8\% after 5 hours of measuring time. If LENA’s feedback suggests any potential development problem, parents can take action at a crucial early stage. LENA is a new processing system not only for parents/caregivers but for pediatricians, speech language pathologists, child development psychologists, and other researchers as well. This system represents one of the first breakthroughs in assessing early childhood language development and child environment conditions.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/WQSJHWAN/Xu et al. - Signal Processing for Young Child Speech Language .pdf}
}

@online{yamashitaImprovingNaturalnessSimulated2022,
  title = {Improving the {{Naturalness}} of {{Simulated Conversations}} for {{End-to-End Neural Diarization}}},
  author = {Yamashita, Natsuo and Horiguchi, Shota and Homma, Takeshi},
  date = {2022-04-24},
  eprint = {2204.11232},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2204.11232},
  urldate = {2023-03-13},
  abstract = {This paper investigates a method for simulating natural conversation in the model training of end-to-end neural diarization (EEND). Due to the lack of any annotated real conversational dataset, EEND is usually pretrained on a large-scale simulated conversational dataset first and then adapted to the target real dataset. Simulated datasets play an essential role in the training of EEND, but as yet there has been insufficient investigation into an optimal simulation method. We thus propose a method to simulate natural conversational speech. In contrast to conventional methods, which simply combine the speech of multiple speakers, our method takes turn-taking into account. We define four types of speaker transition and sequentially arrange them to simulate natural conversations. The dataset simulated using our method was found to be statistically similar to the real dataset in terms of the silence and overlap ratios. The experimental results on two-speaker diarization using the CALLHOME and CSJ datasets showed that the simulated dataset contributes to improving the performance of EEND.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/VJNJ6TA8/Yamashita et al. - 2022 - Improving the Naturalness of Simulated Conversatio.pdf;/Users/brono/Zotero/storage/2MZY345Z/2204.html}
}

@inproceedings{yangRobustEndtoendSpeaker2022,
  title = {Robust {{End-to-end Speaker Diarization}} with {{Generic Neural Clustering}}},
  booktitle = {Interspeech 2022},
  author = {Yang, Chenyu and Wang, Yu},
  date = {2022-09-18},
  pages = {1471--1475},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2022-10404},
  url = {https://www.isca-speech.org/archive/interspeech_2022/yang22r_interspeech.html},
  urldate = {2023-04-08},
  abstract = {End-to-end speaker diarization approaches have shown exceptional performance over the traditional modular approaches. To further improve the performance of the end-to-end speaker diarization for real speech recordings, recently works have been proposed which integrate unsupervised clustering algorithms with the end-to-end neural diarization models. However, these methods have a number of drawbacks: 1) The unsupervised clustering algorithms cannot leverage the supervision from the available datasets; 2) The K-means-based unsupervised algorithms that are explored often suffer from the constraint violation problem; 3) There is unavoidable mismatch between the supervised training and the unsupervised inference. In this paper, a robust generic neural clustering approach is proposed that can be integrated with any chunk-level predictor to accomplish a fully supervised end-to-end speaker diarization model. Also, by leveraging the sequence modelling ability of a recurrent neural network, the proposed neural clustering approach can dynamically estimate the number of speakers during inference. Experimental show that when integrating an attractor-based chunklevel predictor, the proposed neural clustering approach can yield better Diarization Error Rate (DER) than the constrained K-means-based clustering approaches under the mismatched conditions.},
  eventtitle = {Interspeech 2022},
  langid = {english},
  file = {/Users/brono/Zotero/storage/3CMPC7EP/Yang and Wang - 2022 - Robust End-to-end Speaker Diarization with Generic.pdf}
}

@inproceedings{yilmazLanguageDiarizationSemisupervised2017,
  title = {Language Diarization for Semi-Supervised Bilingual Acoustic Model Training},
  booktitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  author = {Yilmaz, Emre and McLaren, Mitchell and family=Heuvel, given=Henk, prefix=van den, useprefix=true and family=Leeuwen, given=David A., prefix=van, useprefix=true},
  date = {2017-12},
  pages = {91--96},
  publisher = {{IEEE}},
  location = {{Okinawa}},
  doi = {10.1109/ASRU.2017.8268921},
  url = {http://ieeexplore.ieee.org/document/8268921/},
  urldate = {2023-04-02},
  abstract = {In this paper, we investigate several automatic transcription schemes for using raw bilingual broadcast news data in semi-supervised bilingual acoustic model training. Specifically, we compare the transcription quality provided by a bilingual ASR system with another system performing language diarization at the front-end followed by two monolingual ASR systems chosen based on the assigned language label. Our research focuses on the Frisian-Dutch code-switching (CS) speech that is extracted from the archives of a local radio broadcaster. Using 11 hours of manually transcribed Frisian speech as a reference, we aim to increase the amount of available training data by using these automatic transcription techniques. By merging the manually and automatically transcribed data, we learn bilingual acoustic models and run ASR experiments on the development and test data of the FAME! speech corpus to quantify the quality of the automatic transcriptions. Using these acoustic models, we present speech recognition and CS detection accuracies. The results demonstrate that applying language diarization to the raw speech data to enable using the monolingual resources improves the automatic transcription quality compared to a baseline system using a bilingual ASR system.},
  eventtitle = {2017 {{IEEE Automatic Speech Recognition}} and {{Understanding Workshop}} ({{ASRU}})},
  isbn = {978-1-5090-4788-8},
  langid = {english},
  file = {/Users/brono/Zotero/storage/D9AKALJA/Yilmaz et al. - 2017 - Language diarization for semi-supervised bilingual.pdf}
}

@online{yilmazLargeScaleSpeakerDiarization2019,
  title = {Large-{{Scale Speaker Diarization}} of {{Radio Broadcast Archives}}},
  author = {Yılmaz, Emre and Derinel, Adem and Kun, Zhou and family=Heuvel, given=Henk, prefix=van den, useprefix=false and Brummer, Niko and Li, Haizhou and family=Leeuwen, given=David A., prefix=van, useprefix=true},
  date = {2019-06-28},
  eprint = {1906.07955},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1906.07955},
  urldate = {2023-04-04},
  abstract = {This paper describes our initial efforts to build a large-scale speaker diarization (SD) and identification system on a recently digitized radio broadcast archive from the Netherlands which has more than 6500 audio tapes with 3000 hours of Frisian-Dutch speech recorded between 1950-2016. The employed large-scale diarization scheme involves two stages: (1) tape-level speaker diarization providing pseudo-speaker identities and (2) speaker linking to relate pseudo-speakers appearing in multiple tapes. Having access to the speaker models of several frequently appearing speakers from the previously collected FAME! speech corpus, we further perform speaker identification by linking these known speakers to the pseudo-speakers identified at the first stage. In this work, we present a recently created longitudinal and multilingual SD corpus designed for large-scale SD research and evaluate the performance of a new speaker linking system using x-vectors with PLDA to quantify cross-tape speaker similarity on this corpus. The performance of this speaker linking system is evaluated on a small subset of the archive which is manually annotated with speaker information. The speaker linking performance reported on this subset (53 hours) and the whole archive (3000 hours) is compared to quantify the impact of scaling up in the amount of speech data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/NR3FK2V2/Yılmaz et al. - 2019 - Large-Scale Speaker Diarization of Radio Broadcast.pdf;/Users/brono/Zotero/storage/FG2HD829/1906.html}
}

@online{yilmazLargeScaleSpeakerDiarization2019a,
  title = {Large-{{Scale Speaker Diarization}} of {{Radio Broadcast Archives}}},
  author = {Yılmaz, Emre and Derinel, Adem and Kun, Zhou and family=Heuvel, given=Henk, prefix=van den, useprefix=false and Brummer, Niko and Li, Haizhou and family=Leeuwen, given=David A., prefix=van, useprefix=true},
  date = {2019-06-28},
  eprint = {1906.07955},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1906.07955},
  urldate = {2023-04-16},
  abstract = {This paper describes our initial efforts to build a large-scale speaker diarization (SD) and identification system on a recently digitized radio broadcast archive from the Netherlands which has more than 6500 audio tapes with 3000 hours of Frisian-Dutch speech recorded between 1950-2016. The employed large-scale diarization scheme involves two stages: (1) tape-level speaker diarization providing pseudo-speaker identities and (2) speaker linking to relate pseudo-speakers appearing in multiple tapes. Having access to the speaker models of several frequently appearing speakers from the previously collected FAME! speech corpus, we further perform speaker identification by linking these known speakers to the pseudo-speakers identified at the first stage. In this work, we present a recently created longitudinal and multilingual SD corpus designed for large-scale SD research and evaluate the performance of a new speaker linking system using x-vectors with PLDA to quantify cross-tape speaker similarity on this corpus. The performance of this speaker linking system is evaluated on a small subset of the archive which is manually annotated with speaker information. The speaker linking performance reported on this subset (53 hours) and the whole archive (3000 hours) is compared to quantify the impact of scaling up in the amount of speech data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/DJ3F78QW/Yılmaz et al. - 2019 - Large-Scale Speaker Diarization of Radio Broadcast.pdf;/Users/brono/Zotero/storage/7J5D64VM/1906.html}
}

@article{yilmazLongitudinalBilingualFrisianDutch,
  title = {A {{Longitudinal Bilingual Frisian-Dutch Radio Broadcast Database Designed}} for {{Code-Switching Research}}},
  author = {Yılmaz, Emre and Andringa, Maaike and Kingma, Sigrid and Dijkstra, Jelske},
  abstract = {We present a new speech database containing 18.5 hours of annotated radio broadcasts in the Frisian language. Frisian is mostly spoken in the province Fryslaˆn and it is the second official language of the Netherlands. The recordings are collected from the archives of Omrop Fryslaˆn, the regional public broadcaster of the province Fryslaˆn. The database covers almost a 50-year time span. The native speakers of Frisian are mostly bilingual and often code-switch in daily conversations due to the extensive influence of the Dutch language. Considering the longitudinal and code-switching nature of the data, an appropriate annotation protocol has been designed and the data is manually annotated with the orthographic transcription, speaker identities, dialect information, code-switching details and background noise/music information.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/RRCXU3SS/Yılmaz et al. - A Longitudinal Bilingual Frisian-Dutch Radio Broad.pdf}
}

@inproceedings{yilmazOpenSourceSpeech2016,
  title = {Open {{Source Speech}} and {{Language Resources}} for {{Frisian}}},
  booktitle = {Interspeech 2016},
  author = {Yılmaz, Emre and family=Heuvel, given=Henk, prefix=van den, useprefix=false and Dijkstra, Jelske and family=Velde, given=Hans Van, prefix=de, useprefix=false and Kampstra, Frederik and Algra, Jouke and Leeuwen, David Van},
  date = {2016-09-08},
  pages = {1536--1540},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2016-48},
  url = {https://www.isca-speech.org/archive/interspeech_2016/ylmaz16b_interspeech.html},
  urldate = {2023-04-16},
  abstract = {In this paper, we present several open source speech and language resources for the under-resourced Frisian language. Frisian is mostly spoken in the province of Fryslaˆn which is located in the north of the Netherlands. The native speakers of Frisian are Frisian-Dutch bilingual and often code-switch in daily conversations. The resources presented in this paper include a code-switching speech database containing radio broadcasts, a phonetic lexicon with more than 70k words and a language model trained on a text corpus with more than 38M words. With this contribution, we aim to share the Frisian resources we have collected in the scope of the FAME! project, in which a spoken document retrieval system is built for the disclosure of the regional broadcaster’s radio archives. These resources enable research on code-switching and longitudinal speech and language change. Moreover, a sample automatic speech recognition (ASR) recipe for the Kaldi toolkit will also be provided online to facilitate the Frisian ASR research.},
  eventtitle = {Interspeech 2016},
  langid = {english},
  file = {/Users/brono/Zotero/storage/DWNU8JJH/Yılmaz et al. - 2016 - Open Source Speech and Language Resources for Fris.pdf}
}

@online{yilmazSemisupervisedAcousticModel2018,
  title = {Semi-Supervised Acoustic Model Training for Speech with Code-Switching},
  author = {Yılmaz, Emre and McLaren, Mitchell and family=Heuvel, given=Henk, prefix=van den, useprefix=false and family=Leeuwen, given=David A., prefix=van, useprefix=true},
  date = {2018-10-23},
  eprint = {1810.09699},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.09699},
  urldate = {2023-04-04},
  abstract = {In the FAME! project, we aim to develop an automatic speech recognition (ASR) system for Frisian-Dutch code-switching (CS) speech extracted from the archives of a local broadcaster with the ultimate goal of building a spoken document retrieval system. Unlike Dutch, Frisian is a low-resourced language with a very limited amount of manually annotated speech data. In this paper, we describe several automatic annotation approaches to enable using of a large amount of raw bilingual broadcast data for acoustic model training in a semi-supervised setting. Previously, it has been shown that the best-performing ASR system is obtained by two-stage multilingual deep neural network (DNN) training using 11 hours of manually annotated CS speech (reference) data together with speech data from other high-resourced languages. We compare the quality of transcriptions provided by this bilingual ASR system with several other approaches that use a language recognition system for assigning language labels to raw speech segments at the front-end and using monolingual ASR resources for transcription. We further investigate automatic annotation of the speakers appearing in the raw broadcast data by first labeling with (pseudo) speaker tags using a speaker diarization system and then linking to the known speakers appearing in the reference data using a speaker recognition system. These speaker labels are essential for speaker-adaptive training in the proposed setting. We train acoustic models using the manually and automatically annotated data and run recognition experiments on the development and test data of the FAME! speech corpus to quantify the quality of the automatic annotations. The ASR and CS detection results demonstrate the potential of using automatic language and speaker tagging in semi-supervised bilingual acoustic model training.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/ZAAJHBTX/Yılmaz et al. - 2018 - Semi-supervised acoustic model training for speech.pdf;/Users/brono/Zotero/storage/9VGRJFAH/1810.html}
}

@online{yilmazSemisupervisedAcousticModel2018a,
  title = {Semi-Supervised Acoustic Model Training for Speech with Code-Switching},
  author = {Yılmaz, Emre and McLaren, Mitchell and family=Heuvel, given=Henk, prefix=van den, useprefix=false and family=Leeuwen, given=David A., prefix=van, useprefix=true},
  date = {2018-10-23},
  eprint = {1810.09699},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.09699},
  urldate = {2023-04-16},
  abstract = {In the FAME! project, we aim to develop an automatic speech recognition (ASR) system for Frisian-Dutch code-switching (CS) speech extracted from the archives of a local broadcaster with the ultimate goal of building a spoken document retrieval system. Unlike Dutch, Frisian is a low-resourced language with a very limited amount of manually annotated speech data. In this paper, we describe several automatic annotation approaches to enable using of a large amount of raw bilingual broadcast data for acoustic model training in a semi-supervised setting. Previously, it has been shown that the best-performing ASR system is obtained by two-stage multilingual deep neural network (DNN) training using 11 hours of manually annotated CS speech (reference) data together with speech data from other high-resourced languages. We compare the quality of transcriptions provided by this bilingual ASR system with several other approaches that use a language recognition system for assigning language labels to raw speech segments at the front-end and using monolingual ASR resources for transcription. We further investigate automatic annotation of the speakers appearing in the raw broadcast data by first labeling with (pseudo) speaker tags using a speaker diarization system and then linking to the known speakers appearing in the reference data using a speaker recognition system. These speaker labels are essential for speaker-adaptive training in the proposed setting. We train acoustic models using the manually and automatically annotated data and run recognition experiments on the development and test data of the FAME! speech corpus to quantify the quality of the automatic annotations. The ASR and CS detection results demonstrate the potential of using automatic language and speaker tagging in semi-supervised bilingual acoustic model training.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/M59TJJJU/Yılmaz et al. - 2018 - Semi-supervised acoustic model training for speech.pdf;/Users/brono/Zotero/storage/MKBMGNRS/1810.html}
}

@inproceedings{yinNeuralSpeechTurn2018,
  title = {Neural {{Speech Turn Segmentation}} and {{Affinity Propagation}} for {{Speaker Diarization}}},
  booktitle = {Interspeech 2018},
  author = {Yin, Ruiqing and Bredin, Hervé and Barras, Claude},
  date = {2018-09-02},
  pages = {1393--1397},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1750},
  url = {https://www.isca-speech.org/archive/interspeech_2018/yin18b_interspeech.html},
  urldate = {2023-04-07},
  abstract = {Speaker diarization is the task of determining “who speaks when” in an audio stream. Most diarization systems rely on statistical models to address four sub-tasks: speech activity detection (SAD), speaker change detection (SCD), speech turn clustering, and re-segmentation. First, following the recent success of recurrent neural networks (RNN) for SAD and SCD, we propose to address re-segmentation with Long-Short Term Memory (LSTM) networks. Then, we propose to use affinity propagation on top of neural speaker embeddings for speech turn clustering, outperforming regular Hierarchical Agglomerative Clustering (HAC). Finally, all these modules are combined and jointly optimized to form a speaker diarization pipeline in which all but the clustering step are based on RNNs. We provide experimental results on the French Broadcast dataset ETAPE where we reach state-of-the-art performance.},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/5JFPSJ5K/Yin et al. - 2018 - Neural Speech Turn Segmentation and Affinity Propa.pdf}
}

@incollection{zajicInvestigationSegmentationIVector2016,
  title = {Investigation of {{Segmentation}} in I-{{Vector Based Speaker Diarization}} of {{Telephone Speech}}},
  booktitle = {Speech and {{Computer}}},
  author = {Zajíc, Zbyněk and Kunešová, Marie and Radová, Vlasta},
  editor = {Ronzhin, Andrey and Potapova, Rodmonga and Németh, Géza},
  date = {2016},
  volume = {9811},
  pages = {411--418},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-43958-7_49},
  url = {http://link.springer.com/10.1007/978-3-319-43958-7_49},
  urldate = {2023-03-20},
  isbn = {978-3-319-43957-0 978-3-319-43958-7},
  langid = {english},
  file = {/Users/brono/Zotero/storage/HCLGGI9I/Zajíc et al. - 2016 - Investigation of Segmentation in i-Vector Based Sp.pdf}
}

@article{zajicUWBNTISSpeakerDiarization,
  title = {{{UWB-NTIS Speaker Diarization Systems}} for the {{DIHARDIII}} 2020 {{Challenge}}},
  author = {Zajıc, Zbyneˇk},
  abstract = {This document describes the diarization system developed by the team from the New Technologies for the Information Society (NTIS) research center of the University of West Bohemia (team “UWB-NTIS”), for the Third DIHARD Speech Diarization Challenge.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/L34U5ZJK/Zajıc - UWB-NTIS Speaker Diarization Systems for the DIHAR.pdf}
}

@inproceedings{zajicZCUNTISSpeakerDiarization2018,
  title = {{{ZCU-NTIS Speaker Diarization System}} for the {{DIHARD}} 2018 {{Challenge}}},
  booktitle = {Interspeech 2018},
  author = {Zajíc, Zbyněk and Kunešová, Marie and Zelinka, Jan and Hrúz, Marek},
  date = {2018-09-02},
  pages = {2788--2792},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1252},
  url = {https://www.isca-speech.org/archive/interspeech_2018/zajic18_interspeech.html},
  urldate = {2023-04-17},
  abstract = {In this paper, we present the system developed by the team from the New Technologies for the Information Society (NTIS) research center of the University of West Bohemia, for the First DIHARD Speech Diarization Challenge. The base of our system follows the currently-standard approach of segmentation, i-vector extraction, clustering, and resegmentation. Here, we describe the modifications to the system which allowed us to apply it to data from a range of different domains. The main contribution to our achievement is a Neural Network (NN) based domain classifier, which categorizes each conversation into one of the ten domains present in the development set. This classification determines the specific system configuration, such as the expected number of speakers and the stopping criterion for the hierarchical clustering. At the time of writing of this abstract, our best submission achieves a DER of 26.90\% and an MI of 8.34 bits on the evaluation set (gold speech/nonspeech segmentation).},
  eventtitle = {Interspeech 2018},
  langid = {english},
  file = {/Users/brono/Zotero/storage/4MPJMYW2/Zajíc et al. - 2018 - ZCU-NTIS Speaker Diarization System for the DIHARD.pdf}
}

@online{zeghidourDIVEEndtoendSpeech2021,
  title = {{{DIVE}}: {{End-to-end Speech Diarization}} via {{Iterative Speaker Embedding}}},
  shorttitle = {{{DIVE}}},
  author = {Zeghidour, Neil and Teboul, Olivier and Grangier, David},
  date = {2021-05-28},
  eprint = {2105.13802},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2105.13802},
  urldate = {2023-03-13},
  abstract = {We introduce DIVE, an end-to-end speaker diarization algorithm. Our neural algorithm presents the diarization task as an iterative process: it repeatedly builds a representation for each speaker before predicting the voice activity of each speaker conditioned on the extracted representations. This strategy intrinsically resolves the speaker ordering ambiguity without requiring the classical permutation invariant training loss. In contrast with prior work, our model does not rely on pretrained speaker representations and optimizes all parameters of the system with a multi-speaker voice activity loss. Importantly, our loss explicitly excludes unreliable speaker turn boundaries from training, which is adapted to the standard collar-based Diarization Error Rate (DER) evaluation. Overall, these contributions yield a system redefining the state-of-the-art on the standard CALLHOME benchmark, with 6.7\% DER compared to 7.8\% for the best alternative.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/DLWFLRAY/Zeghidour et al. - 2021 - DIVE End-to-end Speech Diarization via Iterative .pdf;/Users/brono/Zotero/storage/8WL4WL2M/2105.html}
}

@online{zengEndtoEndSolutionMandarinEnglish2019,
  title = {On the {{End-to-End Solution}} to {{Mandarin-English Code-switching Speech Recognition}}},
  author = {Zeng, Zhiping and Khassanov, Yerbolat and Pham, Van Tung and Xu, Haihua and Chng, Eng Siong and Li, Haizhou},
  date = {2019-07-11},
  eprint = {1811.00241},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1811.00241},
  urldate = {2023-06-09},
  abstract = {Code-switching (CS) refers to a linguistic phenomenon where a speaker uses different languages in an utterance or between alternating utterances. In this work, we study end-to-end (E2E) approaches to the Mandarin-English code-switching speech recognition (CSSR) task. We first examine the effectiveness of using data augmentation and byte-pair encoding (BPE) subword units. More importantly, we propose a multitask learning recipe, where a language identification task is explicitly learned in addition to the E2E speech recognition task. Furthermore, we introduce an efficient word vocabulary expansion method for language modeling to alleviate data sparsity issues under the code-switching scenario. Experimental results on the SEAME data, a Mandarin-English CS corpus, demonstrate the effectiveness of the proposed methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/brono/Zotero/storage/DYYPEUDX/Zeng et al. - 2019 - On the End-to-End Solution to Mandarin-English Cod.pdf;/Users/brono/Zotero/storage/4LLWWML7/1811.html}
}

@article{zhangAgglomerativeClusteringMaximum2013,
  title = {Agglomerative Clustering via Maximum Incremental Path Integral},
  author = {Zhang, Wei and Zhao, Deli and Wang, Xiaogang},
  date = {2013-11},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {46},
  number = {11},
  pages = {3056--3065},
  issn = {00313203},
  doi = {10.1016/j.patcog.2013.04.013},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320313001830},
  urldate = {2023-03-06},
  abstract = {Agglomerative clustering, which iteratively merges small clusters, is commonly used for clustering because it is conceptually simple and produces a hierarchy of clusters. In this paper, we propose a novel graph-structural agglomerative clustering algorithm, where the graph encodes local structures of data. The idea is to define a structural descriptor of clusters on the graph and to assume that two clusters have large affinity if their structural descriptors undergo substantial change when merging them into one cluster. A key insight of this paper to treat a cluster as a dynamical system and its samples as states. Based on that, Path Integral, which has been introduced in statistical mechanics and quantum mechanics, is utilized to measure the stability of a dynamical system. It is proposed as the structural descriptor, and the affinity between two clusters is defined as Incremental Path Integral, which can be computed in a closedform exact solution, with linear time complexity with respect to the maximum size of clusters. A probabilistic justification of the algorithm based on absorbing random walk is provided. Experimental comparison on toy data and imagery data shows that it achieves considerable improvement over the state-of-the-art clustering algorithms.},
  langid = {english},
  file = {/Users/brono/Zotero/storage/8W499WIA/Zhang et al. - 2013 - Agglomerative clustering via maximum incremental p.pdf}
}

@inproceedings{zhangParalinguisticApproachSpeaker2017,
  title = {A {{Paralinguistic Approach To Speaker Diarisation}}: {{Using Age}}, {{Gender}}, {{Voice Likability}} and {{Personality Traits}}},
  shorttitle = {A {{Paralinguistic Approach To Speaker Diarisation}}},
  booktitle = {Proceedings of the 25th {{ACM}} International Conference on {{Multimedia}}},
  author = {Zhang, Yue and Weninger, Felix and Liu, Boqing and Schmitt, Maximilian and Eyben, Florian and Schuller, Björn},
  date = {2017-10-19},
  pages = {387--392},
  publisher = {{ACM}},
  location = {{Mountain View California USA}},
  doi = {10.1145/3123266.3123338},
  url = {https://dl.acm.org/doi/10.1145/3123266.3123338},
  urldate = {2023-04-17},
  abstract = {In this work, we present a new view on automatic speaker diarisation, i. e., assessing “who speaks when”, based on the recognition of speaker traits such as age, gender, voice likability, and personality. Traditionally, speaker diarisation is accomplished using low-level audio descriptors (e. g., cepstral or spectral features), neglecting the fact that speakers can be well discriminated by humans according to various perceived characteristics. us, we advocate a novel paralinguistic approach that combines speaker diarisation with speaker characterisation by automatically identifying the speakers according to their individual traits. In a three-tier processing ow, speaker segmentation by voice activity detection (VAD) is initially performed to detect speaker turns. Next, speaker a ributes are predicted using pre-trained paralinguistic models. To tag the speakers, clustering algorithms are applied to the predicted traits. We evaluate our methods against state-of-the-art open source and commercial systems on a corpus of realistic, spontaneous dyadic conversations recorded in the wild from three di erent cultures (Chinese, English, German). Our results provide clear evidence that using paralinguistic features for speaker diarisation is a promising avenue of research.},
  eventtitle = {{{MM}} '17: {{ACM Multimedia Conference}}},
  isbn = {978-1-4503-4906-2},
  langid = {english},
  file = {/Users/brono/Zotero/storage/IU6TR6L3/Zhang et al. - 2017 - A Paralinguistic Approach To Speaker Diarisation .pdf}
}

@online{zhengReformulatingSpeakerDiarization2022,
  title = {Reformulating {{Speaker Diarization}} as {{Community Detection With Emphasis On Topological Structure}}},
  author = {Zheng, Siqi and Suo, Hongbin},
  date = {2022-04-26},
  eprint = {2204.12112},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2204.12112},
  urldate = {2023-04-15},
  abstract = {Clustering-based speaker diarization has stood firm as one of the major approaches in reality, despite recent development in end-to-end diarization. However, clustering methods have not been explored extensively for speaker diarization. Commonly-used methods such as k-means, spectral clustering, and agglomerative hierarchical clustering only take into account properties such as proximity and relative densities. In this paper we propose to view clustering-based diarization as a community detection problem. By doing so the topological structure is considered. This work has four major contributions. First it is shown that Leiden community detection algorithm significantly outperforms the previous methods on the clustering of speaker-segments. Second, we propose to use uniform manifold approximation to reduce dimension while retaining global and local topological structure. Third, a masked filtering approach is introduced to extract "clean" speaker embeddings. Finally, the community structure is applied to an end-to-end post-processing network to obtain diarization results. The final system presents a relative DER reduction of up to 70 percent. The breakdown contribution of each component is analyzed.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/NXNI9AXJ/Zheng and Suo - 2022 - Reformulating Speaker Diarization as Community Det.pdf;/Users/brono/Zotero/storage/9B9J4QFM/2204.html}
}

@inproceedings{zhuComparisonStudyInfantParent2021,
  title = {A {{Comparison Study}} on {{Infant-Parent Voice Diarization}}},
  booktitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhu, Junzhe and Hasegawa-Johnson, Mark and McElwain, Nancy L.},
  date = {2021-06-06},
  pages = {7178--7182},
  publisher = {{IEEE}},
  location = {{Toronto, ON, Canada}},
  doi = {10.1109/ICASSP39728.2021.9413538},
  url = {https://ieeexplore.ieee.org/document/9413538/},
  urldate = {2023-04-17},
  eventtitle = {{{ICASSP}} 2021 - 2021 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  isbn = {978-1-72817-605-5},
  file = {/Users/brono/Zotero/storage/7LLUVWSK/Zhu et al. - 2021 - A Comparison Study on Infant-Parent Voice Diarizat.pdf;/Users/brono/Zotero/storage/YZN7W2QA/Zhu et al. - 2021 - A Comparison Study on Infant-Parent Voice Diarizat.pdf}
}

@online{zhuYVectorMultiscaleWaveform2021,
  title = {Y-{{Vector}}: {{Multiscale Waveform Encoder}} for {{Speaker Embedding}}},
  shorttitle = {Y-{{Vector}}},
  author = {Zhu, Ge and Jiang, Fei and Duan, Zhiyao},
  date = {2021-06-08},
  eprint = {2010.12951},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/2010.12951},
  urldate = {2023-04-14},
  abstract = {State-of-the-art text-independent speaker verification systems typically use cepstral features or filter bank energies as speech features. Recent studies attempted to extract speaker embeddings directly from raw waveforms and have shown competitive results. In this paper, we propose a novel multi-scale waveform encoder that uses three convolution branches with different time scales to compute speech features from the waveform. These features are then processed by squeeze-and-excitation blocks, a multi-level feature aggregator, and a time delayed neural network (TDNN) to compute speaker embedding. We show that the proposed embeddings outperform existing raw-waveform-based speaker embeddings on speaker verification by a large margin. A further analysis of the learned filters shows that the multi-scale encoder attends to different frequency bands at its different scales while resulting in a more flat overall frequency response than any of the single-scale counterparts.},
  pubstate = {preprint},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/brono/Zotero/storage/Z4CTXFW3/Zhu et al. - 2021 - Y-Vector Multiscale Waveform Encoder for Speaker .pdf;/Users/brono/Zotero/storage/8MJPUMR8/2010.html}
}

@article{zirkerIntrasententialVsIntersentential,
  title = {Intrasentential vs. {{Intersentential Code Switching}} in {{Early}} and {{Late Bilinguals}}},
  author = {Zirker, Kelly Ann Hill},
  langid = {english},
  file = {/Users/brono/Zotero/storage/3CY3D45Q/Zirker - Intrasentential vs. Intersentential Code Switching.pdf}
}
@inproceedings{Bredin2020,
  title     = {{pyannote.audio: neural building blocks for speaker diarization}},
  author    = {{Bredin}, Herv{\'e} and {Yin}, Ruiqing and {Coria}, Juan Manuel and {Gelly}, Gregory and {Korshunov}, Pavel and {Lavechin}, Marvin and {Fustes}, Diego and {Titeux}, Hadrien and {Bouaziz}, Wassim and {Gill}, Marie-Philippe},
  booktitle = {ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year      = {2020}
}
@inproceedings{Bredin2021,
  title     = {{End-to-end speaker segmentation for overlap-aware resegmentation}},
  author    = {{Bredin}, Herv{\'e} and {Laurent}, Antoine},
  booktitle = {Proc. Interspeech 2021},
  year      = {2021}
}

@online{baghelDISPLACEChallengeDIarization2023b,
  title       = {{{DISPLACE Challenge}}: {{DIarization}} of {{SPeaker}} and {{LAnguage}} in {{Conversational Environments}}},
  shorttitle  = {{{DISPLACE Challenge}}},
  author      = {Baghel, Shikha and Ramoji, Shreyas and Sidharth and H, Ranjana and Singh, Prachi and Jain, Somil and Chowdhuri, Pratik Roy and Kulkarni, Kaustubh and Padhi, Swapnil and Vijayasenan, Deepu and Ganapathy, Sriram},
  date        = {2023-06-05},
  eprint      = {2303.00830},
  eprinttype  = {arxiv},
  eprintclass = {cs, eess},
  url         = {http://arxiv.org/abs/2303.00830},
  urldate     = {2023-08-02},
  abstract    = {In multilingual societies, social conversations often involve code-mixed speech. The current speech technology may not be well equipped to extract information from multi-lingual multi-speaker conversations. The DISPLACE challenge entails a first-of-kind task to benchmark speaker and language diarization on the same data, as the data contains multi-speaker conversations in multilingual code-mixed speech. The challenge attempts to highlight outstanding issues in speaker diarization (SD) in multilingual settings with code-mixing. Further, language diarization (LD) in multi-speaker settings also introduces new challenges, where the system has to disambiguate speaker switches with code switches. For this challenge, a natural multilingual, multi-speaker conversational dataset is distributed for development and evaluation purposes. The systems are evaluated on single-channel far-field recordings. We also release a baseline system and report the highlights of the system submissions.},
  pubstate    = {preprint},
  keywords    = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing},
  file        = {/Users/brono/Zotero/storage/XSKAAYB4/Baghel et al. - 2023 - DISPLACE Challenge DIarization of SPeaker and LAn.pdf;/Users/brono/Zotero/storage/LNAFQKP7/2303.html}
}
